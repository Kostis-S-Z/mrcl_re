2019-10-27 21:05:35.798321: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-10-27 21:05:35.839348: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000165000 Hz
2019-10-27 21:05:35.841986: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5798d90 executing computations on platform Host. Devices:
2019-10-27 21:05:35.842028: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
2019-10-27 21:05:35.881318: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2019-10-27 21:05:35.920052: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2019-10-27 21:05:35.920139: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (instance-1): /proc/driver/nvidia/version does not exist
GPU is available: False
Downloaded omniglot dataset (v:1.0.0)
Size in disk: 18.827037 MB
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 84, 84, 1)]       0         
_________________________________________________________________
conv2d (Conv2D)              (None, 41, 41, 256)       2560      
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 39, 39, 256)       590080    
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 19, 19, 256)       590080    
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 17, 17, 256)       590080    
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 8, 8, 256)         590080    
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 3, 3, 256)         590080    
_________________________________________________________________
flatten (Flatten)            (None, 2304)              0         
=================================================================
Total params: 2,952,960
Trainable params: 2,952,960
Non-trainable params: 0
_________________________________________________________________
None
Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 2304)]            0         
_________________________________________________________________
dense (Dense)                (None, 1024)              2360320   
_________________________________________________________________
dense_1 (Dense)              (None, 964)               988100    
=================================================================
Total params: 3,348,420
Trainable params: 3,348,420
Non-trainable params: 0
_________________________________________________________________
None
Epoch: 0 Sparsity: 0.5846788194444444 Training loss: 75.212006
Epoch: 1 Sparsity: 0.6273003472222222 Training loss: 11.960771
Epoch: 2 Sparsity: 0.6378472222222222 Training loss: 5.18323
Epoch: 3 Sparsity: 0.7103298611111112 Training loss: 6.4815702
Epoch: 4 Sparsity: 0.7245225694444444 Training loss: 3.88652
Epoch: 5 Sparsity: 0.7372395833333334 Training loss: 4.505903
Epoch: 6 Sparsity: 0.7653645833333332 Training loss: 4.2945285
Epoch: 7 Sparsity: 0.7760416666666666 Training loss: 4.0013266
Epoch: 8 Sparsity: 0.814279513888889 Training loss: 3.7682211
Epoch: 9 Sparsity: 0.8187065972222222 Training loss: 3.8915226
Epoch: 10 Sparsity: 0.851171875 Training loss: 4.2230463
Epoch: 11 Sparsity: 0.8571180555555558 Training loss: 3.539856
Epoch: 12 Sparsity: 0.8756944444444444 Training loss: 4.0799413
Epoch: 13 Sparsity: 0.9145399305555555 Training loss: 3.647513
Epoch: 14 Sparsity: 0.9065538194444445 Training loss: 3.7507646
Epoch: 15 Sparsity: 0.9367187499999998 Training loss: 3.18855
Epoch: 16 Sparsity: 0.9434895833333332 Training loss: 2.966676
Epoch: 17 Sparsity: 0.9547309027777778 Training loss: 3.2982237
Epoch: 18 Sparsity: 0.9699652777777779 Training loss: 3.048865
Epoch: 19 Sparsity: 0.8798177083333334 Training loss: 3.4127975
Epoch: 20 Sparsity: 0.9447482638888889 Training loss: 4.155133
Epoch: 21 Sparsity: 0.9535590277777779 Training loss: 3.111127
Epoch: 22 Sparsity: 0.974392361111111 Training loss: 3.48535
Epoch: 23 Sparsity: 0.894704861111111 Training loss: 3.2084289
Epoch: 24 Sparsity: 0.9525173611111113 Training loss: 4.3703647
Epoch: 25 Sparsity: 0.9711805555555555 Training loss: 3.4464424
Epoch: 26 Sparsity: 0.9117621527777778 Training loss: 2.6353195
Epoch: 27 Sparsity: 0.9240885416666668 Training loss: 4.1715417
Epoch: 28 Sparsity: 0.955078125 Training loss: 3.568335
Epoch: 29 Sparsity: 0.9715277777777777 Training loss: 2.7760642
Epoch: 30 Sparsity: 0.8898003472222221 Training loss: 3.90727
Epoch: 31 Sparsity: 0.9464843749999998 Training loss: 4.3842034
Epoch: 32 Sparsity: 0.965625 Training loss: 3.5571475
Epoch: 33 Sparsity: 0.855642361111111 Training loss: 2.8398705
Epoch: 34 Sparsity: 0.9368923611111111 Training loss: 4.4432063
Epoch: 35 Sparsity: 0.9556857638888889 Training loss: 3.365442
Epoch: 36 Sparsity: 0.9704427083333333 Training loss: 2.7133992
Epoch: 37 Sparsity: 0.9658854166666666 Training loss: 2.7156062
Epoch: 38 Sparsity: 0.9672743055555555 Training loss: 2.4829345
Epoch: 39 Sparsity: 0.9494791666666668 Training loss: 2.586965
Epoch: 40 Sparsity: 0.9772569444444444 Training loss: 3.3140283
Epoch: 41 Sparsity: 0.9642361111111111 Training loss: 2.529541
Epoch: 42 Sparsity: 0.8836371527777779 Training loss: 3.250209
Epoch: 43 Sparsity: 0.9543836805555556 Training loss: 4.2697678
Epoch: 44 Sparsity: 0.9616753472222221 Training loss: 2.8329253
Epoch: 45 Sparsity: 0.9740451388888888 Training loss: 3.2941854
Epoch: 46 Sparsity: 0.9240885416666667 Training loss: 2.5317674
Epoch: 47 Sparsity: 0.9783854166666666 Training loss: 4.4540772
Epoch: 48 Sparsity: 0.9245659722222221 Training loss: 4.606534
Epoch: 49 Sparsity: 0.9569878472222223 Training loss: 4.2844796
Epoch: 50 Sparsity: 0.9734375 Training loss: 3.121623
Epoch: 51 Sparsity: 0.9106770833333334 Training loss: 3.589033
Epoch: 52 Sparsity: 0.9485677083333334 Training loss: 3.9828439
Epoch: 53 Sparsity: 0.9626302083333333 Training loss: 2.5716197
Epoch: 54 Sparsity: 0.9721788194444445 Training loss: 2.694837
Epoch: 55 Sparsity: 0.9665798611111113 Training loss: 2.62163
Epoch: 56 Sparsity: 0.9751736111111109 Training loss: 3.129417
Epoch: 57 Sparsity: 0.9835503472222221 Training loss: 3.2431045
Epoch: 58 Sparsity: 0.935373263888889 Training loss: 4.9911556
Epoch: 59 Sparsity: 0.9640625000000002 Training loss: 3.9500654
Epoch: 60 Sparsity: 0.9736545138888888 Training loss: 2.6328158
Epoch: 61 Sparsity: 0.9762586805555555 Training loss: 3.279127
Epoch: 62 Sparsity: 0.9262586805555555 Training loss: 3.2269125
Epoch: 63 Sparsity: 0.9554253472222222 Training loss: 4.2039833
Epoch: 64 Sparsity: 0.9647569444444445 Training loss: 2.520904
Epoch: 65 Sparsity: 0.9828993055555554 Training loss: 2.6704233
Epoch: 66 Sparsity: 0.928125 Training loss: 5.3670654
Epoch: 67 Sparsity: 0.9685329861111113 Training loss: 4.120811
Epoch: 68 Sparsity: 0.9276475694444445 Training loss: 2.6032395
Epoch: 69 Sparsity: 0.9424479166666668 Training loss: 3.3846931
Epoch: 70 Sparsity: 0.9732638888888887 Training loss: 2.7693784
Epoch: 71 Sparsity: 0.9237847222222223 Training loss: 3.0584505
Epoch: 72 Sparsity: 0.9618489583333334 Training loss: 4.0952353
Epoch: 73 Sparsity: 0.9584635416666668 Training loss: 2.696807
Epoch: 74 Sparsity: 0.9672743055555555 Training loss: 2.9356833
Epoch: 75 Sparsity: 0.903125 Training loss: 2.8753395
Epoch: 76 Sparsity: 0.9371961805555558 Training loss: 4.5999618
Epoch: 77 Sparsity: 0.965625 Training loss: 2.7511923
Epoch: 78 Sparsity: 0.9678385416666666 Training loss: 2.620887
Epoch: 79 Sparsity: 0.9667534722222223 Training loss: 2.638481
Epoch: 80 Sparsity: 0.9696180555555556 Training loss: 3.0047255
Epoch: 81 Sparsity: 0.9227864583333334 Training loss: 2.6169336
Epoch: 82 Sparsity: 0.958810763888889 Training loss: 3.9896314
Epoch: 83 Sparsity: 0.9733940972222224 Training loss: 2.8599162
Epoch: 84 Sparsity: 0.9245225694444444 Training loss: 3.798112
Epoch: 85 Sparsity: 0.9601562499999998 Training loss: 4.174673
Epoch: 86 Sparsity: 0.9727430555555555 Training loss: 2.9278865
Epoch: 87 Sparsity: 0.9022569444444445 Training loss: 4.204818
Epoch: 88 Sparsity: 0.9489149305555555 Training loss: 4.3073835
Epoch: 89 Sparsity: 0.9705729166666666 Training loss: 2.797664
Epoch: 90 Sparsity: 0.9443142361111112 Training loss: 6.0937605
Epoch: 91 Sparsity: 0.9521701388888889 Training loss: 4.054496
Epoch: 92 Sparsity: 0.9065104166666667 Training loss: 2.6730897
Epoch: 93 Sparsity: 0.9587239583333333 Training loss: 3.8255894
Epoch: 94 Sparsity: 0.9632812499999999 Training loss: 2.4856987
Epoch: 95 Sparsity: 0.9221354166666668 Training loss: 2.636729
Epoch: 96 Sparsity: 0.9481336805555556 Training loss: 3.6221626
Epoch: 97 Sparsity: 0.9684027777777778 Training loss: 2.578254
Epoch: 98 Sparsity: 0.9198350694444445 Training loss: 2.579033
Epoch: 99 Sparsity: 0.9608072916666666 Training loss: 3.6582296
Epoch: 100 Sparsity: 0.9268663194444444 Training loss: 2.9049733
Epoch: 101 Sparsity: 0.9377604166666668 Training loss: 3.9882143
Epoch: 102 Sparsity: 0.9629774305555555 Training loss: 3.255317
Epoch: 103 Sparsity: 0.9106336805555555 Training loss: 3.3968863
Epoch: 104 Sparsity: 0.9442708333333332 Training loss: 3.330324
Epoch: 105 Sparsity: 0.9628472222222223 Training loss: 2.8666353
Epoch: 106 Sparsity: 0.9217447916666666 Training loss: 2.646647
Epoch: 107 Sparsity: 0.9521267361111111 Training loss: 3.4191108
Epoch: 108 Sparsity: 0.9615017361111111 Training loss: 2.7558646
Epoch: 109 Sparsity: 0.9092013888888889 Training loss: 3.412433
Epoch: 110 Sparsity: 0.9473524305555555 Training loss: 4.0829625
Epoch: 111 Sparsity: 0.9713975694444444 Training loss: 2.9316978
Epoch: 112 Sparsity: 0.9171875 Training loss: 4.3445454
Epoch: 113 Sparsity: 0.9523003472222223 Training loss: 3.49534
Epoch: 114 Sparsity: 0.9562065972222221 Training loss: 2.7824576
Epoch: 115 Sparsity: 0.8977430555555556 Training loss: 2.9584775
Epoch: 116 Sparsity: 0.9333333333333332 Training loss: 3.3917384
Epoch: 117 Sparsity: 0.960546875 Training loss: 2.402125
Epoch: 118 Sparsity: 0.9330729166666666 Training loss: 2.744522
Epoch: 119 Sparsity: 0.9430989583333332 Training loss: 3.3325574
Epoch: 120 Sparsity: 0.9567708333333333 Training loss: 2.9775422
Epoch: 121 Sparsity: 0.961111111111111 Training loss: 2.575528
Epoch: 122 Sparsity: 0.96328125 Training loss: 2.8856826
Epoch: 123 Sparsity: 0.9091579861111111 Training loss: 3.1661332
Epoch: 124 Sparsity: 0.9391059027777777 Training loss: 3.9725826
Epoch: 125 Sparsity: 0.9595920138888889 Training loss: 3.1098666
Epoch: 126 Sparsity: 0.9047743055555555 Training loss: 4.778166
Epoch: 127 Sparsity: 0.929296875 Training loss: 3.2741053
Epoch: 128 Sparsity: 0.9664062500000001 Training loss: 2.7755842
Epoch: 129 Sparsity: 0.9258246527777778 Training loss: 3.3800356
Epoch: 130 Sparsity: 0.9584635416666668 Training loss: 3.7644284
Epoch: 131 Sparsity: 0.9539496527777779 Training loss: 2.6245892
Epoch: 132 Sparsity: 0.9661024305555553 Training loss: 2.6041026
Epoch: 133 Sparsity: 0.9372395833333333 Training loss: 2.7233622
Epoch: 134 Sparsity: 0.9599392361111111 Training loss: 3.2647579
Epoch: 135 Sparsity: 0.9144097222222223 Training loss: 3.327499
Epoch: 136 Sparsity: 0.93359375 Training loss: 3.6726015
Epoch: 137 Sparsity: 0.9657552083333334 Training loss: 2.940729
Epoch: 138 Sparsity: 0.9421874999999998 Training loss: 5.0368223
Epoch: 139 Sparsity: 0.9609809027777777 Training loss: 3.76216
Epoch: 140 Sparsity: 0.9253038194444445 Training loss: 3.0475051
Epoch: 141 Sparsity: 0.9234809027777777 Training loss: 2.901396
Epoch: 142 Sparsity: 0.955251736111111 Training loss: 2.7625299
Epoch: 143 Sparsity: 0.9713107638888889 Training loss: 2.7091384
Epoch: 144 Sparsity: 0.9452690972222222 Training loss: 6.2132235
Epoch: 145 Sparsity: 0.9555555555555554 Training loss: 2.9348307
Epoch: 146 Sparsity: 0.906640625 Training loss: 3.8133807
Epoch: 147 Sparsity: 0.9403645833333334 Training loss: 3.5188973
Epoch: 148 Sparsity: 0.9515625 Training loss: 2.4535663
Epoch: 149 Sparsity: 0.9106336805555555 Training loss: 3.1922824
Epoch: 150 Sparsity: 0.9352864583333333 Training loss: 3.8203855
Epoch: 151 Sparsity: 0.9694444444444443 Training loss: 2.9243546
Epoch: 152 Sparsity: 0.9292534722222223 Training loss: 3.992986
Epoch: 153 Sparsity: 0.9507378472222223 Training loss: 3.8439412
Epoch: 154 Sparsity: 0.9077256944444445 Training loss: 3.2191591
Epoch: 155 Sparsity: 0.93125 Training loss: 3.7659628
Epoch: 156 Sparsity: 0.9522135416666666 Training loss: 2.6074607
Epoch: 157 Sparsity: 0.9166232638888889 Training loss: 3.76101
Epoch: 158 Sparsity: 0.9404513888888888 Training loss: 3.3958297
Epoch: 159 Sparsity: 0.9402777777777779 Training loss: 2.6666424
Epoch: 160 Sparsity: 0.9556423611111112 Training loss: 2.772922
Epoch: 161 Sparsity: 0.905078125 Training loss: 3.569779
Epoch: 162 Sparsity: 0.9368055555555556 Training loss: 3.847462
Epoch: 163 Sparsity: 0.9720052083333334 Training loss: 2.8659754
Epoch: 164 Sparsity: 0.9267361111111111 Training loss: 5.3147817
Epoch: 165 Sparsity: 0.9344618055555554 Training loss: 3.037779
Epoch: 166 Sparsity: 0.966579861111111 Training loss: 2.7393756
Epoch: 167 Sparsity: 0.915625 Training loss: 4.067747
Epoch: 168 Sparsity: 0.952951388888889 Training loss: 3.5729532
Epoch: 169 Sparsity: 0.9180989583333332 Training loss: 2.850682
Epoch: 170 Sparsity: 0.9434895833333334 Training loss: 3.62641
Epoch: 171 Sparsity: 0.915842013888889 Training loss: 2.5913627
Epoch: 172 Sparsity: 0.951388888888889 Training loss: 3.2308347
Epoch: 173 Sparsity: 0.9131944444444444 Training loss: 2.9150949
Epoch: 174 Sparsity: 0.9397569444444442 Training loss: 3.2230623
Epoch: 175 Sparsity: 0.9216145833333333 Training loss: 2.5454123
Epoch: 176 Sparsity: 0.9638454861111111 Training loss: 2.943669
Epoch: 177 Sparsity: 0.9364583333333334 Training loss: 2.6546085
Epoch: 178 Sparsity: 0.9440104166666666 Training loss: 2.9980664
Epoch: 179 Sparsity: 0.9715711805555556 Training loss: 2.5444903
Epoch: 180 Sparsity: 0.9379774305555555 Training loss: 3.8681161
Epoch: 181 Sparsity: 0.9610677083333332 Training loss: 3.3706803
Epoch: 182 Sparsity: 0.9212239583333333 Training loss: 4.36985
Epoch: 183 Sparsity: 0.9523871527777779 Training loss: 3.113989
Epoch: 184 Sparsity: 0.927734375 Training loss: 4.1407876
Epoch: 185 Sparsity: 0.9411024305555558 Training loss: 3.6184986
Epoch: 186 Sparsity: 0.9520399305555556 Training loss: 2.4420538
Epoch: 187 Sparsity: 0.9386718750000002 Training loss: 2.3794107
Epoch: 188 Sparsity: 0.9686631944444443 Training loss: 2.5442603
Epoch: 189 Sparsity: 0.9173611111111111 Training loss: 3.3760188
Epoch: 190 Sparsity: 0.9459201388888889 Training loss: 3.6607633
Epoch: 191 Sparsity: 0.8987847222222222 Training loss: 3.089174
Epoch: 192 Sparsity: 0.9204861111111112 Training loss: 3.1523309
Epoch: 193 Sparsity: 0.955295138888889 Training loss: 2.5594294
Epoch: 194 Sparsity: 0.9297309027777777 Training loss: 5.6781983
Epoch: 195 Sparsity: 0.9466145833333334 Training loss: 3.3559568
Epoch: 196 Sparsity: 0.9063802083333332 Training loss: 2.8383853
Epoch: 197 Sparsity: 0.933376736111111 Training loss: 3.6911001
Epoch: 198 Sparsity: 0.9535590277777779 Training loss: 2.4708936
Epoch: 199 Sparsity: 0.9131510416666666 Training loss: 2.723889
Epoch: 200 Sparsity: 0.9424913194444444 Training loss: 3.65478
Epoch: 201 Sparsity: 0.9133246527777776 Training loss: 2.4501238
Epoch: 202 Sparsity: 0.9309027777777776 Training loss: 3.205462
Epoch: 203 Sparsity: 0.9106336805555555 Training loss: 2.8687387
Epoch: 204 Sparsity: 0.9387152777777779 Training loss: 3.2635677
Epoch: 205 Sparsity: 0.91953125 Training loss: 2.5463681
Epoch: 206 Sparsity: 0.9404079861111111 Training loss: 3.401451
Epoch: 207 Sparsity: 0.9046440972222222 Training loss: 2.5331376
Epoch: 208 Sparsity: 0.9425347222222221 Training loss: 4.200591
Epoch: 209 Sparsity: 0.9243923611111112 Training loss: 2.787984
Epoch: 210 Sparsity: 0.9546875 Training loss: 2.8840163
Epoch: 211 Sparsity: 0.8977430555555556 Training loss: 4.1671295
Epoch: 212 Sparsity: 0.9375868055555557 Training loss: 2.654218
Epoch: 213 Sparsity: 0.9400173611111112 Training loss: 2.5989037
Epoch: 214 Sparsity: 0.9635850694444444 Training loss: 2.5926292
Epoch: 215 Sparsity: 0.9062934027777778 Training loss: 4.687143
Epoch: 216 Sparsity: 0.9367621527777779 Training loss: 3.5171225
Epoch: 217 Sparsity: 0.9231336805555557 Training loss: 2.4578102
Epoch: 218 Sparsity: 0.9457465277777777 Training loss: 3.6724017
Epoch: 219 Sparsity: 0.8838541666666666 Training loss: 2.730164
Epoch: 220 Sparsity: 0.9255642361111113 Training loss: 3.6743402
Epoch: 221 Sparsity: 0.8851128472222222 Training loss: 3.2032437
Epoch: 222 Sparsity: 0.9362847222222224 Training loss: 3.1011894
Epoch: 223 Sparsity: 0.9356336805555555 Training loss: 2.5460215
Epoch: 224 Sparsity: 0.9496527777777777 Training loss: 2.8530333
Epoch: 225 Sparsity: 0.8904947916666668 Training loss: 3.1305063
Epoch: 226 Sparsity: 0.9194010416666666 Training loss: 3.4216354
Epoch: 227 Sparsity: 0.8680989583333332 Training loss: 2.86127
Epoch: 228 Sparsity: 0.9186631944444443 Training loss: 2.626783
Epoch: 229 Sparsity: 0.9677517361111111 Training loss: 2.484085
Epoch: 230 Sparsity: 0.9110243055555554 Training loss: 5.197974
Epoch: 231 Sparsity: 0.9434027777777778 Training loss: 3.2020097
Epoch: 232 Sparsity: 0.9186197916666666 Training loss: 2.6613894
Epoch: 233 Sparsity: 0.9394965277777778 Training loss: 2.7459319
Epoch: 234 Sparsity: 0.9158420138888888 Training loss: 2.6501007
Epoch: 235 Sparsity: 0.962890625 Training loss: 2.7289186
Epoch: 236 Sparsity: 0.9133246527777776 Training loss: 5.170902
Epoch: 237 Sparsity: 0.9301215277777779 Training loss: 2.9591038
Epoch: 238 Sparsity: 0.963671875 Training loss: 2.6251414
Epoch: 239 Sparsity: 0.9098958333333333 Training loss: 5.052857
Epoch: 240 Sparsity: 0.9359809027777779 Training loss: 2.5646632
Epoch: 241 Sparsity: 0.9172309027777776 Training loss: 2.4583788
Epoch: 242 Sparsity: 0.9534288194444445 Training loss: 2.5320592
Epoch: 243 Sparsity: 0.9140190972222222 Training loss: 2.5129898
Epoch: 244 Sparsity: 0.9427951388888889 Training loss: 3.4323068
Epoch: 245 Sparsity: 0.9125 Training loss: 4.8083854
Epoch: 246 Sparsity: 0.9383680555555556 Training loss: 2.6960845
Epoch: 247 Sparsity: 0.936328125 Training loss: 2.4699233
Epoch: 248 Sparsity: 0.9696614583333332 Training loss: 2.5618033
Epoch: 249 Sparsity: 0.91484375 Training loss: 5.52427
Epoch: 250 Sparsity: 0.9409288194444443 Training loss: 2.6304724
Epoch: 251 Sparsity: 0.8898003472222221 Training loss: 4.374429
Epoch: 252 Sparsity: 0.931467013888889 Training loss: 3.23806
Epoch: 253 Sparsity: 0.9155381944444445 Training loss: 2.5506701
Epoch: 254 Sparsity: 0.9438802083333332 Training loss: 2.7296398
Epoch: 255 Sparsity: 0.8922309027777778 Training loss: 3.5178921
Epoch: 256 Sparsity: 0.9096354166666668 Training loss: 2.898305
Epoch: 257 Sparsity: 0.9484809027777776 Training loss: 2.6949172
Epoch: 258 Sparsity: 0.9063802083333334 Training loss: 3.983547
Epoch: 259 Sparsity: 0.93203125 Training loss: 3.2631357
Epoch: 260 Sparsity: 0.9073784722222221 Training loss: 2.6110027
Epoch: 261 Sparsity: 0.9154079861111113 Training loss: 3.2181299
Epoch: 262 Sparsity: 0.9479600694444444 Training loss: 2.5897567
Epoch: 263 Sparsity: 0.9066840277777777 Training loss: 4.3333726
Epoch: 264 Sparsity: 0.9245225694444444 Training loss: 3.2271411
Epoch: 265 Sparsity: 0.8866319444444445 Training loss: 2.4797175
Epoch: 266 Sparsity: 0.91796875 Training loss: 2.4989004
Epoch: 267 Sparsity: 0.9286892361111111 Training loss: 2.6098623
Epoch: 268 Sparsity: 0.9584635416666668 Training loss: 2.740913
Epoch: 269 Sparsity: 0.9324652777777777 Training loss: 5.5828958
Epoch: 270 Sparsity: 0.9499565972222224 Training loss: 3.0199456
Epoch: 271 Sparsity: 0.9068142361111111 Training loss: 3.4425285
Epoch: 272 Sparsity: 0.9341145833333334 Training loss: 2.8468795
Epoch: 273 Sparsity: 0.8731770833333332 Training loss: 4.5489254
Epoch: 274 Sparsity: 0.9310329861111111 Training loss: 2.6681921
Epoch: 275 Sparsity: 0.9239583333333334 Training loss: 2.5745003
Epoch: 276 Sparsity: 0.9660590277777779 Training loss: 2.5285602
Epoch: 277 Sparsity: 0.91953125 Training loss: 4.526052
Epoch: 278 Sparsity: 0.931423611111111 Training loss: 3.176944
Epoch: 279 Sparsity: 0.8941406250000001 Training loss: 2.4735315
Epoch: 280 Sparsity: 0.9291232638888889 Training loss: 2.9699616
Epoch: 281 Sparsity: 0.874826388888889 Training loss: 2.9113054
Epoch: 282 Sparsity: 0.9292534722222223 Training loss: 2.9192936
Epoch: 283 Sparsity: 0.8881076388888889 Training loss: 2.5350754
Epoch: 284 Sparsity: 0.9385416666666666 Training loss: 2.6642363
Epoch: 285 Sparsity: 0.878515625 Training loss: 3.6477368
Epoch: 286 Sparsity: 0.9200954861111112 Training loss: 2.836319
Epoch: 287 Sparsity: 0.8639322916666667 Training loss: 2.7881925
Epoch: 288 Sparsity: 0.9012586805555556 Training loss: 3.0436962
Epoch: 289 Sparsity: 0.9103732638888887 Training loss: 2.4849927
Epoch: 290 Sparsity: 0.9182291666666667 Training loss: 2.6339948
Epoch: 291 Sparsity: 0.9079427083333333 Training loss: 2.5507138
Epoch: 292 Sparsity: 0.9342881944444444 Training loss: 2.8974555
Epoch: 293 Sparsity: 0.8925347222222223 Training loss: 2.555805
Epoch: 294 Sparsity: 0.927734375 Training loss: 2.674012
Epoch: 295 Sparsity: 0.8928385416666667 Training loss: 3.7585096
Epoch: 296 Sparsity: 0.91875 Training loss: 3.2012126
Epoch: 297 Sparsity: 0.9266059027777779 Training loss: 2.4480124
Epoch: 298 Sparsity: 0.9641493055555556 Training loss: 2.635774
Epoch: 299 Sparsity: 0.9140625 Training loss: 4.8439546
Epoch: 300 Sparsity: 0.9387586805555556 Training loss: 2.6434937
Epoch: 301 Sparsity: 0.9066406249999999 Training loss: 2.6328151
Epoch: 302 Sparsity: 0.9489149305555555 Training loss: 2.692669
Epoch: 303 Sparsity: 0.9 Training loss: 2.9300008
Epoch: 304 Sparsity: 0.9409288194444445 Training loss: 2.9299057
Epoch: 305 Sparsity: 0.8970052083333334 Training loss: 2.5724835
Epoch: 306 Sparsity: 0.9284722222222221 Training loss: 2.9291365
Epoch: 307 Sparsity: 0.8952256944444444 Training loss: 2.398124
Epoch: 308 Sparsity: 0.9167534722222224 Training loss: 2.8363032
Epoch: 309 Sparsity: 0.8683159722222223 Training loss: 2.742293
Epoch: 310 Sparsity: 0.9188802083333332 Training loss: 2.6800783
Epoch: 311 Sparsity: 0.9364149305555556 Training loss: 2.5686166
Epoch: 312 Sparsity: 0.9146267361111111 Training loss: 2.4063268
Epoch: 313 Sparsity: 0.8990885416666667 Training loss: 2.6607277
Epoch: 314 Sparsity: 0.923828125 Training loss: 2.8475347
Epoch: 315 Sparsity: 0.9077256944444445 Training loss: 2.4854403
Epoch: 316 Sparsity: 0.9377170138888887 Training loss: 2.5028932
Epoch: 317 Sparsity: 0.8940538194444445 Training loss: 2.6457446
Epoch: 318 Sparsity: 0.9102430555555555 Training loss: 2.5847483
Epoch: 319 Sparsity: 0.945703125 Training loss: 2.4229128
Epoch: 320 Sparsity: 0.9003472222222222 Training loss: 3.0871363
Epoch: 321 Sparsity: 0.9214409722222223 Training loss: 2.3889093
Epoch: 322 Sparsity: 0.9152777777777779 Training loss: 2.420253
Epoch: 323 Sparsity: 0.9244791666666666 Training loss: 2.874645
Epoch: 324 Sparsity: 0.9145833333333334 Training loss: 2.549322
Epoch: 325 Sparsity: 0.9498263888888889 Training loss: 2.5989091
Epoch: 326 Sparsity: 0.884982638888889 Training loss: 2.7305017
Epoch: 327 Sparsity: 0.915798611111111 Training loss: 3.118321
Epoch: 328 Sparsity: 0.9606770833333333 Training loss: 2.5384061
Epoch: 329 Sparsity: 0.9290364583333333 Training loss: 3.9600475
Epoch: 330 Sparsity: 0.9387152777777779 Training loss: 2.527583
Epoch: 331 Sparsity: 0.8930121527777777 Training loss: 2.6529088
Epoch: 332 Sparsity: 0.9221788194444442 Training loss: 2.7925878
Epoch: 333 Sparsity: 0.9238715277777778 Training loss: 2.466252
Epoch: 334 Sparsity: 0.9683159722222223 Training loss: 2.753409
Epoch: 335 Sparsity: 0.9199652777777777 Training loss: 5.9413533
Epoch: 336 Sparsity: 0.9566406249999998 Training loss: 2.7165606
Epoch: 337 Sparsity: 0.9157552083333332 Training loss: 5.3310385
Epoch: 338 Sparsity: 0.9427083333333334 Training loss: 2.6440115
Epoch: 339 Sparsity: 0.8827256944444445 Training loss: 3.186305
Epoch: 340 Sparsity: 0.9348524305555556 Training loss: 3.1742685
Epoch: 341 Sparsity: 0.9043402777777778 Training loss: 2.4348114
Epoch: 342 Sparsity: 0.93828125 Training loss: 2.873729
Epoch: 343 Sparsity: 0.8964409722222223 Training loss: 2.8572202
Epoch: 344 Sparsity: 0.9258680555555555 Training loss: 2.6533077
Epoch: 345 Sparsity: 0.8994357638888889 Training loss: 2.4604142
Epoch: 346 Sparsity: 0.931814236111111 Training loss: 2.994361
Epoch: 347 Sparsity: 0.8822916666666666 Training loss: 2.9101536
Epoch: 348 Sparsity: 0.9251736111111113 Training loss: 2.439686
Epoch: 349 Sparsity: 0.9013454861111109 Training loss: 2.567212
Epoch: 350 Sparsity: 0.915234375 Training loss: 2.5267787
Epoch: 351 Sparsity: 0.9574652777777779 Training loss: 2.5025203
Epoch: 352 Sparsity: 0.9076822916666666 Training loss: 4.7324777
Epoch: 353 Sparsity: 0.9330729166666668 Training loss: 2.4822054
Epoch: 354 Sparsity: 0.9687934027777778 Training loss: 2.5555646
Epoch: 355 Sparsity: 0.9240885416666667 Training loss: 5.2398963
Epoch: 356 Sparsity: 0.9452256944444443 Training loss: 2.6384847
Epoch: 357 Sparsity: 0.8987413194444445 Training loss: 2.4516194
Epoch: 358 Sparsity: 0.924609375 Training loss: 3.016868
Epoch: 359 Sparsity: 0.9260850694444445 Training loss: 2.4851341
Epoch: 360 Sparsity: 0.9493489583333334 Training loss: 2.553068
Epoch: 361 Sparsity: 0.8991753472222221 Training loss: 2.4873517
Epoch: 362 Sparsity: 0.9235243055555555 Training loss: 2.3989584
Epoch: 363 Sparsity: 0.9443142361111109 Training loss: 2.618707
Epoch: 364 Sparsity: 0.8938802083333333 Training loss: 3.1440594
Epoch: 365 Sparsity: 0.9381510416666666 Training loss: 2.496673
Epoch: 366 Sparsity: 0.9296006944444446 Training loss: 2.479985
Epoch: 367 Sparsity: 0.959157986111111 Training loss: 2.5911105
Epoch: 368 Sparsity: 0.9242621527777779 Training loss: 3.430978
Epoch: 369 Sparsity: 0.9473524305555557 Training loss: 2.507201
Epoch: 370 Sparsity: 0.9077690972222221 Training loss: 3.1988523
Epoch: 371 Sparsity: 0.9286458333333334 Training loss: 2.8494906
Epoch: 372 Sparsity: 0.8788194444444445 Training loss: 2.6064599
Epoch: 373 Sparsity: 0.9188368055555556 Training loss: 2.6440735
Epoch: 374 Sparsity: 0.9561631944444444 Training loss: 2.4055977
Epoch: 375 Sparsity: 0.9018229166666666 Training loss: 4.6875753
Epoch: 376 Sparsity: 0.9438802083333334 Training loss: 2.5596106
Epoch: 377 Sparsity: 0.9144097222222222 Training loss: 2.3856921
Epoch: 378 Sparsity: 0.9438368055555555 Training loss: 2.7006695
Epoch: 379 Sparsity: 0.8893663194444444 Training loss: 2.599917
Epoch: 380 Sparsity: 0.938671875 Training loss: 3.0878608
Epoch: 381 Sparsity: 0.9413628472222222 Training loss: 2.524501
Epoch: 382 Sparsity: 0.948828125 Training loss: 2.8930476
Epoch: 383 Sparsity: 0.9111545138888889 Training loss: 4.2108426
Epoch: 384 Sparsity: 0.9386284722222223 Training loss: 2.4163718
Epoch: 385 Sparsity: 0.9408854166666668 Training loss: 2.6535344
Epoch: 386 Sparsity: 0.9606336805555558 Training loss: 2.5757082
Epoch: 387 Sparsity: 0.9218749999999998 Training loss: 3.9785676
Epoch: 388 Sparsity: 0.9433159722222222 Training loss: 3.3140447
Epoch: 389 Sparsity: 0.9121527777777777 Training loss: 2.5090828
Epoch: 390 Sparsity: 0.9536024305555555 Training loss: 2.6045651
Epoch: 391 Sparsity: 0.9096788194444445 Training loss: 2.8121462
Epoch: 392 Sparsity: 0.9453559027777777 Training loss: 2.9679627
Epoch: 393 Sparsity: 0.8965277777777778 Training loss: 2.9097886
Epoch: 394 Sparsity: 0.9286892361111111 Training loss: 3.2725573
Epoch: 395 Sparsity: 0.9129340277777777 Training loss: 2.4838336
Epoch: 396 Sparsity: 0.9150607638888888 Training loss: 2.4244704
Epoch: 397 Sparsity: 0.9570746527777777 Training loss: 3.0409672
Epoch: 398 Sparsity: 0.9215277777777777 Training loss: 3.5977697
Epoch: 399 Sparsity: 0.9545138888888889 Training loss: 2.6977756
Epoch: 400 Sparsity: 0.9069010416666667 Training loss: 3.6950696
Epoch: 401 Sparsity: 0.9219618055555555 Training loss: 3.0009885
Epoch: 402 Sparsity: 0.91328125 Training loss: 2.5062947
Epoch: 403 Sparsity: 0.9331597222222221 Training loss: 2.5693717
Epoch: 404 Sparsity: 0.9574652777777777 Training loss: 2.5246274
Epoch: 405 Sparsity: 0.913845486111111 Training loss: 2.9503276
Epoch: 406 Sparsity: 0.9342013888888887 Training loss: 3.021703
Epoch: 407 Sparsity: 0.9194444444444445 Training loss: 2.4405289
Epoch: 408 Sparsity: 0.9657552083333334 Training loss: 2.6139863
Epoch: 409 Sparsity: 0.9220052083333334 Training loss: 4.344417
Epoch: 410 Sparsity: 0.9398871527777777 Training loss: 2.7373729
Epoch: 411 Sparsity: 0.8969618055555555 Training loss: 2.8023791
Epoch: 412 Sparsity: 0.9439236111111111 Training loss: 3.0660253
Epoch: 413 Sparsity: 0.926953125 Training loss: 2.4385011
Epoch: 414 Sparsity: 0.9444010416666666 Training loss: 2.5170586
Epoch: 415 Sparsity: 0.9026909722222222 Training loss: 2.7543268
Epoch: 416 Sparsity: 0.933376736111111 Training loss: 2.8097792
Epoch: 417 Sparsity: 0.9121527777777778 Training loss: 2.3830836
Epoch: 418 Sparsity: 0.9575520833333334 Training loss: 2.804096
Epoch: 419 Sparsity: 0.918185763888889 Training loss: 2.5292523
Epoch: 420 Sparsity: 0.9506510416666666 Training loss: 3.5201578
Epoch: 421 Sparsity: 0.8999565972222221 Training loss: 2.8870454
Epoch: 422 Sparsity: 0.9278211805555555 Training loss: 2.7125685
Epoch: 423 Sparsity: 0.9241319444444442 Training loss: 2.4016995
Epoch: 424 Sparsity: 0.9449218749999998 Training loss: 2.6905043
Epoch: 425 Sparsity: 0.9092447916666666 Training loss: 2.6614766
Epoch: 426 Sparsity: 0.9403211805555556 Training loss: 2.848527
Epoch: 427 Sparsity: 0.9161458333333334 Training loss: 2.6026392
Epoch: 428 Sparsity: 0.9258680555555557 Training loss: 2.3945806
Epoch: 429 Sparsity: 0.9198784722222222 Training loss: 2.538626
Epoch: 430 Sparsity: 0.9575520833333334 Training loss: 2.5359905
Epoch: 431 Sparsity: 0.9102864583333334 Training loss: 4.1462154
Epoch: 432 Sparsity: 0.9295572916666666 Training loss: 2.8293328
Epoch: 433 Sparsity: 0.9114149305555556 Training loss: 2.3657775
Epoch: 434 Sparsity: 0.9421874999999998 Training loss: 3.1779335
Epoch: 435 Sparsity: 0.9354166666666666 Training loss: 2.394014
Epoch: 436 Sparsity: 0.933984375 Training loss: 2.6565115
Epoch: 437 Sparsity: 0.9563368055555556 Training loss: 2.653012
Epoch: 438 Sparsity: 0.9061197916666668 Training loss: 2.9994497
Epoch: 439 Sparsity: 0.9485677083333334 Training loss: 2.584246
Epoch: 440 Sparsity: 0.8957465277777776 Training loss: 2.5839727
Epoch: 441 Sparsity: 0.939626736111111 Training loss: 2.8128686
Epoch: 442 Sparsity: 0.9037326388888889 Training loss: 2.9380472
Epoch: 443 Sparsity: 0.945095486111111 Training loss: 2.6705103
Epoch: 444 Sparsity: 0.9459201388888892 Training loss: 2.3843324
Epoch: 445 Sparsity: 0.9188368055555556 Training loss: 2.4154522
Epoch: 446 Sparsity: 0.9524739583333334 Training loss: 2.5429645
Epoch: 447 Sparsity: 0.9264322916666667 Training loss: 2.8404121
Epoch: 448 Sparsity: 0.9330295138888888 Training loss: 2.4476578
Epoch: 449 Sparsity: 0.9628038194444445 Training loss: 2.4311333
Epoch: 450 Sparsity: 0.9128472222222224 Training loss: 2.9364114
Epoch: 451 Sparsity: 0.9467447916666666 Training loss: 3.3149781
Epoch: 452 Sparsity: 0.9073784722222223 Training loss: 4.659738
Epoch: 453 Sparsity: 0.9528211805555555 Training loss: 2.72017
Epoch: 454 Sparsity: 0.906076388888889 Training loss: 2.8270948
Epoch: 455 Sparsity: 0.92734375 Training loss: 2.7904265
Epoch: 456 Sparsity: 0.9133680555555556 Training loss: 2.3575747
Epoch: 457 Sparsity: 0.9450520833333333 Training loss: 2.8567898
Epoch: 458 Sparsity: 0.9029947916666666 Training loss: 3.8251894
Epoch: 459 Sparsity: 0.9331597222222223 Training loss: 3.1749196
Epoch: 460 Sparsity: 0.943185763888889 Training loss: 2.426594
Epoch: 461 Sparsity: 0.9161458333333334 Training loss: 2.6244762
Epoch: 462 Sparsity: 0.9450954861111113 Training loss: 2.5732355
Epoch: 463 Sparsity: 0.8940538194444445 Training loss: 2.911423
Epoch: 464 Sparsity: 0.9250434027777776 Training loss: 3.3428452
Epoch: 465 Sparsity: 0.9276041666666666 Training loss: 2.379489
Epoch: 466 Sparsity: 0.9574652777777779 Training loss: 2.8379304
Epoch: 467 Sparsity: 0.9291666666666666 Training loss: 2.5347087
Epoch: 468 Sparsity: 0.9478732638888889 Training loss: 4.5019064
Epoch: 469 Sparsity: 0.8951388888888889 Training loss: 2.924418
Epoch: 470 Sparsity: 0.9399739583333334 Training loss: 2.777438
Epoch: 471 Sparsity: 0.920138888888889 Training loss: 2.5172598
Epoch: 472 Sparsity: 0.9167100694444444 Training loss: 2.6390257
Epoch: 473 Sparsity: 0.9485243055555556 Training loss: 2.6229527
Epoch: 474 Sparsity: 0.913845486111111 Training loss: 3.210429
Epoch: 475 Sparsity: 0.939453125 Training loss: 2.8531673
Epoch: 476 Sparsity: 0.9281684027777777 Training loss: 2.6298413
Epoch: 477 Sparsity: 0.9647569444444445 Training loss: 2.5777233
Epoch: 478 Sparsity: 0.9266059027777779 Training loss: 4.2720504
Epoch: 479 Sparsity: 0.9295572916666666 Training loss: 2.4987302
Epoch: 480 Sparsity: 0.9148437500000002 Training loss: 2.442649
Epoch: 481 Sparsity: 0.9476562500000002 Training loss: 2.487474
Epoch: 482 Sparsity: 0.905859375 Training loss: 2.480949
Epoch: 483 Sparsity: 0.9450086805555555 Training loss: 2.8328962
Epoch: 484 Sparsity: 0.9234809027777778 Training loss: 2.4653974
Epoch: 485 Sparsity: 0.9510850694444445 Training loss: 2.570735
Epoch: 486 Sparsity: 0.9202256944444445 Training loss: 2.6635702
Epoch: 487 Sparsity: 0.9459635416666666 Training loss: 2.8187168
Epoch: 488 Sparsity: 0.953515625 Training loss: 2.5676184
Epoch: 489 Sparsity: 0.9211805555555556 Training loss: 2.8450775
Epoch: 490 Sparsity: 0.9470052083333332 Training loss: 2.6577785
Epoch: 491 Sparsity: 0.9315104166666668 Training loss: 2.5798254
Epoch: 492 Sparsity: 0.9508246527777778 Training loss: 2.566827
Epoch: 493 Sparsity: 0.909982638888889 Training loss: 2.5805514
Epoch: 494 Sparsity: 0.9456163194444445 Training loss: 2.936733
Epoch: 495 Sparsity: 0.8993489583333334 Training loss: 2.733335
Epoch: 496 Sparsity: 0.9237413194444445 Training loss: 2.5392175
Epoch: 497 Sparsity: 0.9625000000000001 Training loss: 2.6441848
Epoch: 498 Sparsity: 0.914236111111111 Training loss: 3.0284877
Epoch: 499 Sparsity: 0.9476996527777777 Training loss: 2.9652052
Epoch: 500 Sparsity: 0.9134114583333334 Training loss: 2.6225848
Epoch: 501 Sparsity: 0.94140625 Training loss: 3.3019469
Epoch: 502 Sparsity: 0.9233506944444445 Training loss: 2.4329019
Epoch: 503 Sparsity: 0.9525607638888888 Training loss: 2.3981028
Epoch: 504 Sparsity: 0.9042100694444445 Training loss: 3.0533102
Epoch: 505 Sparsity: 0.9328993055555556 Training loss: 2.8146894
Epoch: 506 Sparsity: 0.9498697916666666 Training loss: 2.8503213
Epoch: 507 Sparsity: 0.9095052083333333 Training loss: 2.4339972
Epoch: 508 Sparsity: 0.948828125 Training loss: 2.7347116
Epoch: 509 Sparsity: 0.9057725694444445 Training loss: 2.90471
Epoch: 510 Sparsity: 0.9436197916666667 Training loss: 2.902735
Epoch: 511 Sparsity: 0.9124131944444445 Training loss: 2.4510858
Epoch: 512 Sparsity: 0.9389756944444445 Training loss: 2.6912203
Epoch: 513 Sparsity: 0.9531684027777777 Training loss: 2.5729942
Epoch: 514 Sparsity: 0.9263020833333334 Training loss: 4.8931785
Epoch: 515 Sparsity: 0.9538194444444443 Training loss: 3.2378917
Epoch: 516 Sparsity: 0.9200954861111112 Training loss: 4.6779313
Epoch: 517 Sparsity: 0.9456597222222223 Training loss: 2.6633346
Epoch: 518 Sparsity: 0.9105902777777777 Training loss: 3.233234
Epoch: 519 Sparsity: 0.9359374999999999 Training loss: 2.797813
Epoch: 520 Sparsity: 0.9062500000000002 Training loss: 2.7595332
Epoch: 521 Sparsity: 0.9384548611111111 Training loss: 2.7168808
Epoch: 522 Sparsity: 0.9154079861111111 Training loss: 2.5653574
Epoch: 523 Sparsity: 0.9350260416666668 Training loss: 2.764233
Epoch: 524 Sparsity: 0.9268229166666666 Training loss: 2.4322202
Epoch: 525 Sparsity: 0.9469618055555555 Training loss: 2.516437
Epoch: 526 Sparsity: 0.9059895833333332 Training loss: 2.73605
Epoch: 527 Sparsity: 0.9422743055555556 Training loss: 3.0720208
Epoch: 528 Sparsity: 0.907421875 Training loss: 2.8710303
Epoch: 529 Sparsity: 0.9383680555555556 Training loss: 3.132416
Epoch: 530 Sparsity: 0.9204427083333334 Training loss: 2.5675697
Epoch: 531 Sparsity: 0.9366753472222221 Training loss: 2.5160203
Epoch: 532 Sparsity: 0.9579861111111111 Training loss: 2.4488757
Epoch: 533 Sparsity: 0.9276909722222223 Training loss: 2.7168338
Epoch: 534 Sparsity: 0.9378038194444445 Training loss: 2.7738807
Epoch: 535 Sparsity: 0.9073784722222223 Training loss: 5.3236156
Epoch: 536 Sparsity: 0.9301649305555555 Training loss: 2.973024
Epoch: 537 Sparsity: 0.9307291666666666 Training loss: 2.7273114
Epoch: 538 Sparsity: 0.936328125 Training loss: 2.3957531
Epoch: 539 Sparsity: 0.9280815972222223 Training loss: 2.5808375
Epoch: 540 Sparsity: 0.951953125 Training loss: 2.6236951
Epoch: 541 Sparsity: 0.9086805555555555 Training loss: 2.6327388
Epoch: 542 Sparsity: 0.9389756944444443 Training loss: 3.2356455
Epoch: 543 Sparsity: 0.8967013888888887 Training loss: 2.7966232
Epoch: 544 Sparsity: 0.927734375 Training loss: 2.6207893
Epoch: 545 Sparsity: 0.8979166666666666 Training loss: 2.4801462
Epoch: 546 Sparsity: 0.9366753472222221 Training loss: 2.757414
Epoch: 547 Sparsity: 0.9285156250000002 Training loss: 2.558774
Epoch: 548 Sparsity: 0.9598958333333334 Training loss: 2.5339556
Epoch: 549 Sparsity: 0.9237847222222223 Training loss: 3.87388
Epoch: 550 Sparsity: 0.9204427083333334 Training loss: 2.46273
Epoch: 551 Sparsity: 0.9517795138888889 Training loss: 2.605897
Epoch: 552 Sparsity: 0.9221788194444447 Training loss: 2.413105
Epoch: 553 Sparsity: 0.9483940972222223 Training loss: 2.800835
Epoch: 554 Sparsity: 0.915842013888889 Training loss: 3.664479
Epoch: 555 Sparsity: 0.9319444444444445 Training loss: 2.4060261
Epoch: 556 Sparsity: 0.9106336805555555 Training loss: 2.622779
Epoch: 557 Sparsity: 0.9311197916666668 Training loss: 2.5903368
Epoch: 558 Sparsity: 0.9078125 Training loss: 2.6942127
Epoch: 559 Sparsity: 0.9313368055555553 Training loss: 2.631299
Epoch: 560 Sparsity: 0.8993923611111111 Training loss: 2.794338
Epoch: 561 Sparsity: 0.9325086805555556 Training loss: 2.8237238
Epoch: 562 Sparsity: 0.9224826388888889 Training loss: 2.617798
Epoch: 563 Sparsity: 0.9526909722222221 Training loss: 2.473601
Epoch: 564 Sparsity: 0.9144965277777779 Training loss: 2.4667063
Epoch: 565 Sparsity: 0.9436631944444445 Training loss: 2.425161
Epoch: 566 Sparsity: 0.9006944444444445 Training loss: 2.8526235
Epoch: 567 Sparsity: 0.9309895833333334 Training loss: 2.8187788
Epoch: 568 Sparsity: 0.9090277777777777 Training loss: 2.3957865
Epoch: 569 Sparsity: 0.9439236111111111 Training loss: 2.4892578
Epoch: 570 Sparsity: 0.9191840277777779 Training loss: 2.3958359
Epoch: 571 Sparsity: 0.9307725694444443 Training loss: 2.4874184
Epoch: 572 Sparsity: 0.9622395833333334 Training loss: 2.6458838
Epoch: 573 Sparsity: 0.9216145833333333 Training loss: 4.217355
Epoch: 574 Sparsity: 0.943142361111111 Training loss: 2.472538
Epoch: 575 Sparsity: 0.9101996527777777 Training loss: 2.5360975
Epoch: 576 Sparsity: 0.9358506944444445 Training loss: 2.4091115
Epoch: 577 Sparsity: 0.909982638888889 Training loss: 2.4013894
Epoch: 578 Sparsity: 0.9339409722222222 Training loss: 2.839119
Epoch: 579 Sparsity: 0.9351128472222223 Training loss: 2.4764755
Epoch: 580 Sparsity: 0.9680989583333334 Training loss: 2.6662831
Epoch: 581 Sparsity: 0.9296440972222222 Training loss: 5.226456
Epoch: 582 Sparsity: 0.9557725694444443 Training loss: 2.627298
Epoch: 583 Sparsity: 0.9138020833333333 Training loss: 2.7518094
Epoch: 584 Sparsity: 0.9446614583333333 Training loss: 2.4361877
Epoch: 585 Sparsity: 0.9368489583333333 Training loss: 2.5401235
Epoch: 586 Sparsity: 0.9313802083333333 Training loss: 2.706795
Epoch: 587 Sparsity: 0.9503472222222223 Training loss: 2.6425095
Epoch: 588 Sparsity: 0.91640625 Training loss: 2.5926585
Epoch: 589 Sparsity: 0.947265625 Training loss: 2.5102322
Epoch: 590 Sparsity: 0.918359375 Training loss: 2.4713435
Epoch: 591 Sparsity: 0.9327690972222221 Training loss: 2.4969666
Epoch: 592 Sparsity: 0.8994357638888889 Training loss: 2.5709767
Epoch: 593 Sparsity: 0.9390624999999998 Training loss: 3.151829
Epoch: 594 Sparsity: 0.9521267361111111 Training loss: 2.5568976
Epoch: 595 Sparsity: 0.9155815972222221 Training loss: 2.843781
Epoch: 596 Sparsity: 0.9412760416666668 Training loss: 2.839002
Epoch: 597 Sparsity: 0.9153211805555556 Training loss: 2.7583325
Epoch: 598 Sparsity: 0.9332465277777777 Training loss: 2.372791
Epoch: 599 Sparsity: 0.9600260416666668 Training loss: 2.618527
Epoch: 600 Sparsity: 0.9249131944444443 Training loss: 5.058829
Epoch: 601 Sparsity: 0.9502604166666668 Training loss: 2.6850486
Epoch: 602 Sparsity: 0.9108506944444444 Training loss: 2.466291
Epoch: 603 Sparsity: 0.9366753472222221 Training loss: 2.4132683
Epoch: 604 Sparsity: 0.9114149305555556 Training loss: 2.935771
Epoch: 605 Sparsity: 0.9408854166666666 Training loss: 2.647187
Epoch: 606 Sparsity: 0.9145833333333334 Training loss: 2.7731981
Epoch: 607 Sparsity: 0.9453559027777777 Training loss: 2.8449728
Epoch: 608 Sparsity: 0.9346788194444443 Training loss: 2.3357804
Epoch: 609 Sparsity: 0.9254774305555555 Training loss: 2.3356776
Epoch: 610 Sparsity: 0.9504774305555556 Training loss: 2.8631172
Epoch: 611 Sparsity: 0.9286024305555557 Training loss: 2.4215186
Epoch: 612 Sparsity: 0.9478298611111111 Training loss: 2.8389208
Epoch: 613 Sparsity: 0.9532118055555557 Training loss: 2.5519638
Epoch: 614 Sparsity: 0.9321180555555555 Training loss: 2.3471985
Epoch: 615 Sparsity: 0.9556423611111111 Training loss: 2.5191352
Epoch: 616 Sparsity: 0.9054687499999998 Training loss: 4.033302
Epoch: 617 Sparsity: 0.9422309027777779 Training loss: 2.7760081
Epoch: 618 Sparsity: 0.9378038194444445 Training loss: 2.438179
Epoch: 619 Sparsity: 0.9315104166666668 Training loss: 2.3044906
Epoch: 620 Sparsity: 0.9427083333333334 Training loss: 2.4818
Epoch: 621 Sparsity: 0.9556423611111111 Training loss: 2.4905052
Epoch: 622 Sparsity: 0.960720486111111 Training loss: 2.7162476
Epoch: 623 Sparsity: 0.9205295138888889 Training loss: 2.5735936
Epoch: 624 Sparsity: 0.9508680555555555 Training loss: 2.589595
Epoch: 625 Sparsity: 0.9514322916666667 Training loss: 2.3712883
Epoch: 626 Sparsity: 0.9546006944444445 Training loss: 2.6999295
Epoch: 627 Sparsity: 0.9230034722222223 Training loss: 2.4419591
Epoch: 628 Sparsity: 0.9578559027777779 Training loss: 3.0252922
Epoch: 629 Sparsity: 0.9259982638888887 Training loss: 3.5784695
Epoch: 630 Sparsity: 0.9495225694444445 Training loss: 2.6695373
Epoch: 631 Sparsity: 0.9252604166666666 Training loss: 2.9020996
Epoch: 632 Sparsity: 0.9434461805555555 Training loss: 2.9455504
Epoch: 633 Sparsity: 0.9328993055555556 Training loss: 2.5619764
Epoch: 634 Sparsity: 0.9506944444444443 Training loss: 2.4287605
Epoch: 635 Sparsity: 0.909982638888889 Training loss: 2.4656138
Epoch: 636 Sparsity: 0.9509114583333332 Training loss: 3.4364855
Epoch: 637 Sparsity: 0.9539496527777779 Training loss: 2.4083474
Epoch: 638 Sparsity: 0.9207465277777779 Training loss: 2.4233706
Epoch: 639 Sparsity: 0.950390625 Training loss: 2.7500734
Epoch: 640 Sparsity: 0.9260416666666668 Training loss: 2.6054971
Epoch: 641 Sparsity: 0.9534722222222223 Training loss: 2.5863776
Epoch: 642 Sparsity: 0.9188368055555556 Training loss: 2.8574338
Epoch: 643 Sparsity: 0.9481770833333334 Training loss: 2.558106
Epoch: 644 Sparsity: 0.9440972222222224 Training loss: 2.4917617
Epoch: 645 Sparsity: 0.925564236111111 Training loss: 2.5429344
Epoch: 646 Sparsity: 0.941579861111111 Training loss: 2.9947197
Epoch: 647 Sparsity: 0.9210069444444444 Training loss: 2.6271873
Epoch: 648 Sparsity: 0.9322916666666666 Training loss: 2.389076
Epoch: 649 Sparsity: 0.9526475694444445 Training loss: 2.415774
Epoch: 650 Sparsity: 0.9077256944444445 Training loss: 3.2413263
Epoch: 651 Sparsity: 0.942578125 Training loss: 3.799742
Epoch: 652 Sparsity: 0.9235243055555555 Training loss: 2.5694113
Epoch: 653 Sparsity: 0.9477864583333334 Training loss: 3.0460858
Epoch: 654 Sparsity: 0.933767361111111 Training loss: 2.4265254
Epoch: 655 Sparsity: 0.9614583333333334 Training loss: 2.6145594
Epoch: 656 Sparsity: 0.916232638888889 Training loss: 4.510315
Epoch: 657 Sparsity: 0.9433593749999998 Training loss: 2.6401622
Epoch: 658 Sparsity: 0.9127170138888889 Training loss: 3.7408116
Epoch: 659 Sparsity: 0.9427083333333334 Training loss: 2.5848405
Epoch: 660 Sparsity: 0.9409722222222223 Training loss: 2.4892738
Epoch: 661 Sparsity: 0.9110677083333334 Training loss: 2.4458697
Epoch: 662 Sparsity: 0.9443576388888889 Training loss: 2.5470393
Epoch: 663 Sparsity: 0.9533420138888887 Training loss: 2.7423766
Epoch: 664 Sparsity: 0.9091145833333334 Training loss: 2.6042285
Epoch: 665 Sparsity: 0.9438368055555555 Training loss: 2.8360198
Epoch: 666 Sparsity: 0.9148003472222221 Training loss: 2.5134814
Epoch: 667 Sparsity: 0.9425347222222223 Training loss: 2.419313
Epoch: 668 Sparsity: 0.9029079861111111 Training loss: 2.972354
Epoch: 669 Sparsity: 0.9421875 Training loss: 2.5393317
Epoch: 670 Sparsity: 0.9196614583333332 Training loss: 2.4896839
Epoch: 671 Sparsity: 0.9391059027777777 Training loss: 2.6742911
Epoch: 672 Sparsity: 0.9309461805555556 Training loss: 2.665554
Epoch: 673 Sparsity: 0.9043402777777777 Training loss: 2.5715258
Epoch: 674 Sparsity: 0.9408420138888891 Training loss: 3.1220248
Epoch: 675 Sparsity: 0.9305121527777777 Training loss: 2.3449907
Epoch: 676 Sparsity: 0.9500434027777779 Training loss: 2.8149078
Epoch: 677 Sparsity: 0.9182725694444447 Training loss: 3.543091
Epoch: 678 Sparsity: 0.9377604166666668 Training loss: 2.372995
Epoch: 679 Sparsity: 0.9320746527777777 Training loss: 2.4118357
Epoch: 680 Sparsity: 0.9516059027777777 Training loss: 2.48806
Epoch: 681 Sparsity: 0.9418402777777777 Training loss: 2.3257918
Epoch: 682 Sparsity: 0.9594618055555555 Training loss: 2.4273016
Epoch: 683 Sparsity: 0.918185763888889 Training loss: 3.6165755
Epoch: 684 Sparsity: 0.9493923611111112 Training loss: 3.618167
Epoch: 685 Sparsity: 0.9145833333333334 Training loss: 2.49607
Epoch: 686 Sparsity: 0.9125434027777777 Training loss: 2.4349267
Epoch: 687 Sparsity: 0.9495659722222223 Training loss: 2.808668
Epoch: 688 Sparsity: 0.921875 Training loss: 2.3984916
Epoch: 689 Sparsity: 0.94453125 Training loss: 2.8204002
Epoch: 690 Sparsity: 0.919704861111111 Training loss: 2.6239886
Epoch: 691 Sparsity: 0.9173177083333334 Training loss: 2.5350468
Epoch: 692 Sparsity: 0.948828125 Training loss: 2.8184521
Epoch: 693 Sparsity: 0.927517361111111 Training loss: 2.6687279
Epoch: 694 Sparsity: 0.9321180555555557 Training loss: 2.3134725
Epoch: 695 Sparsity: 0.9554253472222223 Training loss: 2.4339235
Epoch: 696 Sparsity: 0.9266493055555554 Training loss: 2.635159
Epoch: 697 Sparsity: 0.9536458333333334 Training loss: 2.7037957
Epoch: 698 Sparsity: 0.9285590277777777 Training loss: 2.3742
Epoch: 699 Sparsity: 0.9500868055555556 Training loss: 2.4332497
Epoch: 700 Sparsity: 0.9485243055555556 Training loss: 2.3522036
Epoch: 701 Sparsity: 0.9539930555555556 Training loss: 2.4324765
Epoch: 702 Sparsity: 0.9230902777777776 Training loss: 2.608841
Epoch: 703 Sparsity: 0.9445746527777779 Training loss: 2.366922
Epoch: 704 Sparsity: 0.9521267361111111 Training loss: 2.5096567
Epoch: 705 Sparsity: 0.9137586805555555 Training loss: 2.9196582
Epoch: 706 Sparsity: 0.9506076388888888 Training loss: 3.0004678
Epoch: 707 Sparsity: 0.9486111111111113 Training loss: 2.5094593
Epoch: 708 Sparsity: 0.9414496527777778 Training loss: 2.4388044
Epoch: 709 Sparsity: 0.951388888888889 Training loss: 2.639066
Epoch: 710 Sparsity: 0.9225694444444444 Training loss: 2.5119803
Epoch: 711 Sparsity: 0.9539930555555556 Training loss: 2.8197975
Epoch: 712 Sparsity: 0.9271267361111111 Training loss: 4.156433
Epoch: 713 Sparsity: 0.952951388888889 Training loss: 2.8581002
Epoch: 714 Sparsity: 0.9271701388888889 Training loss: 2.7508593
Epoch: 715 Sparsity: 0.9461371527777777 Training loss: 2.5443194
Epoch: 716 Sparsity: 0.9603732638888889 Training loss: 2.4709136
Epoch: 717 Sparsity: 0.9270833333333333 Training loss: 3.2941432
Epoch: 718 Sparsity: 0.9496961805555557 Training loss: 2.653301
Epoch: 719 Sparsity: 0.910373263888889 Training loss: 3.0079079
Epoch: 720 Sparsity: 0.9471788194444443 Training loss: 3.0588734
Epoch: 721 Sparsity: 0.929470486111111 Training loss: 2.4833322
Epoch: 722 Sparsity: 0.9524739583333334 Training loss: 2.5266416
Epoch: 723 Sparsity: 0.91171875 Training loss: 3.099673
Epoch: 724 Sparsity: 0.9476128472222222 Training loss: 3.0497108
Epoch: 725 Sparsity: 0.9357638888888887 Training loss: 2.335622
Epoch: 726 Sparsity: 0.9569010416666666 Training loss: 2.5706713
Epoch: 727 Sparsity: 0.9342447916666666 Training loss: 2.5316942
Epoch: 728 Sparsity: 0.9505208333333334 Training loss: 2.4484165
Epoch: 729 Sparsity: 0.9196180555555555 Training loss: 2.4212236
Epoch: 730 Sparsity: 0.9535590277777777 Training loss: 2.6155357
Epoch: 731 Sparsity: 0.9243055555555555 Training loss: 3.3782127
Epoch: 732 Sparsity: 0.9502170138888888 Training loss: 2.9495745
Epoch: 733 Sparsity: 0.937109375 Training loss: 2.507347
Epoch: 734 Sparsity: 0.9527343750000001 Training loss: 2.5065987
Epoch: 735 Sparsity: 0.9158420138888888 Training loss: 3.0133672
Epoch: 736 Sparsity: 0.9460069444444444 Training loss: 3.2316012
Epoch: 737 Sparsity: 0.9208767361111112 Training loss: 2.646007
Epoch: 738 Sparsity: 0.9519531250000002 Training loss: 2.890283
Epoch: 739 Sparsity: 0.9386284722222221 Training loss: 2.5273051
Epoch: 740 Sparsity: 0.9404079861111112 Training loss: 2.572008
Epoch: 741 Sparsity: 0.9159288194444445 Training loss: 2.5836408
Epoch: 742 Sparsity: 0.9463541666666666 Training loss: 2.5731626
Epoch: 743 Sparsity: 0.9577690972222224 Training loss: 2.5429761
Epoch: 744 Sparsity: 0.9266927083333334 Training loss: 2.7482405
Epoch: 745 Sparsity: 0.9560763888888889 Training loss: 2.7587879
Epoch: 746 Sparsity: 0.9290798611111111 Training loss: 2.5322764
Epoch: 747 Sparsity: 0.9387586805555557 Training loss: 2.353191
Epoch: 748 Sparsity: 0.9602430555555556 Training loss: 2.5881383
Epoch: 749 Sparsity: 0.9366319444444444 Training loss: 2.7025065
Epoch: 750 Sparsity: 0.9453993055555555 Training loss: 2.3821344
Epoch: 751 Sparsity: 0.9319010416666668 Training loss: 2.4911397
Epoch: 752 Sparsity: 0.9527777777777778 Training loss: 2.8166099
Epoch: 753 Sparsity: 0.9293836805555555 Training loss: 2.795213
Epoch: 754 Sparsity: 0.9499565972222221 Training loss: 2.4956706
Epoch: 755 Sparsity: 0.9456597222222223 Training loss: 2.43817
Epoch: 756 Sparsity: 0.9534722222222223 Training loss: 2.459044
Epoch: 757 Sparsity: 0.912326388888889 Training loss: 2.3561387
Epoch: 758 Sparsity: 0.9488715277777778 Training loss: 2.7690277
Epoch: 759 Sparsity: 0.9448784722222223 Training loss: 2.4853146
Epoch: 760 Sparsity: 0.9424045138888891 Training loss: 2.4549365
Epoch: 761 Sparsity: 0.9562065972222221 Training loss: 2.4874578
Epoch: 762 Sparsity: 0.9332465277777777 Training loss: 2.3681686
Epoch: 763 Sparsity: 0.9534722222222223 Training loss: 2.4038038
Epoch: 764 Sparsity: 0.9268229166666668 Training loss: 2.491658
Epoch: 765 Sparsity: 0.9596788194444444 Training loss: 2.6080782
Epoch: 766 Sparsity: 0.9427083333333334 Training loss: 2.5254202
Epoch: 767 Sparsity: 0.9575086805555555 Training loss: 2.880047
Epoch: 768 Sparsity: 0.9105034722222222 Training loss: 2.977205
Epoch: 769 Sparsity: 0.95234375 Training loss: 2.879908
Epoch: 770 Sparsity: 0.9308159722222223 Training loss: 2.7349765
Epoch: 771 Sparsity: 0.9463107638888889 Training loss: 2.3390853
Epoch: 772 Sparsity: 0.9554253472222223 Training loss: 2.386922
Epoch: 773 Sparsity: 0.960763888888889 Training loss: 2.4828384
Epoch: 774 Sparsity: 0.9541232638888889 Training loss: 2.4835424
Epoch: 775 Sparsity: 0.9346354166666666 Training loss: 2.4887607
Epoch: 776 Sparsity: 0.9432725694444445 Training loss: 2.633849
Epoch: 777 Sparsity: 0.9440972222222221 Training loss: 2.5275917
Epoch: 778 Sparsity: 0.941232638888889 Training loss: 2.388477
Epoch: 779 Sparsity: 0.9511284722222222 Training loss: 2.998677
Epoch: 780 Sparsity: 0.9486979166666666 Training loss: 2.4473653
Epoch: 781 Sparsity: 0.9602430555555556 Training loss: 2.7369766
Epoch: 782 Sparsity: 0.9283420138888889 Training loss: 2.392854
Epoch: 783 Sparsity: 0.9618923611111111 Training loss: 2.5305107
Epoch: 784 Sparsity: 0.9348958333333334 Training loss: 3.3316748
Epoch: 785 Sparsity: 0.961111111111111 Training loss: 2.5353606
Epoch: 786 Sparsity: 0.9583767361111113 Training loss: 2.9706874
Epoch: 787 Sparsity: 0.9329427083333334 Training loss: 2.5908997
Epoch: 788 Sparsity: 0.9658420138888889 Training loss: 2.6599686
Epoch: 789 Sparsity: 0.9348090277777779 Training loss: 4.044153
Epoch: 790 Sparsity: 0.9591145833333332 Training loss: 3.1006613
Epoch: 791 Sparsity: 0.9457899305555555 Training loss: 2.463992
Epoch: 792 Sparsity: 0.9511718749999998 Training loss: 2.6755342
Epoch: 793 Sparsity: 0.9680121527777779 Training loss: 2.4904578
Epoch: 794 Sparsity: 0.9512586805555555 Training loss: 2.4490895
Epoch: 795 Sparsity: 0.9629774305555555 Training loss: 2.4024422
Epoch: 796 Sparsity: 0.9416666666666668 Training loss: 2.5600865
Epoch: 797 Sparsity: 0.9387152777777779 Training loss: 2.3045096
Epoch: 798 Sparsity: 0.959157986111111 Training loss: 2.8590407
Epoch: 799 Sparsity: 0.950998263888889 Training loss: 2.3540895
Epoch: 800 Sparsity: 0.9470052083333333 Training loss: 2.426641
Epoch: 801 Sparsity: 0.943576388888889 Training loss: 2.3555846
Epoch: 802 Sparsity: 0.9641059027777779 Training loss: 3.3537774
Epoch: 803 Sparsity: 0.9410590277777778 Training loss: 2.5305855
Epoch: 804 Sparsity: 0.96796875 Training loss: 2.8958151
Epoch: 805 Sparsity: 0.9419270833333332 Training loss: 2.7196498
Epoch: 806 Sparsity: 0.9571614583333334 Training loss: 2.4085693
Epoch: 807 Sparsity: 0.959765625 Training loss: 2.3747528
Epoch: 808 Sparsity: 0.9260416666666667 Training loss: 3.1415887
Epoch: 809 Sparsity: 0.9436631944444442 Training loss: 2.7640214
Epoch: 810 Sparsity: 0.954904513888889 Training loss: 2.7163184
Epoch: 811 Sparsity: 0.9373697916666668 Training loss: 2.4434433
Epoch: 812 Sparsity: 0.9673177083333334 Training loss: 2.484409
Epoch: 813 Sparsity: 0.9446180555555556 Training loss: 4.8677535
Epoch: 814 Sparsity: 0.9565972222222223 Training loss: 2.4959111
Epoch: 815 Sparsity: 0.92421875 Training loss: 2.7354364
Epoch: 816 Sparsity: 0.9575954861111112 Training loss: 2.8884873
Epoch: 817 Sparsity: 0.9296006944444446 Training loss: 3.756911
Epoch: 818 Sparsity: 0.9598090277777777 Training loss: 2.8527486
Epoch: 819 Sparsity: 0.9375434027777778 Training loss: 2.491908
Epoch: 820 Sparsity: 0.9581163194444444 Training loss: 2.8050046
Epoch: 821 Sparsity: 0.9331163194444445 Training loss: 2.5275455
Epoch: 822 Sparsity: 0.9622829861111113 Training loss: 2.5488105
Epoch: 823 Sparsity: 0.9316840277777777 Training loss: 2.6830485
Epoch: 824 Sparsity: 0.9563802083333334 Training loss: 2.531645
Epoch: 825 Sparsity: 0.9515625 Training loss: 2.687948
Epoch: 826 Sparsity: 0.9100694444444445 Training loss: 2.731063
Epoch: 827 Sparsity: 0.949435763888889 Training loss: 3.6956902
Epoch: 828 Sparsity: 0.9622829861111111 Training loss: 2.5791728
Epoch: 829 Sparsity: 0.9321614583333332 Training loss: 2.5605164
Epoch: 830 Sparsity: 0.9578993055555557 Training loss: 2.465332
Epoch: 831 Sparsity: 0.931814236111111 Training loss: 2.4487846
Epoch: 832 Sparsity: 0.9564236111111111 Training loss: 2.6268778
Epoch: 833 Sparsity: 0.9450086805555555 Training loss: 2.3710697
Epoch: 834 Sparsity: 0.9684895833333333 Training loss: 2.4482307
Epoch: 835 Sparsity: 0.9259114583333334 Training loss: 2.5829616
Epoch: 836 Sparsity: 0.9579861111111111 Training loss: 3.5160089
Epoch: 837 Sparsity: 0.9312934027777777 Training loss: 2.6614442
Epoch: 838 Sparsity: 0.9573350694444447 Training loss: 2.6862218
Epoch: 839 Sparsity: 0.9271267361111111 Training loss: 2.6105072
Epoch: 840 Sparsity: 0.9365017361111111 Training loss: 2.344139
Epoch: 841 Sparsity: 0.9526909722222221 Training loss: 2.4173396
Epoch: 842 Sparsity: 0.9327690972222223 Training loss: 2.4001431
Epoch: 843 Sparsity: 0.9373697916666666 Training loss: 2.5626435
Epoch: 844 Sparsity: 0.95390625 Training loss: 2.5969102
Epoch: 845 Sparsity: 0.9247395833333334 Training loss: 2.4566956
Epoch: 846 Sparsity: 0.9506510416666666 Training loss: 2.6764205
Epoch: 847 Sparsity: 0.9443142361111112 Training loss: 2.3866255
Epoch: 848 Sparsity: 0.9252604166666668 Training loss: 2.645485
Epoch: 849 Sparsity: 0.959201388888889 Training loss: 2.7659912
Epoch: 850 Sparsity: 0.9472222222222222 Training loss: 2.4428077
Epoch: 851 Sparsity: 0.9477864583333334 Training loss: 2.7403116
Epoch: 852 Sparsity: 0.9271267361111113 Training loss: 2.552337
Epoch: 853 Sparsity: 0.9548177083333333 Training loss: 2.6166399
Epoch: 854 Sparsity: 0.9322482638888889 Training loss: 2.3703818
Epoch: 855 Sparsity: 0.961154513888889 Training loss: 2.8608375
Epoch: 856 Sparsity: 0.9418402777777779 Training loss: 2.4931955
Epoch: 857 Sparsity: 0.9542100694444444 Training loss: 2.5939991
Epoch: 858 Sparsity: 0.9224826388888887 Training loss: 2.3962846
Epoch: 859 Sparsity: 0.9595052083333334 Training loss: 2.6689134
Epoch: 860 Sparsity: 0.9620659722222221 Training loss: 2.7836006
Epoch: 861 Sparsity: 0.925954861111111 Training loss: 3.8109615
Epoch: 862 Sparsity: 0.9451822916666666 Training loss: 2.4464018
Epoch: 863 Sparsity: 0.9639322916666666 Training loss: 2.8627527
Epoch: 864 Sparsity: 0.9337239583333332 Training loss: 3.575601
Epoch: 865 Sparsity: 0.9608072916666666 Training loss: 2.5215418
Epoch: 866 Sparsity: 0.9369357638888889 Training loss: 3.0200167
Epoch: 867 Sparsity: 0.9555989583333334 Training loss: 2.6343453
Epoch: 868 Sparsity: 0.9236545138888888 Training loss: 2.7630537
Epoch: 869 Sparsity: 0.9541666666666668 Training loss: 2.5358405
Epoch: 870 Sparsity: 0.9556857638888889 Training loss: 2.7664518
Epoch: 871 Sparsity: 0.9271267361111111 Training loss: 2.3851097
Epoch: 872 Sparsity: 0.9608072916666666 Training loss: 2.8476293
Epoch: 873 Sparsity: 0.914453125 Training loss: 3.111633
Epoch: 874 Sparsity: 0.9160156250000002 Training loss: 3.4058828
Epoch: 875 Sparsity: 0.9440538194444444 Training loss: 2.6789377
Epoch: 876 Sparsity: 0.9079861111111113 Training loss: 2.527498
Epoch: 877 Sparsity: 0.9469184027777778 Training loss: 2.7446008
Epoch: 878 Sparsity: 0.9505208333333334 Training loss: 2.4718473
Epoch: 879 Sparsity: 0.9519965277777779 Training loss: 2.567479
Epoch: 880 Sparsity: 0.9439236111111111 Training loss: 2.3557642
Epoch: 881 Sparsity: 0.9464409722222223 Training loss: 2.3414233
Epoch: 882 Sparsity: 0.9555989583333334 Training loss: 2.4858868
Epoch: 883 Sparsity: 0.9484809027777779 Training loss: 2.61882
Epoch: 884 Sparsity: 0.9483506944444444 Training loss: 2.5093908
Epoch: 885 Sparsity: 0.9459635416666667 Training loss: 2.7399375
Epoch: 886 Sparsity: 0.9604600694444443 Training loss: 2.561357
Epoch: 887 Sparsity: 0.9425781250000002 Training loss: 2.3943465
Epoch: 888 Sparsity: 0.9653645833333334 Training loss: 2.64349
Epoch: 889 Sparsity: 0.9418836805555555 Training loss: 2.5912645
Epoch: 890 Sparsity: 0.9614149305555555 Training loss: 2.5460057
Epoch: 891 Sparsity: 0.9429687499999998 Training loss: 2.3615806
Epoch: 892 Sparsity: 0.962717013888889 Training loss: 2.423199
Epoch: 893 Sparsity: 0.9489583333333332 Training loss: 2.3466742
Epoch: 894 Sparsity: 0.9499999999999998 Training loss: 2.3233962
Epoch: 895 Sparsity: 0.9453993055555555 Training loss: 2.6752365
Epoch: 896 Sparsity: 0.9520399305555556 Training loss: 2.9580665
Epoch: 897 Sparsity: 0.968923611111111 Training loss: 2.4655466
Epoch: 898 Sparsity: 0.9505208333333334 Training loss: 2.5508451
Epoch: 899 Sparsity: 0.9579861111111112 Training loss: 2.4746387
Epoch: 900 Sparsity: 0.9629340277777777 Training loss: 2.5405715
Epoch: 901 Sparsity: 0.9273871527777778 Training loss: 3.5675242
Epoch: 902 Sparsity: 0.9606336805555555 Training loss: 3.370797
Epoch: 903 Sparsity: 0.9369791666666668 Training loss: 2.2997088
Epoch: 904 Sparsity: 0.9622395833333334 Training loss: 2.6382213
Epoch: 905 Sparsity: 0.941015625 Training loss: 2.601636
Epoch: 906 Sparsity: 0.9602430555555556 Training loss: 2.8403938
Epoch: 907 Sparsity: 0.935546875 Training loss: 2.6163747
Epoch: 908 Sparsity: 0.9624131944444445 Training loss: 2.4368932
Epoch: 909 Sparsity: 0.9272135416666668 Training loss: 2.9598875
Epoch: 910 Sparsity: 0.9565104166666666 Training loss: 2.5595067
Epoch: 911 Sparsity: 0.9207899305555556 Training loss: 4.407911
Epoch: 912 Sparsity: 0.9534288194444445 Training loss: 2.4147637
Epoch: 913 Sparsity: 0.9619357638888888 Training loss: 2.582272
Epoch: 914 Sparsity: 0.9351996527777778 Training loss: 3.5552588
Epoch: 915 Sparsity: 0.9326822916666668 Training loss: 2.361521
Epoch: 916 Sparsity: 0.9536892361111111 Training loss: 2.3750186
Epoch: 917 Sparsity: 0.9518229166666666 Training loss: 2.4696498
Epoch: 918 Sparsity: 0.9614583333333332 Training loss: 2.5022354
Epoch: 919 Sparsity: 0.9394097222222223 Training loss: 2.6950142
Epoch: 920 Sparsity: 0.9523003472222221 Training loss: 2.7148168
Epoch: 921 Sparsity: 0.973828125 Training loss: 2.4730184
Epoch: 922 Sparsity: 0.9536458333333334 Training loss: 4.579272
Epoch: 923 Sparsity: 0.9402777777777777 Training loss: 2.5004451
Epoch: 924 Sparsity: 0.9453559027777777 Training loss: 2.3955047
Epoch: 925 Sparsity: 0.9437934027777777 Training loss: 2.3132467
Epoch: 926 Sparsity: 0.9677517361111111 Training loss: 2.4079165
Epoch: 927 Sparsity: 0.9546440972222221 Training loss: 2.324866
Epoch: 928 Sparsity: 0.9677951388888889 Training loss: 2.8411841
Epoch: 929 Sparsity: 0.9598090277777779 Training loss: 2.46698
Epoch: 930 Sparsity: 0.9266059027777777 Training loss: 3.0899549
Epoch: 931 Sparsity: 0.9591145833333332 Training loss: 3.4598207
Epoch: 932 Sparsity: 0.9450520833333333 Training loss: 2.5143127
Epoch: 933 Sparsity: 0.9681857638888889 Training loss: 2.6340988
Epoch: 934 Sparsity: 0.9572916666666667 Training loss: 2.3823738
Epoch: 935 Sparsity: 0.9648003472222222 Training loss: 2.6069672
Epoch: 936 Sparsity: 0.934765625 Training loss: 2.3172853
Epoch: 937 Sparsity: 0.925564236111111 Training loss: 4.0118446
Epoch: 938 Sparsity: 0.9573784722222222 Training loss: 3.2541864
Epoch: 939 Sparsity: 0.9404513888888889 Training loss: 2.5899825
Epoch: 940 Sparsity: 0.9586371527777777 Training loss: 2.8793633
Epoch: 941 Sparsity: 0.972265625 Training loss: 2.4855378
Epoch: 942 Sparsity: 0.9499999999999998 Training loss: 3.6477675
Epoch: 943 Sparsity: 0.9692708333333334 Training loss: 2.8420217
Epoch: 944 Sparsity: 0.9355034722222222 Training loss: 2.691768
Epoch: 945 Sparsity: 0.963671875 Training loss: 2.4749703
Epoch: 946 Sparsity: 0.9352430555555555 Training loss: 3.7817254
Epoch: 947 Sparsity: 0.9618489583333332 Training loss: 2.7796316
Epoch: 948 Sparsity: 0.9368923611111113 Training loss: 2.5454352
Epoch: 949 Sparsity: 0.9628038194444445 Training loss: 2.6217256
Epoch: 950 Sparsity: 0.9501302083333332 Training loss: 2.428757
Epoch: 951 Sparsity: 0.950998263888889 Training loss: 2.6297157
Epoch: 952 Sparsity: 0.9356770833333332 Training loss: 2.3614082
Epoch: 953 Sparsity: 0.9650173611111109 Training loss: 2.6336286
Epoch: 954 Sparsity: 0.946875 Training loss: 2.3433883
Epoch: 955 Sparsity: 0.9582465277777779 Training loss: 2.3263133
Epoch: 956 Sparsity: 0.9276909722222222 Training loss: 2.4389231
Epoch: 957 Sparsity: 0.9571614583333332 Training loss: 2.9133043
Epoch: 958 Sparsity: 0.9576822916666666 Training loss: 2.3635237
Epoch: 959 Sparsity: 0.9506076388888889 Training loss: 2.3199317
Epoch: 960 Sparsity: 0.973046875 Training loss: 2.7669272
Epoch: 961 Sparsity: 0.949435763888889 Training loss: 2.4611182
Epoch: 962 Sparsity: 0.9684895833333332 Training loss: 2.7243316
Epoch: 963 Sparsity: 0.9551649305555555 Training loss: 2.703372
Epoch: 964 Sparsity: 0.9592881944444445 Training loss: 2.356147
Epoch: 965 Sparsity: 0.9733940972222221 Training loss: 2.8099597
Epoch: 966 Sparsity: 0.947482638888889 Training loss: 2.709346
Epoch: 967 Sparsity: 0.969921875 Training loss: 2.9258053
Epoch: 968 Sparsity: 0.9440538194444444 Training loss: 2.613247
Epoch: 969 Sparsity: 0.9646701388888888 Training loss: 2.5213802
Epoch: 970 Sparsity: 0.9363281249999998 Training loss: 2.5267243
Epoch: 971 Sparsity: 0.942578125 Training loss: 2.3555303
Epoch: 972 Sparsity: 0.9706163194444445 Training loss: 2.6977353
Epoch: 973 Sparsity: 0.9608506944444445 Training loss: 2.3284218
Epoch: 974 Sparsity: 0.93671875 Training loss: 2.5124578
Epoch: 975 Sparsity: 0.9686631944444446 Training loss: 2.738158
Epoch: 976 Sparsity: 0.9679253472222221 Training loss: 2.5319715
Epoch: 977 Sparsity: 0.9520833333333332 Training loss: 2.5617263
Epoch: 978 Sparsity: 0.9715277777777777 Training loss: 2.6123316
Epoch: 979 Sparsity: 0.949609375 Training loss: 2.774394
Epoch: 980 Sparsity: 0.9697048611111111 Training loss: 2.6461747
Epoch: 981 Sparsity: 0.9604166666666666 Training loss: 2.406104
Epoch: 982 Sparsity: 0.9642361111111111 Training loss: 2.4860356
Epoch: 983 Sparsity: 0.9345920138888889 Training loss: 3.8201935
Epoch: 984 Sparsity: 0.9684461805555555 Training loss: 2.9397264
Epoch: 985 Sparsity: 0.9546006944444445 Training loss: 2.5191329
Epoch: 986 Sparsity: 0.9609809027777777 Training loss: 2.6270761
Epoch: 987 Sparsity: 0.9626302083333333 Training loss: 2.5530522
Epoch: 988 Sparsity: 0.9547743055555555 Training loss: 2.3099253
Epoch: 989 Sparsity: 0.9693576388888889 Training loss: 2.5641763
Epoch: 990 Sparsity: 0.945138888888889 Training loss: 2.5532432
Epoch: 991 Sparsity: 0.9680989583333334 Training loss: 2.5049632
Epoch: 992 Sparsity: 0.969921875 Training loss: 2.5601811
Epoch: 993 Sparsity: 0.9581597222222223 Training loss: 2.6793456
Epoch: 994 Sparsity: 0.968923611111111 Training loss: 2.9112804
Epoch: 995 Sparsity: 0.9467447916666666 Training loss: 2.9543002
Epoch: 996 Sparsity: 0.9671874999999999 Training loss: 2.6757953
Epoch: 997 Sparsity: 0.9541666666666668 Training loss: 2.4106953
Epoch: 998 Sparsity: 0.9639756944444444 Training loss: 2.9210513
Epoch: 999 Sparsity: 0.9571180555555555 Training loss: 2.6426213
2019-10-27 21:48:13.241816: W tensorflow/python/util/util.cc:299] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
WARNING:tensorflow:From /home/mihaela_stoycheva/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:From /home/mihaela_stoycheva/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
Epoch: 1000 Sparsity: 0.9450520833333333 Training loss: 2.4042516
Epoch: 1001 Sparsity: 0.955859375 Training loss: 2.3809774
Epoch: 1002 Sparsity: 0.9468315972222223 Training loss: 2.4887624
Epoch: 1003 Sparsity: 0.9733506944444444 Training loss: 2.5513208
Epoch: 1004 Sparsity: 0.9335503472222223 Training loss: 3.756947
Epoch: 1005 Sparsity: 0.9638454861111109 Training loss: 2.494625
Epoch: 1006 Sparsity: 0.9486111111111111 Training loss: 2.6582015
Epoch: 1007 Sparsity: 0.9588107638888888 Training loss: 2.4036143
Epoch: 1008 Sparsity: 0.9432291666666668 Training loss: 2.5785525
Epoch: 1009 Sparsity: 0.9451822916666666 Training loss: 2.4703565
Epoch: 1010 Sparsity: 0.9717013888888889 Training loss: 3.3522651
Epoch: 1011 Sparsity: 0.9588975694444445 Training loss: 2.4634833
Epoch: 1012 Sparsity: 0.9688368055555555 Training loss: 2.4133353
Epoch: 1013 Sparsity: 0.9598524305555556 Training loss: 2.4814227
Epoch: 1014 Sparsity: 0.9379340277777777 Training loss: 3.8603518
Epoch: 1015 Sparsity: 0.9670572916666668 Training loss: 2.9101477
Epoch: 1016 Sparsity: 0.957421875 Training loss: 2.3079228
Epoch: 1017 Sparsity: 0.9771701388888889 Training loss: 2.6139174
Epoch: 1018 Sparsity: 0.9660590277777776 Training loss: 2.329892
Epoch: 1019 Sparsity: 0.9625000000000001 Training loss: 2.3231184
Epoch: 1020 Sparsity: 0.9719184027777776 Training loss: 3.1920285
Epoch: 1021 Sparsity: 0.9643663194444445 Training loss: 2.3035858
Epoch: 1022 Sparsity: 0.9725694444444445 Training loss: 2.4060276
Epoch: 1023 Sparsity: 0.9595920138888887 Training loss: 2.4419599
Epoch: 1024 Sparsity: 0.972482638888889 Training loss: 2.6468544
Epoch: 1025 Sparsity: 0.9686197916666668 Training loss: 2.5270855
Epoch: 1026 Sparsity: 0.9743489583333333 Training loss: 2.4394164
Epoch: 1027 Sparsity: 0.9727864583333334 Training loss: 2.3264678
Epoch: 1028 Sparsity: 0.9730468750000002 Training loss: 2.6781275
Epoch: 1029 Sparsity: 0.9508680555555555 Training loss: 2.4410722
Epoch: 1030 Sparsity: 0.9694444444444444 Training loss: 3.0303535
Epoch: 1031 Sparsity: 0.978515625 Training loss: 2.4120114
Epoch: 1032 Sparsity: 0.9777343750000002 Training loss: 2.355523
Epoch: 1033 Sparsity: 0.9634114583333334 Training loss: 2.451828
Epoch: 1034 Sparsity: 0.9750868055555555 Training loss: 3.1420548
Epoch: 1035 Sparsity: 0.9657118055555556 Training loss: 2.4514983
Epoch: 1036 Sparsity: 0.9825520833333334 Training loss: 2.5462532
Epoch: 1037 Sparsity: 0.9723524305555555 Training loss: 2.3282945
Epoch: 1038 Sparsity: 0.9702256944444445 Training loss: 2.477235
Epoch: 1039 Sparsity: 0.9803385416666668 Training loss: 2.5211842
Epoch: 1040 Sparsity: 0.9653645833333334 Training loss: 3.141621
Epoch: 1041 Sparsity: 0.968923611111111 Training loss: 2.3104188
Epoch: 1042 Sparsity: 0.9757378472222221 Training loss: 2.5304239
Epoch: 1043 Sparsity: 0.9465277777777776 Training loss: 3.1180143
Epoch: 1044 Sparsity: 0.9765624999999998 Training loss: 2.73108
Epoch: 1045 Sparsity: 0.9597222222222221 Training loss: 2.4810302
Epoch: 1046 Sparsity: 0.9788194444444445 Training loss: 2.475258
Epoch: 1047 Sparsity: 0.9770399305555555 Training loss: 2.5240195
Epoch: 1048 Sparsity: 0.9715277777777777 Training loss: 3.8657823
Epoch: 1049 Sparsity: 0.972482638888889 Training loss: 2.8350077
Epoch: 1050 Sparsity: 0.9562065972222223 Training loss: 2.862292
Epoch: 1051 Sparsity: 0.9679253472222223 Training loss: 3.176783
Epoch: 1052 Sparsity: 0.9463107638888889 Training loss: 2.610662
Epoch: 1053 Sparsity: 0.9717447916666668 Training loss: 3.22014
Epoch: 1054 Sparsity: 0.9478298611111112 Training loss: 2.6447573
Epoch: 1055 Sparsity: 0.9744791666666666 Training loss: 2.7882333
Epoch: 1056 Sparsity: 0.9646267361111113 Training loss: 2.4326873
Epoch: 1057 Sparsity: 0.9703125 Training loss: 2.4355824
Epoch: 1058 Sparsity: 0.9515625 Training loss: 3.360236
Epoch: 1059 Sparsity: 0.9692708333333334 Training loss: 2.6467006
Epoch: 1060 Sparsity: 0.978342013888889 Training loss: 2.4369035
Epoch: 1061 Sparsity: 0.9501302083333334 Training loss: 3.5288112
Epoch: 1062 Sparsity: 0.9716145833333334 Training loss: 2.4275577
Epoch: 1063 Sparsity: 0.9597222222222224 Training loss: 2.3898232
Epoch: 1064 Sparsity: 0.9678819444444444 Training loss: 2.3826728
Epoch: 1065 Sparsity: 0.9805555555555554 Training loss: 2.4305701
Epoch: 1066 Sparsity: 0.9563368055555556 Training loss: 4.06796
Epoch: 1067 Sparsity: 0.9769097222222223 Training loss: 2.487792
Epoch: 1068 Sparsity: 0.9724826388888888 Training loss: 2.4628782
Epoch: 1069 Sparsity: 0.9594184027777779 Training loss: 2.393686
Epoch: 1070 Sparsity: 0.9588975694444443 Training loss: 2.5235944
Epoch: 1071 Sparsity: 0.9545138888888889 Training loss: 2.8208482
Epoch: 1072 Sparsity: 0.9733072916666666 Training loss: 2.545456
Epoch: 1073 Sparsity: 0.9498697916666666 Training loss: 4.13575
Epoch: 1074 Sparsity: 0.9718315972222221 Training loss: 2.5458312
Epoch: 1075 Sparsity: 0.9649305555555557 Training loss: 2.326709
Epoch: 1076 Sparsity: 0.9639756944444444 Training loss: 2.4805522
Epoch: 1077 Sparsity: 0.9618489583333334 Training loss: 2.656253
Epoch: 1078 Sparsity: 0.9730034722222222 Training loss: 2.5126355
Epoch: 1079 Sparsity: 0.9493055555555555 Training loss: 2.438923
Epoch: 1080 Sparsity: 0.9622829861111111 Training loss: 2.5408406
Epoch: 1081 Sparsity: 0.9791232638888889 Training loss: 2.6638129
Epoch: 1082 Sparsity: 0.9556423611111112 Training loss: 3.1431417
Epoch: 1083 Sparsity: 0.9747395833333334 Training loss: 2.7587378
Epoch: 1084 Sparsity: 0.9504774305555556 Training loss: 2.8372078
Epoch: 1085 Sparsity: 0.970703125 Training loss: 2.7257836
Epoch: 1086 Sparsity: 0.9451822916666666 Training loss: 2.4997447
Epoch: 1087 Sparsity: 0.9718315972222223 Training loss: 2.5441086
Epoch: 1088 Sparsity: 0.9463975694444444 Training loss: 2.8407927
Epoch: 1089 Sparsity: 0.95625 Training loss: 2.4841447
Epoch: 1090 Sparsity: 0.9467881944444445 Training loss: 2.6713195
Epoch: 1091 Sparsity: 0.9700086805555556 Training loss: 2.5290434
Epoch: 1092 Sparsity: 0.9405815972222221 Training loss: 2.6625733
Epoch: 1093 Sparsity: 0.9675347222222221 Training loss: 2.802772
Epoch: 1094 Sparsity: 0.9397569444444445 Training loss: 2.7591395
Epoch: 1095 Sparsity: 0.9658854166666666 Training loss: 2.4646144
Epoch: 1096 Sparsity: 0.9561197916666666 Training loss: 2.4185793
Epoch: 1097 Sparsity: 0.9546006944444445 Training loss: 2.4300294
Epoch: 1098 Sparsity: 0.9713541666666667 Training loss: 2.884101
Epoch: 1099 Sparsity: 0.93984375 Training loss: 3.2530897
Epoch: 1100 Sparsity: 0.97109375 Training loss: 2.4559407
Epoch: 1101 Sparsity: 0.9576388888888889 Training loss: 2.3333802
Epoch: 1102 Sparsity: 0.962717013888889 Training loss: 2.6043901
Epoch: 1103 Sparsity: 0.9481336805555557 Training loss: 2.4441953
Epoch: 1104 Sparsity: 0.9517361111111111 Training loss: 2.3950417
Epoch: 1105 Sparsity: 0.9364583333333334 Training loss: 2.2995584
Epoch: 1106 Sparsity: 0.9684027777777777 Training loss: 3.589786
Epoch: 1107 Sparsity: 0.9651909722222223 Training loss: 2.624847
Epoch: 1108 Sparsity: 0.9678819444444443 Training loss: 2.76744
Epoch: 1109 Sparsity: 0.931857638888889 Training loss: 3.0323873
Epoch: 1110 Sparsity: 0.9662326388888889 Training loss: 3.1509638
Epoch: 1111 Sparsity: 0.9567708333333333 Training loss: 2.4925497
Epoch: 1112 Sparsity: 0.9685763888888888 Training loss: 2.8342853
Epoch: 1113 Sparsity: 0.9470052083333332 Training loss: 2.5249226
Epoch: 1114 Sparsity: 0.9739583333333333 Training loss: 2.7135146
Epoch: 1115 Sparsity: 0.96640625 Training loss: 2.5155268
Epoch: 1116 Sparsity: 0.9546874999999998 Training loss: 3.9289484
Epoch: 1117 Sparsity: 0.9714409722222224 Training loss: 2.7609224
Epoch: 1118 Sparsity: 0.9573784722222222 Training loss: 3.585354
Epoch: 1119 Sparsity: 0.9751302083333332 Training loss: 2.5213473
Epoch: 1120 Sparsity: 0.9535590277777777 Training loss: 2.4729166
Epoch: 1121 Sparsity: 0.9654513888888889 Training loss: 2.799957
Epoch: 1122 Sparsity: 0.9449652777777778 Training loss: 2.4287465
Epoch: 1123 Sparsity: 0.9713975694444443 Training loss: 2.5642817
Epoch: 1124 Sparsity: 0.9528645833333332 Training loss: 2.4168386
Epoch: 1125 Sparsity: 0.9736111111111112 Training loss: 2.9060504
Epoch: 1126 Sparsity: 0.9463107638888889 Training loss: 2.5319836
Epoch: 1127 Sparsity: 0.9647569444444443 Training loss: 2.402043
Epoch: 1128 Sparsity: 0.9535590277777779 Training loss: 2.647972
Epoch: 1129 Sparsity: 0.9532552083333332 Training loss: 2.3676903
Epoch: 1130 Sparsity: 0.9543402777777779 Training loss: 2.5337873
Epoch: 1131 Sparsity: 0.9737847222222221 Training loss: 2.5213678
Epoch: 1132 Sparsity: 0.96953125 Training loss: 2.3523269
Epoch: 1133 Sparsity: 0.9636718749999998 Training loss: 2.6351655
Epoch: 1134 Sparsity: 0.9403211805555557 Training loss: 2.4801817
Epoch: 1135 Sparsity: 0.9671875 Training loss: 2.4283285
Epoch: 1136 Sparsity: 0.9532552083333332 Training loss: 2.4299693
Epoch: 1137 Sparsity: 0.9762586805555558 Training loss: 2.60328
Epoch: 1138 Sparsity: 0.9542534722222221 Training loss: 3.3195815
Epoch: 1139 Sparsity: 0.9652777777777777 Training loss: 2.7214117
Epoch: 1140 Sparsity: 0.9624131944444445 Training loss: 2.4479623
Epoch: 1141 Sparsity: 0.9661024305555556 Training loss: 2.3786006
Epoch: 1142 Sparsity: 0.9519965277777779 Training loss: 2.3528259
Epoch: 1143 Sparsity: 0.9494357638888891 Training loss: 2.3651211
Epoch: 1144 Sparsity: 0.9802517361111113 Training loss: 2.640954
Epoch: 1145 Sparsity: 0.9598524305555556 Training loss: 2.8684704
Epoch: 1146 Sparsity: 0.9732204861111112 Training loss: 2.4821546
Epoch: 1147 Sparsity: 0.9391493055555555 Training loss: 2.6380105
Epoch: 1148 Sparsity: 0.9700086805555556 Training loss: 2.7166736
Epoch: 1149 Sparsity: 0.9406684027777776 Training loss: 2.9281468
Epoch: 1150 Sparsity: 0.9516493055555555 Training loss: 2.463523
Epoch: 1151 Sparsity: 0.9661458333333334 Training loss: 2.60165
Epoch: 1152 Sparsity: 0.9442708333333332 Training loss: 2.589462
Epoch: 1153 Sparsity: 0.9493055555555555 Training loss: 2.2956228
Epoch: 1154 Sparsity: 0.9662326388888891 Training loss: 2.6626093
Epoch: 1155 Sparsity: 0.9594618055555555 Training loss: 2.4465857
Epoch: 1156 Sparsity: 0.9661024305555556 Training loss: 2.7517967
Epoch: 1157 Sparsity: 0.9515190972222222 Training loss: 2.6813307
Epoch: 1158 Sparsity: 0.9758680555555556 Training loss: 2.6402907
Epoch: 1159 Sparsity: 0.958810763888889 Training loss: 2.6804266
Epoch: 1160 Sparsity: 0.968576388888889 Training loss: 2.6909392
Epoch: 1161 Sparsity: 0.9522569444444444 Training loss: 2.30362
Epoch: 1162 Sparsity: 0.9711371527777777 Training loss: 2.9112954
Epoch: 1163 Sparsity: 0.9747395833333334 Training loss: 2.3269851
Epoch: 1164 Sparsity: 0.9658854166666666 Training loss: 2.3066618
Epoch: 1165 Sparsity: 0.9796440972222221 Training loss: 2.9584575
Epoch: 1166 Sparsity: 0.94453125 Training loss: 2.969939
Epoch: 1167 Sparsity: 0.9751302083333334 Training loss: 2.4126792
Epoch: 1168 Sparsity: 0.9407552083333333 Training loss: 2.54384
Epoch: 1169 Sparsity: 0.9710069444444447 Training loss: 2.9057221
Epoch: 1170 Sparsity: 0.9755642361111111 Training loss: 2.7045984
Epoch: 1171 Sparsity: 0.9598090277777777 Training loss: 2.4751997
Epoch: 1172 Sparsity: 0.9726996527777777 Training loss: 2.4143221
Epoch: 1173 Sparsity: 0.9558159722222224 Training loss: 3.222081
Epoch: 1174 Sparsity: 0.9547309027777778 Training loss: 2.3143651
Epoch: 1175 Sparsity: 0.973046875 Training loss: 2.3840399
Epoch: 1176 Sparsity: 0.9654513888888889 Training loss: 2.3818543
Epoch: 1177 Sparsity: 0.9656684027777777 Training loss: 2.6112487
Epoch: 1178 Sparsity: 0.9609809027777777 Training loss: 2.2956622
Epoch: 1179 Sparsity: 0.9766927083333334 Training loss: 2.4461737
Epoch: 1180 Sparsity: 0.9654079861111111 Training loss: 2.3072672
Epoch: 1181 Sparsity: 0.9735243055555556 Training loss: 2.7090814
Epoch: 1182 Sparsity: 0.9448350694444443 Training loss: 4.508374
Epoch: 1183 Sparsity: 0.971484375 Training loss: 2.6307724
Epoch: 1184 Sparsity: 0.9628472222222223 Training loss: 2.5300288
Epoch: 1185 Sparsity: 0.9596354166666666 Training loss: 2.736453
Epoch: 1186 Sparsity: 0.973828125 Training loss: 2.5415614
Epoch: 1187 Sparsity: 0.961154513888889 Training loss: 2.6499035
Epoch: 1188 Sparsity: 0.9663194444444443 Training loss: 2.4287632
Epoch: 1189 Sparsity: 0.9654513888888887 Training loss: 2.3802054
Epoch: 1190 Sparsity: 0.9757378472222221 Training loss: 2.8823316
Epoch: 1191 Sparsity: 0.9630208333333334 Training loss: 2.603527
Epoch: 1192 Sparsity: 0.9729600694444445 Training loss: 2.5439928
Epoch: 1193 Sparsity: 0.9403211805555557 Training loss: 2.7301435
Epoch: 1194 Sparsity: 0.9694444444444444 Training loss: 2.973021
Epoch: 1195 Sparsity: 0.9635850694444444 Training loss: 2.3142793
Epoch: 1196 Sparsity: 0.9730902777777777 Training loss: 2.6254551
Epoch: 1197 Sparsity: 0.9460069444444443 Training loss: 2.5440407
Epoch: 1198 Sparsity: 0.9758680555555556 Training loss: 3.030988
Epoch: 1199 Sparsity: 0.9503906250000002 Training loss: 3.2359688
Epoch: 1200 Sparsity: 0.9756944444444444 Training loss: 2.8839352
Epoch: 1201 Sparsity: 0.9752604166666666 Training loss: 2.3178682
Epoch: 1202 Sparsity: 0.9766493055555555 Training loss: 2.5150805
Epoch: 1203 Sparsity: 0.9581597222222221 Training loss: 3.2483416
Epoch: 1204 Sparsity: 0.9695746527777777 Training loss: 2.4318385
Epoch: 1205 Sparsity: 0.9708333333333332 Training loss: 2.457902
Epoch: 1206 Sparsity: 0.9569878472222223 Training loss: 2.5597155
Epoch: 1207 Sparsity: 0.97109375 Training loss: 2.3495975
Epoch: 1208 Sparsity: 0.9695746527777779 Training loss: 2.3295445
Epoch: 1209 Sparsity: 0.9781684027777778 Training loss: 2.6813056
Epoch: 1210 Sparsity: 0.9568142361111113 Training loss: 2.6518593
Epoch: 1211 Sparsity: 0.9791232638888889 Training loss: 2.9890718
Epoch: 1212 Sparsity: 0.9642795138888889 Training loss: 3.1826713
Epoch: 1213 Sparsity: 0.9674479166666666 Training loss: 2.4791892
Epoch: 1214 Sparsity: 0.9586371527777778 Training loss: 2.356571
Epoch: 1215 Sparsity: 0.9778211805555556 Training loss: 2.5032957
Epoch: 1216 Sparsity: 0.9654079861111111 Training loss: 2.476242
Epoch: 1217 Sparsity: 0.9753472222222224 Training loss: 2.8644054
Epoch: 1218 Sparsity: 0.9734809027777777 Training loss: 2.401654
Epoch: 1219 Sparsity: 0.9819010416666666 Training loss: 2.4057975
Epoch: 1220 Sparsity: 0.9608940972222223 Training loss: 2.8876379
Epoch: 1221 Sparsity: 0.9795572916666668 Training loss: 2.4707446
Epoch: 1222 Sparsity: 0.9812934027777777 Training loss: 2.461921
Epoch: 1223 Sparsity: 0.9700954861111111 Training loss: 2.3970177
Epoch: 1224 Sparsity: 0.9788628472222222 Training loss: 2.905593
Epoch: 1225 Sparsity: 0.9648003472222223 Training loss: 2.3265088
Epoch: 1226 Sparsity: 0.9807291666666667 Training loss: 2.56702
Epoch: 1227 Sparsity: 0.9800347222222221 Training loss: 2.6174421
Epoch: 1228 Sparsity: 0.9662326388888889 Training loss: 2.3669808
Epoch: 1229 Sparsity: 0.9614583333333334 Training loss: 2.31193
Epoch: 1230 Sparsity: 0.9772569444444444 Training loss: 2.5892296
Epoch: 1231 Sparsity: 0.9768663194444442 Training loss: 2.4156682
Epoch: 1232 Sparsity: 0.9720920138888889 Training loss: 2.3861356
Epoch: 1233 Sparsity: 0.9651041666666668 Training loss: 2.2753873
Epoch: 1234 Sparsity: 0.9784722222222222 Training loss: 3.399064
Epoch: 1235 Sparsity: 0.9533854166666668 Training loss: 3.1406105
Epoch: 1236 Sparsity: 0.9736545138888889 Training loss: 3.5492885
Epoch: 1237 Sparsity: 0.9640625 Training loss: 2.3051102
Epoch: 1238 Sparsity: 0.9776475694444446 Training loss: 2.6070936
Epoch: 1239 Sparsity: 0.9611979166666667 Training loss: 2.8867905
Epoch: 1240 Sparsity: 0.9698350694444444 Training loss: 2.4120986
Epoch: 1241 Sparsity: 0.9726996527777778 Training loss: 2.3558104
Epoch: 1242 Sparsity: 0.9584635416666666 Training loss: 2.4716983
Epoch: 1243 Sparsity: 0.9605034722222223 Training loss: 2.306923
Epoch: 1244 Sparsity: 0.9732638888888889 Training loss: 2.944091
Epoch: 1245 Sparsity: 0.9673611111111111 Training loss: 2.3537514
Epoch: 1246 Sparsity: 0.9751302083333334 Training loss: 2.4335191
Epoch: 1247 Sparsity: 0.9797743055555556 Training loss: 2.679996
Epoch: 1248 Sparsity: 0.9631944444444445 Training loss: 3.432078
Epoch: 1249 Sparsity: 0.974435763888889 Training loss: 2.4812407
Epoch: 1250 Sparsity: 0.9664930555555555 Training loss: 2.5766337
Epoch: 1251 Sparsity: 0.9728732638888887 Training loss: 2.8668983
Epoch: 1252 Sparsity: 0.954861111111111 Training loss: 2.4516466
Epoch: 1253 Sparsity: 0.9706597222222223 Training loss: 2.4179194
Epoch: 1254 Sparsity: 0.9561631944444444 Training loss: 2.3956325
Epoch: 1255 Sparsity: 0.9635850694444444 Training loss: 2.646112
Epoch: 1256 Sparsity: 0.9734375 Training loss: 2.7528584
Epoch: 1257 Sparsity: 0.9523871527777779 Training loss: 2.3624175
Epoch: 1258 Sparsity: 0.9736545138888889 Training loss: 2.4952435
Epoch: 1259 Sparsity: 0.9510850694444445 Training loss: 2.956099
Epoch: 1260 Sparsity: 0.9725260416666666 Training loss: 2.4723768
Epoch: 1261 Sparsity: 0.9467447916666666 Training loss: 2.4054787
Epoch: 1262 Sparsity: 0.9527777777777777 Training loss: 2.4350898
Epoch: 1263 Sparsity: 0.9759548611111113 Training loss: 2.4292095
Epoch: 1264 Sparsity: 0.952734375 Training loss: 2.3711789
Epoch: 1265 Sparsity: 0.9510416666666666 Training loss: 2.470427
Epoch: 1266 Sparsity: 0.9589843750000002 Training loss: 2.3717587
Epoch: 1267 Sparsity: 0.9662760416666668 Training loss: 2.630066
Epoch: 1268 Sparsity: 0.9546440972222221 Training loss: 2.3229444
Epoch: 1269 Sparsity: 0.9766059027777778 Training loss: 2.452641
Epoch: 1270 Sparsity: 0.9621527777777776 Training loss: 3.1483488
Epoch: 1271 Sparsity: 0.9680555555555556 Training loss: 2.9434984
Epoch: 1272 Sparsity: 0.97421875 Training loss: 2.3894691
Epoch: 1273 Sparsity: 0.9710503472222223 Training loss: 2.3291516
Epoch: 1274 Sparsity: 0.9467013888888888 Training loss: 2.40566
Epoch: 1275 Sparsity: 0.9736979166666668 Training loss: 2.9230206
Epoch: 1276 Sparsity: 0.9693576388888887 Training loss: 2.3236833
Epoch: 1277 Sparsity: 0.9671006944444445 Training loss: 2.4542496
Epoch: 1278 Sparsity: 0.9768229166666667 Training loss: 2.5299988
Epoch: 1279 Sparsity: 0.9569444444444445 Training loss: 2.7852998
Epoch: 1280 Sparsity: 0.9773003472222221 Training loss: 2.734424
Epoch: 1281 Sparsity: 0.9840277777777777 Training loss: 2.5490725
Epoch: 1282 Sparsity: 0.972265625 Training loss: 2.346188
Epoch: 1283 Sparsity: 0.9654079861111111 Training loss: 2.3934462
Epoch: 1284 Sparsity: 0.9708333333333332 Training loss: 2.3344073
Epoch: 1285 Sparsity: 0.983203125 Training loss: 2.4699907
Epoch: 1286 Sparsity: 0.967578125 Training loss: 4.9440966
Epoch: 1287 Sparsity: 0.9808159722222222 Training loss: 2.975625
Epoch: 1288 Sparsity: 0.9557725694444444 Training loss: 3.6129365
Epoch: 1289 Sparsity: 0.9754774305555556 Training loss: 2.7815912
Epoch: 1290 Sparsity: 0.961154513888889 Training loss: 2.5396247
Epoch: 1291 Sparsity: 0.9726128472222222 Training loss: 2.6797879
Epoch: 1292 Sparsity: 0.9596354166666666 Training loss: 2.6064715
Epoch: 1293 Sparsity: 0.977734375 Training loss: 2.5939708
Epoch: 1294 Sparsity: 0.9655815972222221 Training loss: 2.604519
Epoch: 1295 Sparsity: 0.9725694444444445 Training loss: 2.6434696
Epoch: 1296 Sparsity: 0.9526475694444443 Training loss: 2.483983
Epoch: 1297 Sparsity: 0.9713541666666667 Training loss: 2.544687
Epoch: 1298 Sparsity: 0.974826388888889 Training loss: 2.3627183
Epoch: 1299 Sparsity: 0.9477864583333334 Training loss: 3.0081697
Epoch: 1300 Sparsity: 0.9721354166666666 Training loss: 3.2726405
Epoch: 1301 Sparsity: 0.962673611111111 Training loss: 2.4571307
Epoch: 1302 Sparsity: 0.9771267361111111 Training loss: 2.5620842
Epoch: 1303 Sparsity: 0.9635850694444444 Training loss: 2.8847399
Epoch: 1304 Sparsity: 0.9775173611111111 Training loss: 2.9883478
Epoch: 1305 Sparsity: 0.9674913194444444 Training loss: 2.64572
Epoch: 1306 Sparsity: 0.9779513888888888 Training loss: 2.8431385
Epoch: 1307 Sparsity: 0.9702690972222221 Training loss: 2.5466182
Epoch: 1308 Sparsity: 0.973828125 Training loss: 2.452061
Epoch: 1309 Sparsity: 0.9451822916666666 Training loss: 2.9470906
Epoch: 1310 Sparsity: 0.97421875 Training loss: 3.1154017
Epoch: 1311 Sparsity: 0.9631510416666667 Training loss: 2.4556625
Epoch: 1312 Sparsity: 0.9644097222222223 Training loss: 2.3026724
Epoch: 1313 Sparsity: 0.9701388888888889 Training loss: 2.537689
Epoch: 1314 Sparsity: 0.982421875 Training loss: 2.567431
Epoch: 1315 Sparsity: 0.970703125 Training loss: 3.7849643
Epoch: 1316 Sparsity: 0.9818142361111111 Training loss: 2.417449
Epoch: 1317 Sparsity: 0.9656684027777779 Training loss: 3.5365212
Epoch: 1318 Sparsity: 0.976953125 Training loss: 2.5256562
Epoch: 1319 Sparsity: 0.9736111111111111 Training loss: 2.3346071
Epoch: 1320 Sparsity: 0.9818576388888889 Training loss: 2.9729316
Epoch: 1321 Sparsity: 0.9786024305555555 Training loss: 2.5742886
Epoch: 1322 Sparsity: 0.9497829861111112 Training loss: 2.9004924
Epoch: 1323 Sparsity: 0.9793402777777777 Training loss: 2.922873
Epoch: 1324 Sparsity: 0.9702690972222223 Training loss: 3.5432801
Epoch: 1325 Sparsity: 0.9681857638888889 Training loss: 2.4212203
Epoch: 1326 Sparsity: 0.9858940972222221 Training loss: 2.6801288
Epoch: 1327 Sparsity: 0.9684895833333333 Training loss: 5.2580485
Epoch: 1328 Sparsity: 0.963064236111111 Training loss: 2.363157
Epoch: 1329 Sparsity: 0.9775607638888889 Training loss: 2.774823
Epoch: 1330 Sparsity: 0.9649739583333334 Training loss: 2.470308
Epoch: 1331 Sparsity: 0.9723524305555555 Training loss: 2.36301
Epoch: 1332 Sparsity: 0.970876736111111 Training loss: 2.5336008
Epoch: 1333 Sparsity: 0.9716579861111111 Training loss: 2.6582136
Epoch: 1334 Sparsity: 0.9622829861111113 Training loss: 2.5439906
Epoch: 1335 Sparsity: 0.9809027777777779 Training loss: 2.654721
Epoch: 1336 Sparsity: 0.9633680555555557 Training loss: 3.4210665
Epoch: 1337 Sparsity: 0.9647135416666668 Training loss: 2.3270068
Epoch: 1338 Sparsity: 0.9740017361111111 Training loss: 2.3322952
Epoch: 1339 Sparsity: 0.9717447916666668 Training loss: 2.5162418
Epoch: 1340 Sparsity: 0.984592013888889 Training loss: 2.5199668
Epoch: 1341 Sparsity: 0.9688802083333334 Training loss: 3.3044145
Epoch: 1342 Sparsity: 0.9647135416666666 Training loss: 2.3325195
Epoch: 1343 Sparsity: 0.9786024305555555 Training loss: 3.0996017
Epoch: 1344 Sparsity: 0.966015625 Training loss: 2.4107277
Epoch: 1345 Sparsity: 0.9825086805555557 Training loss: 2.6351702
Epoch: 1346 Sparsity: 0.9637152777777779 Training loss: 2.8464086
Epoch: 1347 Sparsity: 0.9777777777777776 Training loss: 2.6674047
Epoch: 1348 Sparsity: 0.9821614583333333 Training loss: 2.533382
Epoch: 1349 Sparsity: 0.9680555555555556 Training loss: 2.8351219
Epoch: 1350 Sparsity: 0.9758246527777776 Training loss: 2.644324
Epoch: 1351 Sparsity: 0.968576388888889 Training loss: 2.6039534
Epoch: 1352 Sparsity: 0.9766927083333332 Training loss: 2.5366073
Epoch: 1353 Sparsity: 0.9707465277777778 Training loss: 2.4169
Epoch: 1354 Sparsity: 0.9761284722222221 Training loss: 2.3101442
Epoch: 1355 Sparsity: 0.9886718750000002 Training loss: 2.4887774
Epoch: 1356 Sparsity: 0.9756510416666668 Training loss: 4.8273406
Epoch: 1357 Sparsity: 0.984765625 Training loss: 2.869003
Epoch: 1358 Sparsity: 0.9678819444444444 Training loss: 2.5639908
Epoch: 1359 Sparsity: 0.9766059027777777 Training loss: 2.349242
Epoch: 1360 Sparsity: 0.9854166666666666 Training loss: 2.5427308
Epoch: 1361 Sparsity: 0.9582899305555556 Training loss: 3.289231
Epoch: 1362 Sparsity: 0.9783854166666666 Training loss: 2.5599267
Epoch: 1363 Sparsity: 0.957248263888889 Training loss: 2.9250383
Epoch: 1364 Sparsity: 0.9746961805555555 Training loss: 2.3658175
Epoch: 1365 Sparsity: 0.9746961805555555 Training loss: 2.4714367
Epoch: 1366 Sparsity: 0.9668402777777778 Training loss: 2.44988
Epoch: 1367 Sparsity: 0.9707465277777778 Training loss: 2.5404608
Epoch: 1368 Sparsity: 0.9853732638888889 Training loss: 2.979713
Epoch: 1369 Sparsity: 0.9696614583333332 Training loss: 4.6530657
Epoch: 1370 Sparsity: 0.9848524305555557 Training loss: 2.9024103
Epoch: 1371 Sparsity: 0.9733072916666667 Training loss: 3.5655284
Epoch: 1372 Sparsity: 0.9766927083333334 Training loss: 2.4024832
Epoch: 1373 Sparsity: 0.9691840277777777 Training loss: 2.5887797
Epoch: 1374 Sparsity: 0.9815972222222223 Training loss: 2.7738645
Epoch: 1375 Sparsity: 0.9717447916666666 Training loss: 2.5789616
Epoch: 1376 Sparsity: 0.9833767361111111 Training loss: 2.5807865
Epoch: 1377 Sparsity: 0.9680121527777779 Training loss: 2.896529
Epoch: 1378 Sparsity: 0.9803819444444445 Training loss: 2.9938414
Epoch: 1379 Sparsity: 0.9828993055555555 Training loss: 2.4646254
Epoch: 1380 Sparsity: 0.9633680555555555 Training loss: 4.115137
Epoch: 1381 Sparsity: 0.9665364583333333 Training loss: 2.3147457
Epoch: 1382 Sparsity: 0.9825520833333332 Training loss: 3.1600115
Epoch: 1383 Sparsity: 0.966623263888889 Training loss: 2.7329886
Epoch: 1384 Sparsity: 0.9827256944444442 Training loss: 3.0456963
Epoch: 1385 Sparsity: 0.9586371527777777 Training loss: 2.7498972
Epoch: 1386 Sparsity: 0.98125 Training loss: 2.6958477
Epoch: 1387 Sparsity: 0.9690972222222222 Training loss: 2.4369524
Epoch: 1388 Sparsity: 0.9692274305555555 Training loss: 2.3646472
Epoch: 1389 Sparsity: 0.98359375 Training loss: 2.7115147
Epoch: 1390 Sparsity: 0.9710069444444445 Training loss: 2.523471
Epoch: 1391 Sparsity: 0.9814670138888888 Training loss: 2.4017375
Epoch: 1392 Sparsity: 0.9681423611111111 Training loss: 2.6025987
Epoch: 1393 Sparsity: 0.9790798611111112 Training loss: 2.5717006
Epoch: 1394 Sparsity: 0.9662760416666668 Training loss: 3.1286767
Epoch: 1395 Sparsity: 0.9802083333333333 Training loss: 2.656869
Epoch: 1396 Sparsity: 0.9629774305555555 Training loss: 2.5366685
Epoch: 1397 Sparsity: 0.9664496527777777 Training loss: 2.319074
Epoch: 1398 Sparsity: 0.963064236111111 Training loss: 2.2853606
Epoch: 1399 Sparsity: 0.9771267361111111 Training loss: 2.4049342
Epoch: 1400 Sparsity: 0.9766059027777778 Training loss: 2.437781
Epoch: 1401 Sparsity: 0.9680989583333334 Training loss: 2.3000124
Epoch: 1402 Sparsity: 0.9707465277777778 Training loss: 2.3649893
Epoch: 1403 Sparsity: 0.9822482638888888 Training loss: 2.7105472
Epoch: 1404 Sparsity: 0.9655815972222221 Training loss: 3.330405
Epoch: 1405 Sparsity: 0.976736111111111 Training loss: 2.546945
Epoch: 1406 Sparsity: 0.9734375 Training loss: 2.4160347
Epoch: 1407 Sparsity: 0.9677517361111111 Training loss: 2.4313407
Epoch: 1408 Sparsity: 0.9851996527777779 Training loss: 2.539728
Epoch: 1409 Sparsity: 0.974782986111111 Training loss: 2.3805768
Epoch: 1410 Sparsity: 0.9852430555555556 Training loss: 2.6400874
Epoch: 1411 Sparsity: 0.9757378472222223 Training loss: 2.6235876
Epoch: 1412 Sparsity: 0.984765625 Training loss: 2.5514107
Epoch: 1413 Sparsity: 0.9703125 Training loss: 3.236935
Epoch: 1414 Sparsity: 0.9840277777777778 Training loss: 2.5583644
Epoch: 1415 Sparsity: 0.9696180555555556 Training loss: 2.8613455
Epoch: 1416 Sparsity: 0.9774739583333334 Training loss: 2.823765
Epoch: 1417 Sparsity: 0.9694878472222224 Training loss: 2.3153398
Epoch: 1418 Sparsity: 0.9808159722222223 Training loss: 2.4998915
Epoch: 1419 Sparsity: 0.9629774305555555 Training loss: 2.8640206
Epoch: 1420 Sparsity: 0.9784288194444445 Training loss: 2.3811831
Epoch: 1421 Sparsity: 0.9693142361111111 Training loss: 2.6036048
Epoch: 1422 Sparsity: 0.98359375 Training loss: 2.4910274
Epoch: 1423 Sparsity: 0.9696614583333334 Training loss: 2.631437
Epoch: 1424 Sparsity: 0.9799479166666666 Training loss: 2.5221634
Epoch: 1425 Sparsity: 0.970486111111111 Training loss: 2.3843477
Epoch: 1426 Sparsity: 0.9835069444444444 Training loss: 2.4870307
Epoch: 1427 Sparsity: 0.9671006944444445 Training loss: 2.4383047
Epoch: 1428 Sparsity: 0.98125 Training loss: 2.601066
Epoch: 1429 Sparsity: 0.9751736111111112 Training loss: 2.3314197
Epoch: 1430 Sparsity: 0.9654079861111112 Training loss: 2.526707
Epoch: 1431 Sparsity: 0.9847656250000002 Training loss: 2.8036292
Epoch: 1432 Sparsity: 0.9722222222222223 Training loss: 2.7263772
Epoch: 1433 Sparsity: 0.980859375 Training loss: 2.8588262
Epoch: 1434 Sparsity: 0.9702690972222223 Training loss: 2.526668
Epoch: 1435 Sparsity: 0.9865885416666668 Training loss: 2.7994425
Epoch: 1436 Sparsity: 0.9723090277777778 Training loss: 3.2721665
Epoch: 1437 Sparsity: 0.9823350694444443 Training loss: 2.4952948
Epoch: 1438 Sparsity: 0.96875 Training loss: 3.4659812
Epoch: 1439 Sparsity: 0.961111111111111 Training loss: 2.3034616
Epoch: 1440 Sparsity: 0.9794704861111111 Training loss: 2.9823606
Epoch: 1441 Sparsity: 0.9786024305555555 Training loss: 2.3912554
Epoch: 1442 Sparsity: 0.9805121527777778 Training loss: 2.515085
Epoch: 1443 Sparsity: 0.9739149305555557 Training loss: 2.6696181
Epoch: 1444 Sparsity: 0.9746527777777777 Training loss: 2.4530866
Epoch: 1445 Sparsity: 0.9783854166666666 Training loss: 2.3357043
Epoch: 1446 Sparsity: 0.9835069444444444 Training loss: 2.4059615
Epoch: 1447 Sparsity: 0.9692708333333332 Training loss: 4.98115
Epoch: 1448 Sparsity: 0.9865017361111112 Training loss: 2.6726956
Epoch: 1449 Sparsity: 0.9661024305555556 Training loss: 2.9042592
Epoch: 1450 Sparsity: 0.9718315972222223 Training loss: 2.4848154
Epoch: 1451 Sparsity: 0.9499565972222224 Training loss: 2.8611274
Epoch: 1452 Sparsity: 0.9787760416666667 Training loss: 2.9004982
Epoch: 1453 Sparsity: 0.9574652777777779 Training loss: 2.7782526
Epoch: 1454 Sparsity: 0.9753038194444444 Training loss: 2.4929266
Epoch: 1455 Sparsity: 0.9602864583333334 Training loss: 2.6972072
Epoch: 1456 Sparsity: 0.9776475694444444 Training loss: 3.002195
Epoch: 1457 Sparsity: 0.9704427083333332 Training loss: 2.5119312
Epoch: 1458 Sparsity: 0.9819878472222221 Training loss: 2.4939919
Epoch: 1459 Sparsity: 0.9662326388888889 Training loss: 2.7036834
Epoch: 1460 Sparsity: 0.9799479166666668 Training loss: 2.5995195
Epoch: 1461 Sparsity: 0.9687065972222222 Training loss: 2.3320394
Epoch: 1462 Sparsity: 0.9826388888888887 Training loss: 2.6940038
Epoch: 1463 Sparsity: 0.9745659722222223 Training loss: 2.3087308
Epoch: 1464 Sparsity: 0.9641493055555556 Training loss: 2.3246448
Epoch: 1465 Sparsity: 0.9820746527777778 Training loss: 2.6562676
Epoch: 1466 Sparsity: 0.9830729166666667 Training loss: 2.5792575
Epoch: 1467 Sparsity: 0.9730902777777779 Training loss: 4.8044124
Epoch: 1468 Sparsity: 0.981640625 Training loss: 2.4829094
Epoch: 1469 Sparsity: 0.9672743055555555 Training loss: 2.62311
Epoch: 1470 Sparsity: 0.9799479166666666 Training loss: 2.5807273
Epoch: 1471 Sparsity: 0.9758680555555556 Training loss: 2.3595014
Epoch: 1472 Sparsity: 0.9817708333333334 Training loss: 2.3612723
Epoch: 1473 Sparsity: 0.9796875 Training loss: 2.361056
Epoch: 1474 Sparsity: 0.9674479166666666 Training loss: 2.4763544
Epoch: 1475 Sparsity: 0.9807725694444445 Training loss: 2.559956
Epoch: 1476 Sparsity: 0.9855034722222221 Training loss: 2.3806093
Epoch: 1477 Sparsity: 0.9746961805555554 Training loss: 2.8523285
Epoch: 1478 Sparsity: 0.9733506944444444 Training loss: 2.3243668
Epoch: 1479 Sparsity: 0.9826822916666667 Training loss: 2.7231987
Epoch: 1480 Sparsity: 0.9670572916666667 Training loss: 3.3736975
Epoch: 1481 Sparsity: 0.9734375 Training loss: 2.4625583
Epoch: 1482 Sparsity: 0.9744791666666666 Training loss: 2.6157196
Epoch: 1483 Sparsity: 0.9819878472222221 Training loss: 2.743317
Epoch: 1484 Sparsity: 0.9652777777777779 Training loss: 2.8385623
Epoch: 1485 Sparsity: 0.9772569444444444 Training loss: 2.4295235
Epoch: 1486 Sparsity: 0.9825086805555555 Training loss: 2.4945967
Epoch: 1487 Sparsity: 0.9643663194444445 Training loss: 2.7289536
Epoch: 1488 Sparsity: 0.98125 Training loss: 2.984941
Epoch: 1489 Sparsity: 0.9569444444444445 Training loss: 2.8179202
Epoch: 1490 Sparsity: 0.9740885416666666 Training loss: 3.224253
Epoch: 1491 Sparsity: 0.9803819444444445 Training loss: 2.4860268
Epoch: 1492 Sparsity: 0.9868489583333334 Training loss: 2.4368355
Epoch: 1493 Sparsity: 0.9705295138888888 Training loss: 3.3953373
Epoch: 1494 Sparsity: 0.976388888888889 Training loss: 2.5497289
Epoch: 1495 Sparsity: 0.9578559027777777 Training loss: 2.6220293
Epoch: 1496 Sparsity: 0.9581597222222221 Training loss: 2.368051
Epoch: 1497 Sparsity: 0.9778645833333334 Training loss: 3.6521356
Epoch: 1498 Sparsity: 0.9598524305555556 Training loss: 2.6766994
Epoch: 1499 Sparsity: 0.9756510416666666 Training loss: 2.7078714
Epoch: 1500 Sparsity: 0.972829861111111 Training loss: 2.2999818
Epoch: 1501 Sparsity: 0.9739149305555556 Training loss: 2.371564
Epoch: 1502 Sparsity: 0.9768229166666668 Training loss: 2.8022103
Epoch: 1503 Sparsity: 0.9766493055555557 Training loss: 2.6515958
Epoch: 1504 Sparsity: 0.9617621527777779 Training loss: 2.4837232
Epoch: 1505 Sparsity: 0.9742621527777778 Training loss: 3.1054308
Epoch: 1506 Sparsity: 0.9537760416666667 Training loss: 2.3517644
Epoch: 1507 Sparsity: 0.9758246527777779 Training loss: 2.7548814
Epoch: 1508 Sparsity: 0.9675781249999998 Training loss: 2.4715614
Epoch: 1509 Sparsity: 0.9768229166666667 Training loss: 2.559324
Epoch: 1510 Sparsity: 0.96484375 Training loss: 2.8693035
Epoch: 1511 Sparsity: 0.9693576388888887 Training loss: 2.4536903
Epoch: 1512 Sparsity: 0.9778645833333333 Training loss: 2.799647
Epoch: 1513 Sparsity: 0.9693142361111111 Training loss: 2.8214874
Epoch: 1514 Sparsity: 0.9700520833333334 Training loss: 2.3444963
Epoch: 1515 Sparsity: 0.9814236111111111 Training loss: 2.581692
Epoch: 1516 Sparsity: 0.964670138888889 Training loss: 2.3802764
Epoch: 1517 Sparsity: 0.9824652777777777 Training loss: 2.8195891
Epoch: 1518 Sparsity: 0.9702256944444443 Training loss: 4.0820208
Epoch: 1519 Sparsity: 0.9606336805555558 Training loss: 2.366867
Epoch: 1520 Sparsity: 0.9815972222222221 Training loss: 2.5232778
Epoch: 1521 Sparsity: 0.966579861111111 Training loss: 2.4296434
Epoch: 1522 Sparsity: 0.9705729166666668 Training loss: 2.4741971
Epoch: 1523 Sparsity: 0.976171875 Training loss: 2.8284192
Epoch: 1524 Sparsity: 0.9574652777777777 Training loss: 2.391189
Epoch: 1525 Sparsity: 0.9760850694444445 Training loss: 3.2269444
Epoch: 1526 Sparsity: 0.9572916666666667 Training loss: 2.6733584
Epoch: 1527 Sparsity: 0.981640625 Training loss: 2.6924973
Epoch: 1528 Sparsity: 0.9639756944444444 Training loss: 2.6720011
Epoch: 1529 Sparsity: 0.9786458333333334 Training loss: 2.9306417
Epoch: 1530 Sparsity: 0.9638020833333332 Training loss: 2.57005
Epoch: 1531 Sparsity: 0.974435763888889 Training loss: 2.5777245
Epoch: 1532 Sparsity: 0.9804253472222222 Training loss: 2.755887
Epoch: 1533 Sparsity: 0.9658420138888889 Training loss: 4.471155
Epoch: 1534 Sparsity: 0.9637152777777779 Training loss: 2.6513207
Epoch: 1535 Sparsity: 0.9685763888888891 Training loss: 2.4882154
Epoch: 1536 Sparsity: 0.9789930555555555 Training loss: 2.354068
Epoch: 1537 Sparsity: 0.9703993055555555 Training loss: 2.580899
Epoch: 1538 Sparsity: 0.9776909722222223 Training loss: 2.5004358
Epoch: 1539 Sparsity: 0.9789062499999999 Training loss: 2.8154998
Epoch: 1540 Sparsity: 0.9667100694444443 Training loss: 2.9356046
Epoch: 1541 Sparsity: 0.9754340277777779 Training loss: 4.0590277
Epoch: 1542 Sparsity: 0.9735243055555556 Training loss: 2.3864493
Epoch: 1543 Sparsity: 0.9630642361111112 Training loss: 2.439069
Epoch: 1544 Sparsity: 0.97890625 Training loss: 2.5408044
Epoch: 1545 Sparsity: 0.9756944444444444 Training loss: 2.7234905
Epoch: 1546 Sparsity: 0.986111111111111 Training loss: 2.472042
Epoch: 1547 Sparsity: 0.9710503472222222 Training loss: 3.837262
Epoch: 1548 Sparsity: 0.9803385416666668 Training loss: 3.0947032
Epoch: 1549 Sparsity: 0.9690538194444445 Training loss: 2.3837893
Epoch: 1550 Sparsity: 0.9857638888888889 Training loss: 3.074489
Epoch: 1551 Sparsity: 0.9642795138888889 Training loss: 2.5448365
Epoch: 1552 Sparsity: 0.9851128472222224 Training loss: 2.5347419
Epoch: 1553 Sparsity: 0.9662760416666666 Training loss: 4.1943355
Epoch: 1554 Sparsity: 0.9756076388888889 Training loss: 2.873371
Epoch: 1555 Sparsity: 0.9685329861111113 Training loss: 2.539595
Epoch: 1556 Sparsity: 0.9820746527777777 Training loss: 2.4133449
Epoch: 1557 Sparsity: 0.96484375 Training loss: 2.699884
Epoch: 1558 Sparsity: 0.9764322916666666 Training loss: 2.6244206
Epoch: 1559 Sparsity: 0.9581163194444444 Training loss: 2.8001626
Epoch: 1560 Sparsity: 0.9765190972222222 Training loss: 2.9294174
Epoch: 1561 Sparsity: 0.9680989583333333 Training loss: 2.688796
Epoch: 1562 Sparsity: 0.9739583333333334 Training loss: 2.3251388
Epoch: 1563 Sparsity: 0.9774739583333334 Training loss: 2.382511
Epoch: 1564 Sparsity: 0.986545138888889 Training loss: 2.5482774
Epoch: 1565 Sparsity: 0.9738715277777779 Training loss: 2.5470254
Epoch: 1566 Sparsity: 0.9780381944444443 Training loss: 2.4267523
Epoch: 1567 Sparsity: 0.9802083333333333 Training loss: 2.581959
Epoch: 1568 Sparsity: 0.9789496527777779 Training loss: 2.694923
Epoch: 1569 Sparsity: 0.9878038194444445 Training loss: 2.6757624
Epoch: 1570 Sparsity: 0.9796006944444444 Training loss: 2.5993178
Epoch: 1571 Sparsity: 0.9805989583333332 Training loss: 3.1643372
Epoch: 1572 Sparsity: 0.9667534722222223 Training loss: 2.5483258
Epoch: 1573 Sparsity: 0.9825520833333334 Training loss: 2.8688767
Epoch: 1574 Sparsity: 0.9626302083333332 Training loss: 2.3317156
Epoch: 1575 Sparsity: 0.9834635416666666 Training loss: 2.6736488
Epoch: 1576 Sparsity: 0.976171875 Training loss: 2.4921308
Epoch: 1577 Sparsity: 0.9822048611111113 Training loss: 2.531989
Epoch: 1578 Sparsity: 0.972482638888889 Training loss: 3.8540986
Epoch: 1579 Sparsity: 0.9822048611111113 Training loss: 2.7615395
Epoch: 1580 Sparsity: 0.96875 Training loss: 2.3678932
Epoch: 1581 Sparsity: 0.9845052083333334 Training loss: 2.3691874
Epoch: 1582 Sparsity: 0.9672309027777777 Training loss: 3.281479
Epoch: 1583 Sparsity: 0.9827690972222222 Training loss: 3.037415
Epoch: 1584 Sparsity: 0.9802083333333332 Training loss: 2.3247514
Epoch: 1585 Sparsity: 0.9829861111111111 Training loss: 2.5265834
Epoch: 1586 Sparsity: 0.9681857638888889 Training loss: 2.731899
Epoch: 1587 Sparsity: 0.972873263888889 Training loss: 2.439262
Epoch: 1588 Sparsity: 0.9835069444444443 Training loss: 2.5594428
Epoch: 1589 Sparsity: 0.9716579861111111 Training loss: 2.6341064
Epoch: 1590 Sparsity: 0.9781684027777777 Training loss: 3.393942
Epoch: 1591 Sparsity: 0.9633680555555555 Training loss: 2.6605375
Epoch: 1592 Sparsity: 0.980859375 Training loss: 2.5186577
Epoch: 1593 Sparsity: 0.9699652777777779 Training loss: 2.4275491
Epoch: 1594 Sparsity: 0.9795572916666668 Training loss: 2.8821845
Epoch: 1595 Sparsity: 0.9773871527777779 Training loss: 2.443317
Epoch: 1596 Sparsity: 0.9680121527777779 Training loss: 2.355899
Epoch: 1597 Sparsity: 0.9828993055555557 Training loss: 2.7971754
Epoch: 1598 Sparsity: 0.9853298611111112 Training loss: 2.6331673
Epoch: 1599 Sparsity: 0.9852430555555556 Training loss: 2.6931581
Epoch: 1600 Sparsity: 0.9670572916666667 Training loss: 2.954572
Epoch: 1601 Sparsity: 0.980642361111111 Training loss: 3.0380068
Epoch: 1602 Sparsity: 0.9690538194444445 Training loss: 2.3314621
Epoch: 1603 Sparsity: 0.9758246527777776 Training loss: 2.6353235
Epoch: 1604 Sparsity: 0.9774739583333334 Training loss: 2.3191147
Epoch: 1605 Sparsity: 0.9824652777777778 Training loss: 2.3522182
Epoch: 1606 Sparsity: 0.9717013888888889 Training loss: 2.6328747
Epoch: 1607 Sparsity: 0.9791232638888889 Training loss: 3.3166058
Epoch: 1608 Sparsity: 0.9599826388888889 Training loss: 2.3020744
Epoch: 1609 Sparsity: 0.9774305555555556 Training loss: 3.4710739
Epoch: 1610 Sparsity: 0.972265625 Training loss: 2.3147628
Epoch: 1611 Sparsity: 0.9809027777777777 Training loss: 2.9565825
Epoch: 1612 Sparsity: 0.9890190972222224 Training loss: 2.6056662
Epoch: 1613 Sparsity: 0.9808159722222223 Training loss: 2.3384705
Epoch: 1614 Sparsity: 0.98046875 Training loss: 2.414758
Epoch: 1615 Sparsity: 0.9858072916666666 Training loss: 2.8054888
Epoch: 1616 Sparsity: 0.9737413194444444 Training loss: 3.1495142
Epoch: 1617 Sparsity: 0.9799913194444445 Training loss: 2.6009362
Epoch: 1618 Sparsity: 0.9666666666666668 Training loss: 2.7826083
Epoch: 1619 Sparsity: 0.9819010416666668 Training loss: 2.543865
Epoch: 1620 Sparsity: 0.9752170138888889 Training loss: 2.3286815
Epoch: 1621 Sparsity: 0.983984375 Training loss: 2.5324237
Epoch: 1622 Sparsity: 0.9835069444444444 Training loss: 2.620168
Epoch: 1623 Sparsity: 0.9807291666666668 Training loss: 2.4436839
Epoch: 1624 Sparsity: 0.9807725694444442 Training loss: 2.2790513
Epoch: 1625 Sparsity: 0.9662760416666666 Training loss: 2.6172495
Epoch: 1626 Sparsity: 0.9855902777777779 Training loss: 2.555004
Epoch: 1627 Sparsity: 0.9695312499999998 Training loss: 3.1046827
Epoch: 1628 Sparsity: 0.9736545138888889 Training loss: 2.3028665
Epoch: 1629 Sparsity: 0.9772135416666666 Training loss: 2.6447504
Epoch: 1630 Sparsity: 0.9858506944444445 Training loss: 2.5047581
Epoch: 1631 Sparsity: 0.9594184027777779 Training loss: 4.558066
Epoch: 1632 Sparsity: 0.9805989583333334 Training loss: 2.5785012
Epoch: 1633 Sparsity: 0.9641059027777776 Training loss: 2.6610062
Epoch: 1634 Sparsity: 0.9794270833333334 Training loss: 2.7788446
Epoch: 1635 Sparsity: 0.9668402777777778 Training loss: 2.665061
Epoch: 1636 Sparsity: 0.9782552083333332 Training loss: 2.6464312
Epoch: 1637 Sparsity: 0.9581163194444444 Training loss: 2.8302336
Epoch: 1638 Sparsity: 0.9775173611111112 Training loss: 3.0286758
Epoch: 1639 Sparsity: 0.9761284722222221 Training loss: 2.8824837
Epoch: 1640 Sparsity: 0.9629774305555555 Training loss: 2.489368
Epoch: 1641 Sparsity: 0.9791666666666666 Training loss: 2.618921
Epoch: 1642 Sparsity: 0.9635416666666666 Training loss: 2.637782
Epoch: 1643 Sparsity: 0.9792100694444444 Training loss: 2.7928624
Epoch: 1644 Sparsity: 0.9562934027777779 Training loss: 2.9025817
Epoch: 1645 Sparsity: 0.9751302083333332 Training loss: 2.9073513
Epoch: 1646 Sparsity: 0.9602864583333334 Training loss: 2.5308444
Epoch: 1647 Sparsity: 0.9794704861111111 Training loss: 3.0345056
Epoch: 1648 Sparsity: 0.9675781250000002 Training loss: 2.4188673
Epoch: 1649 Sparsity: 0.9726128472222223 Training loss: 2.442842
Epoch: 1650 Sparsity: 0.9611979166666667 Training loss: 2.3540802
Epoch: 1651 Sparsity: 0.9765625000000002 Training loss: 2.9194145
Epoch: 1652 Sparsity: 0.9748697916666667 Training loss: 2.480389
Epoch: 1653 Sparsity: 0.9836805555555556 Training loss: 2.6263902
Epoch: 1654 Sparsity: 0.9658420138888888 Training loss: 2.9012573
Epoch: 1655 Sparsity: 0.9825520833333334 Training loss: 2.985066
Epoch: 1656 Sparsity: 0.9692274305555555 Training loss: 4.204782
Epoch: 1657 Sparsity: 0.9774305555555557 Training loss: 2.561567
Epoch: 1658 Sparsity: 0.9740451388888889 Training loss: 2.3311324
Epoch: 1659 Sparsity: 0.9759548611111111 Training loss: 2.7341573
Epoch: 1660 Sparsity: 0.9575520833333332 Training loss: 2.340262
Epoch: 1661 Sparsity: 0.9824652777777777 Training loss: 2.789904
Epoch: 1662 Sparsity: 0.9664062500000001 Training loss: 2.41158
Epoch: 1663 Sparsity: 0.9845920138888891 Training loss: 2.5872145
Epoch: 1664 Sparsity: 0.9686631944444445 Training loss: 2.5117266
Epoch: 1665 Sparsity: 0.962890625 Training loss: 2.3044364
Epoch: 1666 Sparsity: 0.9852430555555556 Training loss: 2.7229617
Epoch: 1667 Sparsity: 0.965017361111111 Training loss: 2.3780293
Epoch: 1668 Sparsity: 0.9825520833333334 Training loss: 2.3920643
Epoch: 1669 Sparsity: 0.9787760416666667 Training loss: 2.4994013
Epoch: 1670 Sparsity: 0.9608072916666666 Training loss: 2.8109486
Epoch: 1671 Sparsity: 0.9678819444444444 Training loss: 2.322348
Epoch: 1672 Sparsity: 0.9751736111111111 Training loss: 2.4946818
Epoch: 1673 Sparsity: 0.9706163194444445 Training loss: 2.4729717
Epoch: 1674 Sparsity: 0.9796440972222223 Training loss: 2.5265584
Epoch: 1675 Sparsity: 0.9615017361111111 Training loss: 2.8600776
Epoch: 1676 Sparsity: 0.9733940972222221 Training loss: 2.5791116
Epoch: 1677 Sparsity: 0.9553819444444445 Training loss: 2.3100743
Epoch: 1678 Sparsity: 0.9750434027777777 Training loss: 3.746568
Epoch: 1679 Sparsity: 0.9677083333333334 Training loss: 2.4719987
Epoch: 1680 Sparsity: 0.9791232638888889 Training loss: 2.6082168
Epoch: 1681 Sparsity: 0.9672743055555555 Training loss: 3.1147902
Epoch: 1682 Sparsity: 0.9830729166666666 Training loss: 2.7302046
Epoch: 1683 Sparsity: 0.9704427083333332 Training loss: 2.4556653
Epoch: 1684 Sparsity: 0.9788194444444442 Training loss: 3.1871345
Epoch: 1685 Sparsity: 0.9668836805555555 Training loss: 2.493694
Epoch: 1686 Sparsity: 0.9662326388888889 Training loss: 2.3136084
Epoch: 1687 Sparsity: 0.9762586805555555 Training loss: 3.555938
Epoch: 1688 Sparsity: 0.9631510416666667 Training loss: 2.4175184
Epoch: 1689 Sparsity: 0.9754774305555556 Training loss: 2.696472
Epoch: 1690 Sparsity: 0.9818576388888888 Training loss: 2.7455177
Epoch: 1691 Sparsity: 0.9625 Training loss: 2.3474486
Epoch: 1692 Sparsity: 0.9797309027777779 Training loss: 2.4721446
Epoch: 1693 Sparsity: 0.9666666666666666 Training loss: 2.7307036
Epoch: 1694 Sparsity: 0.9774305555555556 Training loss: 3.0045831
Epoch: 1695 Sparsity: 0.9720052083333333 Training loss: 2.5148573
Epoch: 1696 Sparsity: 0.9741753472222221 Training loss: 2.466825
Epoch: 1697 Sparsity: 0.9562934027777776 Training loss: 2.4635696
Epoch: 1698 Sparsity: 0.9782118055555555 Training loss: 2.816485
Epoch: 1699 Sparsity: 0.9616753472222221 Training loss: 2.313548
Epoch: 1700 Sparsity: 0.9797743055555556 Training loss: 2.454398
Epoch: 1701 Sparsity: 0.9664496527777777 Training loss: 3.081313
Epoch: 1702 Sparsity: 0.976171875 Training loss: 2.7522032
Epoch: 1703 Sparsity: 0.9496527777777777 Training loss: 4.1790266
Epoch: 1704 Sparsity: 0.9766927083333334 Training loss: 3.2270088
Epoch: 1705 Sparsity: 0.9625 Training loss: 3.236694
Epoch: 1706 Sparsity: 0.974782986111111 Training loss: 2.7026823
Epoch: 1707 Sparsity: 0.95625 Training loss: 2.3978102
Epoch: 1708 Sparsity: 0.9769965277777777 Training loss: 2.864513
Epoch: 1709 Sparsity: 0.9713107638888887 Training loss: 2.4163284
Epoch: 1710 Sparsity: 0.9717013888888889 Training loss: 2.5444643
Epoch: 1711 Sparsity: 0.9569878472222223 Training loss: 2.6120324
Epoch: 1712 Sparsity: 0.9803819444444446 Training loss: 3.189895
Epoch: 1713 Sparsity: 0.9717013888888889 Training loss: 2.3828485
Epoch: 1714 Sparsity: 0.9756076388888889 Training loss: 2.3861759
Epoch: 1715 Sparsity: 0.9801649305555555 Training loss: 2.7097719
Epoch: 1716 Sparsity: 0.9653645833333332 Training loss: 3.3601046
Epoch: 1717 Sparsity: 0.9772135416666666 Training loss: 2.4178386
Epoch: 1718 Sparsity: 0.9672743055555557 Training loss: 2.312688
Epoch: 1719 Sparsity: 0.9812934027777777 Training loss: 2.6521327
Epoch: 1720 Sparsity: 0.9674045138888889 Training loss: 2.7140682
Epoch: 1721 Sparsity: 0.9760850694444445 Training loss: 2.4334922
Epoch: 1722 Sparsity: 0.9794270833333332 Training loss: 2.5358825
Epoch: 1723 Sparsity: 0.9604166666666666 Training loss: 2.6259797
Epoch: 1724 Sparsity: 0.9795138888888889 Training loss: 2.6619847
Epoch: 1725 Sparsity: 0.9790798611111111 Training loss: 2.4119403
Epoch: 1726 Sparsity: 0.9612413194444445 Training loss: 2.6118815
Epoch: 1727 Sparsity: 0.9782118055555555 Training loss: 2.7860198
Epoch: 1728 Sparsity: 0.9604600694444443 Training loss: 2.611941
Epoch: 1729 Sparsity: 0.9803819444444445 Training loss: 2.750388
Epoch: 1730 Sparsity: 0.973828125 Training loss: 2.4506104
Epoch: 1731 Sparsity: 0.9771267361111111 Training loss: 2.474698
Epoch: 1732 Sparsity: 0.9825086805555555 Training loss: 2.3875246
Epoch: 1733 Sparsity: 0.9701388888888888 Training loss: 2.305533
Epoch: 1734 Sparsity: 0.9834635416666666 Training loss: 2.6034622
Epoch: 1735 Sparsity: 0.9623263888888889 Training loss: 2.4055886
Epoch: 1736 Sparsity: 0.982204861111111 Training loss: 2.8668046
Epoch: 1737 Sparsity: 0.9715711805555556 Training loss: 2.455371
Epoch: 1738 Sparsity: 0.984375 Training loss: 2.736499
Epoch: 1739 Sparsity: 0.9522135416666668 Training loss: 2.6197348
Epoch: 1740 Sparsity: 0.980078125 Training loss: 3.5640826
Epoch: 1741 Sparsity: 0.9532552083333334 Training loss: 2.6842391
Epoch: 1742 Sparsity: 0.9786024305555555 Training loss: 3.0273886
Epoch: 1743 Sparsity: 0.9579427083333332 Training loss: 2.7270963
Epoch: 1744 Sparsity: 0.9789062500000002 Training loss: 3.0872128
Epoch: 1745 Sparsity: 0.9699652777777776 Training loss: 2.3766112
Epoch: 1746 Sparsity: 0.9735243055555556 Training loss: 2.5831537
Epoch: 1747 Sparsity: 0.9469184027777777 Training loss: 2.4057262
Epoch: 1748 Sparsity: 0.972439236111111 Training loss: 2.5468962
Epoch: 1749 Sparsity: 0.9795572916666666 Training loss: 2.5237975
Epoch: 1750 Sparsity: 0.9461371527777777 Training loss: 2.7731974
Epoch: 1751 Sparsity: 0.97578125 Training loss: 2.6458786
Epoch: 1752 Sparsity: 0.9564236111111111 Training loss: 2.5001643
Epoch: 1753 Sparsity: 0.943185763888889 Training loss: 2.315547
Epoch: 1754 Sparsity: 0.9692274305555555 Training loss: 2.6141105
Epoch: 1755 Sparsity: 0.9792100694444444 Training loss: 2.73592
Epoch: 1756 Sparsity: 0.9637152777777777 Training loss: 5.061895
Epoch: 1757 Sparsity: 0.9684027777777778 Training loss: 2.3328674
Epoch: 1758 Sparsity: 0.9417534722222222 Training loss: 2.316363
Epoch: 1759 Sparsity: 0.970529513888889 Training loss: 2.811764
Epoch: 1760 Sparsity: 0.9526041666666668 Training loss: 2.2968905
Epoch: 1761 Sparsity: 0.97421875 Training loss: 2.4758735
Epoch: 1762 Sparsity: 0.961328125 Training loss: 2.2977679
Epoch: 1763 Sparsity: 0.9788628472222222 Training loss: 2.3533525
Epoch: 1764 Sparsity: 0.9682725694444445 Training loss: 2.5665126
Epoch: 1765 Sparsity: 0.9716579861111111 Training loss: 3.5800834
Epoch: 1766 Sparsity: 0.9490885416666668 Training loss: 2.576162
Epoch: 1767 Sparsity: 0.9739583333333333 Training loss: 3.132713
Epoch: 1768 Sparsity: 0.9737847222222221 Training loss: 2.3652334
Epoch: 1769 Sparsity: 0.9789930555555555 Training loss: 2.8304343
Epoch: 1770 Sparsity: 0.955251736111111 Training loss: 2.401069
Epoch: 1771 Sparsity: 0.9765625 Training loss: 2.8280976
Epoch: 1772 Sparsity: 0.9556423611111111 Training loss: 2.488257
Epoch: 1773 Sparsity: 0.974435763888889 Training loss: 3.091743
Epoch: 1774 Sparsity: 0.9661024305555556 Training loss: 2.7443864
Epoch: 1775 Sparsity: 0.9688368055555555 Training loss: 2.5257201
Epoch: 1776 Sparsity: 0.9483072916666666 Training loss: 2.7743332
Epoch: 1777 Sparsity: 0.9701822916666666 Training loss: 2.690348
Epoch: 1778 Sparsity: 0.955859375 Training loss: 2.4735591
Epoch: 1779 Sparsity: 0.9723090277777777 Training loss: 2.5011513
Epoch: 1780 Sparsity: 0.9535590277777779 Training loss: 2.596721
Epoch: 1781 Sparsity: 0.9661458333333334 Training loss: 2.4246597
Epoch: 1782 Sparsity: 0.9487413194444445 Training loss: 2.3198695
Epoch: 1783 Sparsity: 0.9754774305555557 Training loss: 2.656841
Epoch: 1784 Sparsity: 0.9591145833333334 Training loss: 3.613289
Epoch: 1785 Sparsity: 0.9631944444444445 Training loss: 2.475155
Epoch: 1786 Sparsity: 0.970703125 Training loss: 2.446319
Epoch: 1787 Sparsity: 0.9347222222222221 Training loss: 2.6387877
Epoch: 1788 Sparsity: 0.9206597222222224 Training loss: 2.2783563
Epoch: 1789 Sparsity: 0.9618489583333332 Training loss: 3.0029414
Epoch: 1790 Sparsity: 0.9356336805555555 Training loss: 2.4995623
Epoch: 1791 Sparsity: 0.9668836805555557 Training loss: 2.9876263
Epoch: 1792 Sparsity: 0.9564236111111111 Training loss: 2.3567
Epoch: 1793 Sparsity: 0.9539930555555556 Training loss: 2.5196357
Epoch: 1794 Sparsity: 0.9779513888888888 Training loss: 2.3967326
Epoch: 1795 Sparsity: 0.96640625 Training loss: 2.6340058
Epoch: 1796 Sparsity: 0.9766059027777777 Training loss: 2.695151
Epoch: 1797 Sparsity: 0.9434461805555557 Training loss: 2.8790545
Epoch: 1798 Sparsity: 0.9726996527777777 Training loss: 2.6403966
Epoch: 1799 Sparsity: 0.9519097222222221 Training loss: 2.4635649
Epoch: 1800 Sparsity: 0.97265625 Training loss: 2.7493978
Epoch: 1801 Sparsity: 0.95390625 Training loss: 2.850814
Epoch: 1802 Sparsity: 0.9753472222222224 Training loss: 2.9790282
Epoch: 1803 Sparsity: 0.9507378472222221 Training loss: 3.6381633
Epoch: 1804 Sparsity: 0.9696180555555556 Training loss: 2.542592
Epoch: 1805 Sparsity: 0.9368489583333333 Training loss: 2.7666779
Epoch: 1806 Sparsity: 0.9709635416666668 Training loss: 3.140677
Epoch: 1807 Sparsity: 0.9671875 Training loss: 2.5262527
Epoch: 1808 Sparsity: 0.9539496527777779 Training loss: 2.5070498
Epoch: 1809 Sparsity: 0.9758246527777779 Training loss: 2.642546
Epoch: 1810 Sparsity: 0.9572916666666668 Training loss: 3.1832962
Epoch: 1811 Sparsity: 0.9718315972222221 Training loss: 2.4301054
Epoch: 1812 Sparsity: 0.9418402777777779 Training loss: 2.9219213
Epoch: 1813 Sparsity: 0.9652777777777779 Training loss: 3.2546132
Epoch: 1814 Sparsity: 0.9775607638888888 Training loss: 2.7375143
Epoch: 1815 Sparsity: 0.9654947916666666 Training loss: 2.4030116
Epoch: 1816 Sparsity: 0.9381076388888889 Training loss: 3.3037984
Epoch: 1817 Sparsity: 0.9636284722222224 Training loss: 2.6637132
Epoch: 1818 Sparsity: 0.9390190972222223 Training loss: 2.487813
Epoch: 1819 Sparsity: 0.9665364583333332 Training loss: 2.9254854
Epoch: 1820 Sparsity: 0.9497395833333332 Training loss: 2.54253
Epoch: 1821 Sparsity: 0.9590711805555555 Training loss: 2.62993
Epoch: 1822 Sparsity: 0.9671006944444445 Training loss: 2.337352
Epoch: 1823 Sparsity: 0.9510850694444445 Training loss: 2.568968
Epoch: 1824 Sparsity: 0.9683159722222221 Training loss: 3.0680957
Epoch: 1825 Sparsity: 0.9786458333333332 Training loss: 2.4642763
Epoch: 1826 Sparsity: 0.9570312499999998 Training loss: 3.22115
Epoch: 1827 Sparsity: 0.9740017361111111 Training loss: 2.6349485
Epoch: 1828 Sparsity: 0.9530381944444445 Training loss: 2.5377452
Epoch: 1829 Sparsity: 0.970876736111111 Training loss: 2.5105662
Epoch: 1830 Sparsity: 0.9516927083333332 Training loss: 2.5300493
Epoch: 1831 Sparsity: 0.9693142361111112 Training loss: 2.5575318
Epoch: 1832 Sparsity: 0.94375 Training loss: 2.6178563
Epoch: 1833 Sparsity: 0.966579861111111 Training loss: 2.5331883
Epoch: 1834 Sparsity: 0.9667100694444445 Training loss: 2.4125972
Epoch: 1835 Sparsity: 0.9786024305555557 Training loss: 2.757335
Epoch: 1836 Sparsity: 0.9673611111111112 Training loss: 4.958246
Epoch: 1837 Sparsity: 0.9752604166666666 Training loss: 2.5760665
Epoch: 1838 Sparsity: 0.9487413194444443 Training loss: 3.9973445
Epoch: 1839 Sparsity: 0.9738715277777776 Training loss: 2.5208595
Epoch: 1840 Sparsity: 0.9519531250000002 Training loss: 2.5215192
Epoch: 1841 Sparsity: 0.9601996527777776 Training loss: 2.3491533
Epoch: 1842 Sparsity: 0.9726996527777778 Training loss: 2.5162013
Epoch: 1843 Sparsity: 0.9525607638888889 Training loss: 4.359243
Epoch: 1844 Sparsity: 0.9690972222222223 Training loss: 2.5288954
Epoch: 1845 Sparsity: 0.9690538194444445 Training loss: 2.4932852
Epoch: 1846 Sparsity: 0.963671875 Training loss: 2.3278399
Epoch: 1847 Sparsity: 0.9726128472222222 Training loss: 2.3281407
Epoch: 1848 Sparsity: 0.9572048611111112 Training loss: 2.3564677
Epoch: 1849 Sparsity: 0.9732204861111111 Training loss: 2.685574
Epoch: 1850 Sparsity: 0.953125 Training loss: 4.3084936
Epoch: 1851 Sparsity: 0.9730902777777779 Training loss: 2.7992613
Epoch: 1852 Sparsity: 0.9671874999999999 Training loss: 2.4640625
Epoch: 1853 Sparsity: 0.9762586805555555 Training loss: 2.5122569
Epoch: 1854 Sparsity: 0.9752604166666667 Training loss: 2.681197
Epoch: 1855 Sparsity: 0.9625868055555555 Training loss: 2.523318
Epoch: 1856 Sparsity: 0.96953125 Training loss: 2.4350579
Epoch: 1857 Sparsity: 0.9579427083333332 Training loss: 2.6144705
Epoch: 1858 Sparsity: 0.9762152777777777 Training loss: 2.85315
Epoch: 1859 Sparsity: 0.9599392361111111 Training loss: 2.53276
Epoch: 1860 Sparsity: 0.9741753472222221 Training loss: 3.445578
Epoch: 1861 Sparsity: 0.9569444444444443 Training loss: 3.020716
Epoch: 1862 Sparsity: 0.9674913194444444 Training loss: 2.3529565
Epoch: 1863 Sparsity: 0.9497829861111111 Training loss: 2.4565625
Epoch: 1864 Sparsity: 0.9713107638888887 Training loss: 2.8508413
Epoch: 1865 Sparsity: 0.9641059027777779 Training loss: 2.3686204
Epoch: 1866 Sparsity: 0.9712239583333332 Training loss: 2.4425476
Epoch: 1867 Sparsity: 0.9649305555555555 Training loss: 2.4749315
Epoch: 1868 Sparsity: 0.9754340277777777 Training loss: 2.5011756
Epoch: 1869 Sparsity: 0.9615885416666666 Training loss: 2.507064
Epoch: 1870 Sparsity: 0.9588541666666666 Training loss: 4.155622
Epoch: 1871 Sparsity: 0.9790364583333334 Training loss: 2.784863
Epoch: 1872 Sparsity: 0.961154513888889 Training loss: 2.8264112
Epoch: 1873 Sparsity: 0.9759982638888889 Training loss: 3.3792443
Epoch: 1874 Sparsity: 0.9693142361111112 Training loss: 2.5066674
Epoch: 1875 Sparsity: 0.9759114583333334 Training loss: 2.4907308
Epoch: 1876 Sparsity: 0.9662760416666668 Training loss: 3.6633868
Epoch: 1877 Sparsity: 0.9819010416666668 Training loss: 2.5687788
Epoch: 1878 Sparsity: 0.96953125 Training loss: 2.454677
Epoch: 1879 Sparsity: 0.970876736111111 Training loss: 2.53752
Epoch: 1880 Sparsity: 0.9672309027777779 Training loss: 2.479168
Epoch: 1881 Sparsity: 0.9753472222222221 Training loss: 2.8468204
Epoch: 1882 Sparsity: 0.9776909722222223 Training loss: 2.5745077
Epoch: 1883 Sparsity: 0.9536024305555555 Training loss: 2.8918717
Epoch: 1884 Sparsity: 0.9775607638888889 Training loss: 3.0537102
Epoch: 1885 Sparsity: 0.9656684027777777 Training loss: 2.5333111
Epoch: 1886 Sparsity: 0.9741319444444445 Training loss: 2.7629402
Epoch: 1887 Sparsity: 0.9564236111111111 Training loss: 2.4274948
Epoch: 1888 Sparsity: 0.9740451388888889 Training loss: 2.7113485
Epoch: 1889 Sparsity: 0.9559027777777779 Training loss: 2.8850517
Epoch: 1890 Sparsity: 0.9701822916666668 Training loss: 2.3284295
Epoch: 1891 Sparsity: 0.973046875 Training loss: 2.663217
Epoch: 1892 Sparsity: 0.9794270833333332 Training loss: 2.4615166
Epoch: 1893 Sparsity: 0.9439670138888889 Training loss: 2.5630314
Epoch: 1894 Sparsity: 0.9713975694444444 Training loss: 3.4567947
Epoch: 1895 Sparsity: 0.9757378472222221 Training loss: 2.4841654
Epoch: 1896 Sparsity: 0.9720052083333334 Training loss: 2.4355025
Epoch: 1897 Sparsity: 0.9775607638888889 Training loss: 2.6682248
Epoch: 1898 Sparsity: 0.9815104166666666 Training loss: 2.5752409
Epoch: 1899 Sparsity: 0.9633246527777777 Training loss: 2.6487591
Epoch: 1900 Sparsity: 0.9794270833333332 Training loss: 2.7220538
Epoch: 1901 Sparsity: 0.9648003472222222 Training loss: 3.3539958
Epoch: 1902 Sparsity: 0.9772135416666666 Training loss: 2.4403527
Epoch: 1903 Sparsity: 0.9602864583333333 Training loss: 3.3458135
Epoch: 1904 Sparsity: 0.9733506944444444 Training loss: 2.8177469
Epoch: 1905 Sparsity: 0.9454427083333334 Training loss: 2.7064483
Epoch: 1906 Sparsity: 0.9749565972222222 Training loss: 2.7186291
Epoch: 1907 Sparsity: 0.98203125 Training loss: 2.5586388
Epoch: 1908 Sparsity: 0.9546006944444445 Training loss: 3.3451767
Epoch: 1909 Sparsity: 0.9792100694444443 Training loss: 2.9413304
Epoch: 1910 Sparsity: 0.9641493055555553 Training loss: 2.3911755
Epoch: 1911 Sparsity: 0.954861111111111 Training loss: 2.3071473
Epoch: 1912 Sparsity: 0.974782986111111 Training loss: 2.96674
Epoch: 1913 Sparsity: 0.956640625 Training loss: 2.711244
Epoch: 1914 Sparsity: 0.9588975694444445 Training loss: 2.3056865
Epoch: 1915 Sparsity: 0.9635850694444444 Training loss: 2.407707
Epoch: 1916 Sparsity: 0.9746527777777777 Training loss: 2.3825018
Epoch: 1917 Sparsity: 0.9758246527777779 Training loss: 2.5573628
Epoch: 1918 Sparsity: 0.9542100694444444 Training loss: 2.3016806
Epoch: 1919 Sparsity: 0.9779079861111111 Training loss: 3.2521253
Epoch: 1920 Sparsity: 0.9536024305555555 Training loss: 3.026423
Epoch: 1921 Sparsity: 0.9760850694444445 Training loss: 3.184497
Epoch: 1922 Sparsity: 0.961111111111111 Training loss: 2.7634604
Epoch: 1923 Sparsity: 0.9758680555555556 Training loss: 2.8798163
Epoch: 1924 Sparsity: 0.959765625 Training loss: 2.4512513
Epoch: 1925 Sparsity: 0.9777777777777779 Training loss: 2.5323105
Epoch: 1926 Sparsity: 0.9601128472222221 Training loss: 2.597112
Epoch: 1927 Sparsity: 0.9775173611111111 Training loss: 2.7305768
Epoch: 1928 Sparsity: 0.9784722222222223 Training loss: 2.515855
Epoch: 1929 Sparsity: 0.9646701388888888 Training loss: 2.5106156
Epoch: 1930 Sparsity: 0.9798177083333334 Training loss: 2.6469214
Epoch: 1931 Sparsity: 0.9658420138888888 Training loss: 2.572069
Epoch: 1932 Sparsity: 0.9744357638888888 Training loss: 2.407006
Epoch: 1933 Sparsity: 0.98125 Training loss: 2.5839653
Epoch: 1934 Sparsity: 0.9733940972222224 Training loss: 2.6007378
Epoch: 1935 Sparsity: 0.9680989583333334 Training loss: 2.3231924
Epoch: 1936 Sparsity: 0.977734375 Training loss: 2.5401063
Epoch: 1937 Sparsity: 0.9552517361111112 Training loss: 2.771222
Epoch: 1938 Sparsity: 0.9747395833333332 Training loss: 2.450839
Epoch: 1939 Sparsity: 0.974435763888889 Training loss: 2.549889
Epoch: 1940 Sparsity: 0.9495225694444445 Training loss: 2.989639
Epoch: 1941 Sparsity: 0.9638020833333334 Training loss: 2.3241427
Epoch: 1942 Sparsity: 0.9776475694444444 Training loss: 2.7109847
Epoch: 1943 Sparsity: 0.96328125 Training loss: 2.5353496
Epoch: 1944 Sparsity: 0.9763020833333333 Training loss: 2.4733286
Epoch: 1945 Sparsity: 0.9690538194444445 Training loss: 2.6207426
Epoch: 1946 Sparsity: 0.9779513888888889 Training loss: 3.5066054
Epoch: 1947 Sparsity: 0.965060763888889 Training loss: 2.5586877
Epoch: 1948 Sparsity: 0.9747395833333332 Training loss: 2.5331206
Epoch: 1949 Sparsity: 0.9679253472222223 Training loss: 2.8161345
Epoch: 1950 Sparsity: 0.9799913194444445 Training loss: 2.5710855
Epoch: 1951 Sparsity: 0.9698350694444444 Training loss: 2.7617354
Epoch: 1952 Sparsity: 0.9818142361111111 Training loss: 3.5789921
Epoch: 1953 Sparsity: 0.9721354166666666 Training loss: 2.4862058
Epoch: 1954 Sparsity: 0.9761284722222223 Training loss: 2.471893
Epoch: 1955 Sparsity: 0.9877170138888889 Training loss: 2.6287553
Epoch: 1956 Sparsity: 0.9648003472222222 Training loss: 2.8113606
Epoch: 1957 Sparsity: 0.9701388888888888 Training loss: 3.627023
Epoch: 1958 Sparsity: 0.9828993055555555 Training loss: 2.758711
Epoch: 1959 Sparsity: 0.9773003472222224 Training loss: 2.6760943
Epoch: 1960 Sparsity: 0.9502170138888889 Training loss: 3.05539
Epoch: 1961 Sparsity: 0.9742621527777778 Training loss: 2.6003454
Epoch: 1962 Sparsity: 0.9640190972222221 Training loss: 2.403152
Epoch: 1963 Sparsity: 0.9780815972222223 Training loss: 2.6461782
Epoch: 1964 Sparsity: 0.968359375 Training loss: 2.4568563
Epoch: 1965 Sparsity: 0.9826388888888887 Training loss: 2.4852617
Epoch: 1966 Sparsity: 0.9688802083333334 Training loss: 2.6394508
Epoch: 1967 Sparsity: 0.9772569444444444 Training loss: 2.83236
Epoch: 1968 Sparsity: 0.9812934027777779 Training loss: 2.3987303
Epoch: 1969 Sparsity: 0.9802083333333333 Training loss: 2.669464
Epoch: 1970 Sparsity: 0.9596354166666666 Training loss: 2.5334477
Epoch: 1971 Sparsity: 0.9775173611111111 Training loss: 2.5026383
Epoch: 1972 Sparsity: 0.9686631944444445 Training loss: 2.3885114
Epoch: 1973 Sparsity: 0.96875 Training loss: 2.2966983
Epoch: 1974 Sparsity: 0.976779513888889 Training loss: 2.7399843
Epoch: 1975 Sparsity: 0.9559027777777777 Training loss: 2.4816284
Epoch: 1976 Sparsity: 0.9798177083333333 Training loss: 2.9083037
Epoch: 1977 Sparsity: 0.9638888888888889 Training loss: 2.3974686
Epoch: 1978 Sparsity: 0.982595486111111 Training loss: 2.679916
Epoch: 1979 Sparsity: 0.972482638888889 Training loss: 2.4230988
Epoch: 1980 Sparsity: 0.9803385416666666 Training loss: 2.4460094
Epoch: 1981 Sparsity: 0.9674479166666666 Training loss: 2.432715
Epoch: 1982 Sparsity: 0.9743055555555555 Training loss: 2.4927018
Epoch: 1983 Sparsity: 0.9794704861111111 Training loss: 2.5392625
Epoch: 1984 Sparsity: 0.9673611111111111 Training loss: 2.4674273
Epoch: 1985 Sparsity: 0.9783854166666668 Training loss: 2.347925
Epoch: 1986 Sparsity: 0.9823350694444445 Training loss: 2.5801954
Epoch: 1987 Sparsity: 0.9647135416666668 Training loss: 2.6442642
Epoch: 1988 Sparsity: 0.9697916666666666 Training loss: 2.6202552
Epoch: 1989 Sparsity: 0.9792968750000002 Training loss: 2.666693
Epoch: 1990 Sparsity: 0.9694878472222221 Training loss: 2.3235738
Epoch: 1991 Sparsity: 0.976171875 Training loss: 2.901121
Epoch: 1992 Sparsity: 0.9536024305555555 Training loss: 2.3412406
Epoch: 1993 Sparsity: 0.9600694444444444 Training loss: 2.298564
Epoch: 1994 Sparsity: 0.9801649305555558 Training loss: 2.6709998
Epoch: 1995 Sparsity: 0.970876736111111 Training loss: 2.3666396
Epoch: 1996 Sparsity: 0.9774739583333332 Training loss: 3.216576
Epoch: 1997 Sparsity: 0.9586371527777777 Training loss: 2.5253103
Epoch: 1998 Sparsity: 0.9749131944444445 Training loss: 2.5148146
Epoch: 1999 Sparsity: 0.9723958333333332 Training loss: 2.3193069
Epoch: 2000 Sparsity: 0.9791666666666666 Training loss: 3.452293
Epoch: 2001 Sparsity: 0.9612847222222222 Training loss: 2.7874339
Epoch: 2002 Sparsity: 0.9810763888888889 Training loss: 2.9088118
Epoch: 2003 Sparsity: 0.9758680555555556 Training loss: 2.4087698
Epoch: 2004 Sparsity: 0.9784288194444443 Training loss: 2.9773483
Epoch: 2005 Sparsity: 0.9647569444444445 Training loss: 2.3163168
Epoch: 2006 Sparsity: 0.9657552083333332 Training loss: 2.3198187
Epoch: 2007 Sparsity: 0.9793402777777779 Training loss: 2.4831407
Epoch: 2008 Sparsity: 0.9799045138888889 Training loss: 2.5159132
Epoch: 2009 Sparsity: 0.9891493055555556 Training loss: 2.4807827
Epoch: 2010 Sparsity: 0.9755642361111111 Training loss: 3.5036895
Epoch: 2011 Sparsity: 0.9701822916666668 Training loss: 2.3021662
Epoch: 2012 Sparsity: 0.9825520833333332 Training loss: 3.1969151
Epoch: 2013 Sparsity: 0.9720052083333333 Training loss: 3.0559623
Epoch: 2014 Sparsity: 0.9823784722222222 Training loss: 3.1238093
Epoch: 2015 Sparsity: 0.9844618055555555 Training loss: 2.4648795
Epoch: 2016 Sparsity: 0.9790798611111111 Training loss: 4.0550437
Epoch: 2017 Sparsity: 0.9833333333333332 Training loss: 2.532785
Epoch: 2018 Sparsity: 0.9736545138888888 Training loss: 2.8537216
Epoch: 2019 Sparsity: 0.9688368055555555 Training loss: 2.3140287
Epoch: 2020 Sparsity: 0.9861111111111113 Training loss: 2.8457615
Epoch: 2021 Sparsity: 0.9772135416666666 Training loss: 2.3246377
Epoch: 2022 Sparsity: 0.9736545138888889 Training loss: 2.6863828
Epoch: 2023 Sparsity: 0.9862847222222222 Training loss: 2.4189074
Epoch: 2024 Sparsity: 0.9716145833333334 Training loss: 2.5664968
Epoch: 2025 Sparsity: 0.9849826388888889 Training loss: 2.4217556
Epoch: 2026 Sparsity: 0.9634982638888889 Training loss: 2.6482613
Epoch: 2027 Sparsity: 0.9803385416666666 Training loss: 3.0584219
Epoch: 2028 Sparsity: 0.9801215277777777 Training loss: 2.4723551
Epoch: 2029 Sparsity: 0.9855034722222223 Training loss: 2.9465837
Epoch: 2030 Sparsity: 0.9779079861111108 Training loss: 2.359979
Epoch: 2031 Sparsity: 0.9833767361111111 Training loss: 2.837009
Epoch: 2032 Sparsity: 0.9655381944444447 Training loss: 2.4175856
Epoch: 2033 Sparsity: 0.9861979166666666 Training loss: 2.7052372
Epoch: 2034 Sparsity: 0.9762152777777778 Training loss: 2.3296807
Epoch: 2035 Sparsity: 0.9821180555555555 Training loss: 2.4015725
Epoch: 2036 Sparsity: 0.9862847222222223 Training loss: 2.7905655
Epoch: 2037 Sparsity: 0.978125 Training loss: 2.3255022
Epoch: 2038 Sparsity: 0.9546875 Training loss: 2.42944
Epoch: 2039 Sparsity: 0.9797743055555557 Training loss: 3.8111048
Epoch: 2040 Sparsity: 0.9630642361111112 Training loss: 2.4476233
Epoch: 2041 Sparsity: 0.980078125 Training loss: 2.7537491
Epoch: 2042 Sparsity: 0.9725694444444445 Training loss: 2.591288
Epoch: 2043 Sparsity: 0.9851562499999998 Training loss: 2.554229
Epoch: 2044 Sparsity: 0.974782986111111 Training loss: 2.5422053
Epoch: 2045 Sparsity: 0.9829427083333334 Training loss: 2.929256
Epoch: 2046 Sparsity: 0.9712673611111111 Training loss: 2.45757
Epoch: 2047 Sparsity: 0.9832465277777777 Training loss: 2.3971364
Epoch: 2048 Sparsity: 0.9727864583333334 Training loss: 2.4353838
Epoch: 2049 Sparsity: 0.9838107638888888 Training loss: 3.0135667
Epoch: 2050 Sparsity: 0.9786458333333334 Training loss: 2.3419683
Epoch: 2051 Sparsity: 0.9802083333333332 Training loss: 2.4131074
Epoch: 2052 Sparsity: 0.974609375 Training loss: 2.5362167
Epoch: 2053 Sparsity: 0.9875434027777779 Training loss: 2.473667
Epoch: 2054 Sparsity: 0.9735243055555556 Training loss: 2.9786322
Epoch: 2055 Sparsity: 0.9838541666666666 Training loss: 2.6043656
Epoch: 2056 Sparsity: 0.951171875 Training loss: 2.7335365
Epoch: 2057 Sparsity: 0.9802083333333333 Training loss: 3.2320375
Epoch: 2058 Sparsity: 0.9578993055555556 Training loss: 2.688858
Epoch: 2059 Sparsity: 0.976736111111111 Training loss: 3.336831
Epoch: 2060 Sparsity: 0.9741753472222221 Training loss: 2.4717767
Epoch: 2061 Sparsity: 0.984157986111111 Training loss: 2.515473
Epoch: 2062 Sparsity: 0.9770833333333334 Training loss: 2.3484428
Epoch: 2063 Sparsity: 0.9789496527777777 Training loss: 2.3204856
Epoch: 2064 Sparsity: 0.9843315972222222 Training loss: 2.9487526
Epoch: 2065 Sparsity: 0.9755642361111112 Training loss: 2.6084225
Epoch: 2066 Sparsity: 0.982638888888889 Training loss: 2.6151202
Epoch: 2067 Sparsity: 0.9898871527777778 Training loss: 2.4089086
Epoch: 2068 Sparsity: 0.9830295138888889 Training loss: 2.2947829
Epoch: 2069 Sparsity: 0.9858506944444443 Training loss: 2.716479
Epoch: 2070 Sparsity: 0.9673611111111112 Training loss: 2.440069
Epoch: 2071 Sparsity: 0.9871093749999998 Training loss: 4.3144093
Epoch: 2072 Sparsity: 0.9845052083333334 Training loss: 2.364258
Epoch: 2073 Sparsity: 0.9712673611111112 Training loss: 2.48627
Epoch: 2074 Sparsity: 0.9858940972222223 Training loss: 2.6078525
Epoch: 2075 Sparsity: 0.9705729166666668 Training loss: 2.6535726
Epoch: 2076 Sparsity: 0.9879340277777777 Training loss: 2.9352982
Epoch: 2077 Sparsity: 0.977734375 Training loss: 2.3730443
Epoch: 2078 Sparsity: 0.9891493055555556 Training loss: 2.5229106
Epoch: 2079 Sparsity: 0.9793402777777777 Training loss: 2.6818728
Epoch: 2080 Sparsity: 0.9879340277777778 Training loss: 2.587798
Epoch: 2081 Sparsity: 0.9779947916666668 Training loss: 2.462446
Epoch: 2082 Sparsity: 0.9809895833333334 Training loss: 2.3480506
Epoch: 2083 Sparsity: 0.9866319444444445 Training loss: 2.649861
Epoch: 2084 Sparsity: 0.9674045138888889 Training loss: 2.7614014
Epoch: 2085 Sparsity: 0.9849826388888889 Training loss: 2.5869675
Epoch: 2086 Sparsity: 0.9779079861111111 Training loss: 2.4531305
Epoch: 2087 Sparsity: 0.9911458333333332 Training loss: 2.5342667
Epoch: 2088 Sparsity: 0.9723958333333333 Training loss: 3.122788
Epoch: 2089 Sparsity: 0.986328125 Training loss: 2.7771666
Epoch: 2090 Sparsity: 0.9734375 Training loss: 2.7264569
Epoch: 2091 Sparsity: 0.9877170138888889 Training loss: 2.5654466
Epoch: 2092 Sparsity: 0.9763020833333332 Training loss: 3.2532518
Epoch: 2093 Sparsity: 0.9860243055555555 Training loss: 2.834688
Epoch: 2094 Sparsity: 0.9698784722222223 Training loss: 2.410552
Epoch: 2095 Sparsity: 0.9821180555555555 Training loss: 2.3745766
Epoch: 2096 Sparsity: 0.9861979166666666 Training loss: 2.5406504
Epoch: 2097 Sparsity: 0.9690104166666667 Training loss: 2.313104
Epoch: 2098 Sparsity: 0.9881510416666668 Training loss: 2.664409
Epoch: 2099 Sparsity: 0.9717013888888889 Training loss: 2.8324537
Epoch: 2100 Sparsity: 0.9788628472222222 Training loss: 2.299793
Epoch: 2101 Sparsity: 0.9872395833333332 Training loss: 2.8182063
Epoch: 2102 Sparsity: 0.9752604166666666 Training loss: 3.5681086
Epoch: 2103 Sparsity: 0.9848958333333332 Training loss: 2.474745
Epoch: 2104 Sparsity: 0.9828125 Training loss: 2.3624518
Epoch: 2105 Sparsity: 0.97421875 Training loss: 2.291166
Epoch: 2106 Sparsity: 0.986545138888889 Training loss: 2.5494754
Epoch: 2107 Sparsity: 0.9819010416666666 Training loss: 2.603181
Epoch: 2108 Sparsity: 0.9891927083333334 Training loss: 2.6997032
Epoch: 2109 Sparsity: 0.969921875 Training loss: 4.2474732
Epoch: 2110 Sparsity: 0.98671875 Training loss: 2.5545092
Epoch: 2111 Sparsity: 0.9836371527777779 Training loss: 2.5439312
Epoch: 2112 Sparsity: 0.9713975694444443 Training loss: 2.3798819
Epoch: 2113 Sparsity: 0.9834201388888889 Training loss: 3.0527244
Epoch: 2114 Sparsity: 0.9861979166666666 Training loss: 2.533939
Epoch: 2115 Sparsity: 0.9898003472222223 Training loss: 2.6260216
Epoch: 2116 Sparsity: 0.9838541666666668 Training loss: 2.512631
Epoch: 2117 Sparsity: 0.9901041666666668 Training loss: 2.8888564
Epoch: 2118 Sparsity: 0.9802083333333333 Training loss: 5.298231
Epoch: 2119 Sparsity: 0.9874131944444443 Training loss: 2.5805058
Epoch: 2120 Sparsity: 0.9789496527777777 Training loss: 4.9686236
Epoch: 2121 Sparsity: 0.9701822916666668 Training loss: 2.3146615
Epoch: 2122 Sparsity: 0.9863715277777777 Training loss: 2.3558686
Epoch: 2123 Sparsity: 0.9870225694444444 Training loss: 2.3255682
Epoch: 2124 Sparsity: 0.9713541666666667 Training loss: 3.9259448
Epoch: 2125 Sparsity: 0.98671875 Training loss: 2.8989635
Epoch: 2126 Sparsity: 0.9861979166666666 Training loss: 2.4314275
Epoch: 2127 Sparsity: 0.9898003472222223 Training loss: 3.2582705
Epoch: 2128 Sparsity: 0.9723090277777777 Training loss: 2.8308308
Epoch: 2129 Sparsity: 0.9884114583333334 Training loss: 3.386782
Epoch: 2130 Sparsity: 0.9739583333333334 Training loss: 2.3949459
Epoch: 2131 Sparsity: 0.9854600694444444 Training loss: 2.4967313
Epoch: 2132 Sparsity: 0.9749131944444445 Training loss: 2.4124842
Epoch: 2133 Sparsity: 0.9864583333333334 Training loss: 3.086695
Epoch: 2134 Sparsity: 0.9795572916666668 Training loss: 2.7615712
Epoch: 2135 Sparsity: 0.9848958333333332 Training loss: 2.977619
Epoch: 2136 Sparsity: 0.9741753472222221 Training loss: 2.7687547
Epoch: 2137 Sparsity: 0.9751302083333334 Training loss: 2.664739
Epoch: 2138 Sparsity: 0.9833333333333334 Training loss: 2.364287
Epoch: 2139 Sparsity: 0.988107638888889 Training loss: 2.423206
Epoch: 2140 Sparsity: 0.9775607638888889 Training loss: 2.609841
Epoch: 2141 Sparsity: 0.9838107638888888 Training loss: 2.6401672
Epoch: 2142 Sparsity: 0.9921875 Training loss: 2.5917773
Epoch: 2143 Sparsity: 0.9830729166666667 Training loss: 3.1219776
Epoch: 2144 Sparsity: 0.9897135416666666 Training loss: 2.8017118
Epoch: 2145 Sparsity: 0.9786458333333334 Training loss: 2.6065333
Epoch: 2146 Sparsity: 0.988671875 Training loss: 2.8317835
Epoch: 2147 Sparsity: 0.98359375 Training loss: 2.3936245
Epoch: 2148 Sparsity: 0.9725260416666668 Training loss: 2.4427326
Epoch: 2149 Sparsity: 0.9820746527777777 Training loss: 2.384437
Epoch: 2150 Sparsity: 0.9893663194444444 Training loss: 2.7875621
Epoch: 2151 Sparsity: 0.9803385416666666 Training loss: 3.3892722
Epoch: 2152 Sparsity: 0.9751302083333334 Training loss: 2.2819834
Epoch: 2153 Sparsity: 0.9799479166666666 Training loss: 2.339514
Epoch: 2154 Sparsity: 0.9855902777777776 Training loss: 2.6047783
Epoch: 2155 Sparsity: 0.9904947916666668 Training loss: 2.497432
Epoch: 2156 Sparsity: 0.9823350694444446 Training loss: 2.4783745
Epoch: 2157 Sparsity: 0.9891059027777777 Training loss: 2.548267
Epoch: 2158 Sparsity: 0.9862413194444445 Training loss: 2.3675988
Epoch: 2159 Sparsity: 0.990017361111111 Training loss: 2.7445023
Epoch: 2160 Sparsity: 0.9733506944444444 Training loss: 2.8974967
Epoch: 2161 Sparsity: 0.9716579861111112 Training loss: 2.3041887
Epoch: 2162 Sparsity: 0.9883246527777777 Training loss: 4.2052274
Epoch: 2163 Sparsity: 0.9851996527777777 Training loss: 2.445748
Epoch: 2164 Sparsity: 0.9857204861111111 Training loss: 2.3826447
Epoch: 2165 Sparsity: 0.9935763888888889 Training loss: 2.8056705
Epoch: 2166 Sparsity: 0.9838975694444443 Training loss: 2.6891446
Epoch: 2167 Sparsity: 0.9908420138888887 Training loss: 3.5790172
Epoch: 2168 Sparsity: 0.9810329861111111 Training loss: 2.4632366
Epoch: 2169 Sparsity: 0.9907118055555555 Training loss: 2.4994326
Epoch: 2170 Sparsity: 0.984375 Training loss: 3.3141105
Epoch: 2171 Sparsity: 0.9747395833333334 Training loss: 2.503638
Epoch: 2172 Sparsity: 0.9870225694444444 Training loss: 2.414109
Epoch: 2173 Sparsity: 0.9862413194444445 Training loss: 2.3346155
Epoch: 2174 Sparsity: 0.9680555555555556 Training loss: 2.7545938
Epoch: 2175 Sparsity: 0.9756076388888888 Training loss: 2.2843199
Epoch: 2176 Sparsity: 0.9865451388888887 Training loss: 2.6147006
Epoch: 2177 Sparsity: 0.9885850694444445 Training loss: 2.481471
Epoch: 2178 Sparsity: 0.980685763888889 Training loss: 2.4407282
Epoch: 2179 Sparsity: 0.9899739583333333 Training loss: 2.4450483
Epoch: 2180 Sparsity: 0.9742187500000001 Training loss: 2.818425
Epoch: 2181 Sparsity: 0.9895399305555556 Training loss: 2.7223175
Epoch: 2182 Sparsity: 0.9792534722222221 Training loss: 2.326366
Epoch: 2183 Sparsity: 0.9902777777777777 Training loss: 2.555482
Epoch: 2184 Sparsity: 0.9802517361111113 Training loss: 2.6434476
Epoch: 2185 Sparsity: 0.9840277777777778 Training loss: 2.6776133
Epoch: 2186 Sparsity: 0.9878472222222221 Training loss: 2.4209096
Epoch: 2187 Sparsity: 0.972829861111111 Training loss: 2.518487
Epoch: 2188 Sparsity: 0.9823784722222222 Training loss: 2.4099383
Epoch: 2189 Sparsity: 0.9756076388888889 Training loss: 2.3010967
Epoch: 2190 Sparsity: 0.988064236111111 Training loss: 2.5769515
Epoch: 2191 Sparsity: 0.9690104166666667 Training loss: 2.679681
Epoch: 2192 Sparsity: 0.9852864583333334 Training loss: 2.6469884
Epoch: 2193 Sparsity: 0.9773003472222221 Training loss: 3.0707653
Epoch: 2194 Sparsity: 0.9821614583333332 Training loss: 2.6723945
Epoch: 2195 Sparsity: 0.968532986111111 Training loss: 2.8457968
Epoch: 2196 Sparsity: 0.984157986111111 Training loss: 3.1821852
Epoch: 2197 Sparsity: 0.9830729166666666 Training loss: 2.4903839
Epoch: 2198 Sparsity: 0.974609375 Training loss: 2.567181
Epoch: 2199 Sparsity: 0.9862413194444446 Training loss: 2.7083197
Epoch: 2200 Sparsity: 0.9675347222222224 Training loss: 2.3331082
Epoch: 2201 Sparsity: 0.9838107638888889 Training loss: 2.6020067
Epoch: 2202 Sparsity: 0.9690538194444445 Training loss: 2.6043081
Epoch: 2203 Sparsity: 0.9827690972222222 Training loss: 2.494762
Epoch: 2204 Sparsity: 0.972829861111111 Training loss: 2.3424797
Epoch: 2205 Sparsity: 0.9821614583333333 Training loss: 2.7533112
Epoch: 2206 Sparsity: 0.968967013888889 Training loss: 2.7180972
Epoch: 2207 Sparsity: 0.9843315972222222 Training loss: 3.434893
Epoch: 2208 Sparsity: 0.968923611111111 Training loss: 2.540087
Epoch: 2209 Sparsity: 0.9825086805555557 Training loss: 2.733803
Epoch: 2210 Sparsity: 0.9886284722222223 Training loss: 2.4363341
Epoch: 2211 Sparsity: 0.9731336805555555 Training loss: 2.943823
Epoch: 2212 Sparsity: 0.9852864583333334 Training loss: 2.8767316
Epoch: 2213 Sparsity: 0.9768663194444445 Training loss: 2.398217
Epoch: 2214 Sparsity: 0.986328125 Training loss: 2.4993865
Epoch: 2215 Sparsity: 0.9713975694444444 Training loss: 2.3592522
Epoch: 2216 Sparsity: 0.9885850694444445 Training loss: 2.9304807
Epoch: 2217 Sparsity: 0.97734375 Training loss: 3.1133409
Epoch: 2218 Sparsity: 0.9766059027777778 Training loss: 2.32507
Epoch: 2219 Sparsity: 0.9903645833333334 Training loss: 2.7568521
Epoch: 2220 Sparsity: 0.9799913194444445 Training loss: 2.4139993
Epoch: 2221 Sparsity: 0.9883246527777777 Training loss: 2.7575223
Epoch: 2222 Sparsity: 0.9721788194444445 Training loss: 3.6486638
Epoch: 2223 Sparsity: 0.987109375 Training loss: 2.6072807
Epoch: 2224 Sparsity: 0.982248263888889 Training loss: 2.2984126
Epoch: 2225 Sparsity: 0.988671875 Training loss: 2.6638505
Epoch: 2226 Sparsity: 0.976953125 Training loss: 2.4684958
Epoch: 2227 Sparsity: 0.9885850694444445 Training loss: 3.0959525
Epoch: 2228 Sparsity: 0.9847656249999999 Training loss: 2.446015
Epoch: 2229 Sparsity: 0.9798177083333333 Training loss: 2.7902071
Epoch: 2230 Sparsity: 0.9906684027777779 Training loss: 2.5959558
Epoch: 2231 Sparsity: 0.9848090277777779 Training loss: 4.549109
Epoch: 2232 Sparsity: 0.9904079861111112 Training loss: 2.5187142
Epoch: 2233 Sparsity: 0.9852864583333334 Training loss: 2.374848
Epoch: 2234 Sparsity: 0.9838975694444445 Training loss: 3.2621527
Epoch: 2235 Sparsity: 0.9851128472222221 Training loss: 2.3506703
Epoch: 2236 Sparsity: 0.9890190972222224 Training loss: 2.5483654
Epoch: 2237 Sparsity: 0.980078125 Training loss: 2.404782
Epoch: 2238 Sparsity: 0.988107638888889 Training loss: 2.4873781
Epoch: 2239 Sparsity: 0.9898871527777777 Training loss: 2.8056612
Epoch: 2240 Sparsity: 0.9720486111111113 Training loss: 2.5083635
Epoch: 2241 Sparsity: 0.9860243055555555 Training loss: 2.903984
Epoch: 2242 Sparsity: 0.9840277777777777 Training loss: 2.3535318
Epoch: 2243 Sparsity: 0.9823350694444445 Training loss: 2.2948453
Epoch: 2244 Sparsity: 0.9859375 Training loss: 2.682933
Epoch: 2245 Sparsity: 0.9926649305555555 Training loss: 2.6487164
Epoch: 2246 Sparsity: 0.98671875 Training loss: 5.2549605
Epoch: 2247 Sparsity: 0.9895399305555556 Training loss: 3.096002
Epoch: 2248 Sparsity: 0.9745659722222222 Training loss: 2.906342
Epoch: 2249 Sparsity: 0.9907986111111111 Training loss: 2.639858
Epoch: 2250 Sparsity: 0.9837673611111111 Training loss: 2.695647
Epoch: 2251 Sparsity: 0.9898871527777777 Training loss: 3.4460175
Epoch: 2252 Sparsity: 0.9709635416666667 Training loss: 3.788312
Epoch: 2253 Sparsity: 0.9877604166666668 Training loss: 3.0692055
Epoch: 2254 Sparsity: 0.976345486111111 Training loss: 2.5272343
Epoch: 2255 Sparsity: 0.988107638888889 Training loss: 3.3200457
Epoch: 2256 Sparsity: 0.9836371527777779 Training loss: 2.7740486
Epoch: 2257 Sparsity: 0.9851996527777779 Training loss: 2.3643298
Epoch: 2258 Sparsity: 0.9737413194444443 Training loss: 2.4910097
Epoch: 2259 Sparsity: 0.9819010416666668 Training loss: 2.3586502
Epoch: 2260 Sparsity: 0.9914062499999998 Training loss: 2.4326293
Epoch: 2261 Sparsity: 0.9805121527777778 Training loss: 2.3885965
Epoch: 2262 Sparsity: 0.9882378472222222 Training loss: 2.7974536
Epoch: 2263 Sparsity: 0.9707899305555555 Training loss: 2.4041586
Epoch: 2264 Sparsity: 0.9886284722222222 Training loss: 3.1422477
Epoch: 2265 Sparsity: 0.9827256944444445 Training loss: 2.4574785
Epoch: 2266 Sparsity: 0.98828125 Training loss: 2.6808043
Epoch: 2267 Sparsity: 0.9674913194444444 Training loss: 3.487918
Epoch: 2268 Sparsity: 0.9875434027777779 Training loss: 2.8049598
Epoch: 2269 Sparsity: 0.9903211805555555 Training loss: 2.4693897
Epoch: 2270 Sparsity: 0.9833767361111112 Training loss: 2.667817
Epoch: 2271 Sparsity: 0.9862847222222222 Training loss: 2.4696138
Epoch: 2272 Sparsity: 0.9795138888888889 Training loss: 2.5610442
Epoch: 2273 Sparsity: 0.9872829861111111 Training loss: 2.7279992
Epoch: 2274 Sparsity: 0.976779513888889 Training loss: 2.8299575
Epoch: 2275 Sparsity: 0.9822048611111113 Training loss: 2.3221242
Epoch: 2276 Sparsity: 0.9865885416666667 Training loss: 2.761403
Epoch: 2277 Sparsity: 0.9736111111111111 Training loss: 3.4966416
Epoch: 2278 Sparsity: 0.9881510416666668 Training loss: 2.7239366
Epoch: 2279 Sparsity: 0.9799045138888888 Training loss: 2.6051378
Epoch: 2280 Sparsity: 0.9880642361111113 Training loss: 2.843553
Epoch: 2281 Sparsity: 0.9736545138888889 Training loss: 3.482788
Epoch: 2282 Sparsity: 0.9859375 Training loss: 2.664289
Epoch: 2283 Sparsity: 0.975 Training loss: 2.313031
Epoch: 2284 Sparsity: 0.9874131944444443 Training loss: 3.0255976
Epoch: 2285 Sparsity: 0.9837673611111113 Training loss: 3.8956437
Epoch: 2286 Sparsity: 0.9875434027777776 Training loss: 2.6941817
Epoch: 2287 Sparsity: 0.9811631944444444 Training loss: 2.904681
Epoch: 2288 Sparsity: 0.988107638888889 Training loss: 2.6335003
Epoch: 2289 Sparsity: 0.9851996527777779 Training loss: 2.36097
Epoch: 2290 Sparsity: 0.9812934027777779 Training loss: 2.4249923
Epoch: 2291 Sparsity: 0.9889756944444443 Training loss: 2.3514793
Epoch: 2292 Sparsity: 0.9722222222222221 Training loss: 3.1357388
Epoch: 2293 Sparsity: 0.9825520833333334 Training loss: 2.4586194
Epoch: 2294 Sparsity: 0.985546875 Training loss: 2.488585
Epoch: 2295 Sparsity: 0.9649305555555554 Training loss: 3.1380122
Epoch: 2296 Sparsity: 0.981640625 Training loss: 3.0309289
Epoch: 2297 Sparsity: 0.9758246527777779 Training loss: 2.4500797
Epoch: 2298 Sparsity: 0.9845486111111112 Training loss: 2.7562325
Epoch: 2299 Sparsity: 0.9769965277777779 Training loss: 2.3761482
Epoch: 2300 Sparsity: 0.9838975694444445 Training loss: 2.420197
Epoch: 2301 Sparsity: 0.9813802083333334 Training loss: 2.7544234
Epoch: 2302 Sparsity: 0.9852864583333332 Training loss: 2.817248
Epoch: 2303 Sparsity: 0.9782552083333333 Training loss: 2.3247366
Epoch: 2304 Sparsity: 0.9786024305555557 Training loss: 2.3346777
Epoch: 2305 Sparsity: 0.9875434027777776 Training loss: 2.7541964
Epoch: 2306 Sparsity: 0.9617621527777777 Training loss: 3.5830007
Epoch: 2307 Sparsity: 0.9853732638888889 Training loss: 2.393863
Epoch: 2308 Sparsity: 0.9781684027777777 Training loss: 2.5068815
Epoch: 2309 Sparsity: 0.9789930555555555 Training loss: 3.3436432
Epoch: 2310 Sparsity: 0.9810763888888889 Training loss: 2.3780534
Epoch: 2311 Sparsity: 0.9659722222222223 Training loss: 2.3667212
Epoch: 2312 Sparsity: 0.984548611111111 Training loss: 2.7576487
Epoch: 2313 Sparsity: 0.9794270833333334 Training loss: 2.4768596
Epoch: 2314 Sparsity: 0.9835937499999998 Training loss: 2.8222718
Epoch: 2315 Sparsity: 0.9825086805555555 Training loss: 2.3964808
Epoch: 2316 Sparsity: 0.9829861111111111 Training loss: 2.5114982
Epoch: 2317 Sparsity: 0.9916232638888889 Training loss: 2.4017236
Epoch: 2318 Sparsity: 0.9824652777777778 Training loss: 2.8757005
Epoch: 2319 Sparsity: 0.9890190972222221 Training loss: 2.7947454
Epoch: 2320 Sparsity: 0.9792100694444444 Training loss: 2.45515
Epoch: 2321 Sparsity: 0.9889756944444447 Training loss: 2.582297
Epoch: 2322 Sparsity: 0.9808593750000002 Training loss: 2.447922
Epoch: 2323 Sparsity: 0.9873697916666666 Training loss: 2.6789732
Epoch: 2324 Sparsity: 0.9785590277777778 Training loss: 2.4897928
Epoch: 2325 Sparsity: 0.9856336805555556 Training loss: 2.7687497
Epoch: 2326 Sparsity: 0.9811197916666667 Training loss: 2.3159797
Epoch: 2327 Sparsity: 0.9856770833333334 Training loss: 2.4314804
Epoch: 2328 Sparsity: 0.9769097222222222 Training loss: 2.4282966
Epoch: 2329 Sparsity: 0.986328125 Training loss: 2.498316
Epoch: 2330 Sparsity: 0.9736111111111111 Training loss: 2.5417123
Epoch: 2331 Sparsity: 0.986154513888889 Training loss: 2.8626294
Epoch: 2332 Sparsity: 0.9799479166666666 Training loss: 2.3861024
Epoch: 2333 Sparsity: 0.988498263888889 Training loss: 2.8641477
Epoch: 2334 Sparsity: 0.9813368055555556 Training loss: 5.1688995
Epoch: 2335 Sparsity: 0.9841145833333333 Training loss: 2.5485423
Epoch: 2336 Sparsity: 0.959765625 Training loss: 2.7747226
Epoch: 2337 Sparsity: 0.9848524305555555 Training loss: 4.082272
Epoch: 2338 Sparsity: 0.9637586805555556 Training loss: 2.4599047
Epoch: 2339 Sparsity: 0.9834201388888888 Training loss: 2.6249316
Epoch: 2340 Sparsity: 0.978298611111111 Training loss: 2.315634
Epoch: 2341 Sparsity: 0.982421875 Training loss: 2.426391
Epoch: 2342 Sparsity: 0.9869791666666666 Training loss: 2.606922
Epoch: 2343 Sparsity: 0.9811197916666666 Training loss: 2.489659
Epoch: 2344 Sparsity: 0.9597222222222221 Training loss: 2.7887623
Epoch: 2345 Sparsity: 0.9761284722222223 Training loss: 4.86255
Epoch: 2346 Sparsity: 0.9746961805555557 Training loss: 2.303703
Epoch: 2347 Sparsity: 0.9782552083333332 Training loss: 2.7663717
Epoch: 2348 Sparsity: 0.9797743055555556 Training loss: 2.7245545
Epoch: 2349 Sparsity: 0.965234375 Training loss: 2.4167097
Epoch: 2350 Sparsity: 0.9870659722222224 Training loss: 3.4161537
Epoch: 2351 Sparsity: 0.9720052083333334 Training loss: 2.382762
Epoch: 2352 Sparsity: 0.9875868055555556 Training loss: 2.362034
Epoch: 2353 Sparsity: 0.9784722222222222 Training loss: 2.3968978
Epoch: 2354 Sparsity: 0.9891493055555556 Training loss: 2.504094
Epoch: 2355 Sparsity: 0.9851996527777779 Training loss: 2.7872894
Epoch: 2356 Sparsity: 0.9782552083333332 Training loss: 2.6044555
Epoch: 2357 Sparsity: 0.9834635416666666 Training loss: 2.5971706
Epoch: 2358 Sparsity: 0.9822916666666668 Training loss: 2.3869398
Epoch: 2359 Sparsity: 0.9901041666666666 Training loss: 2.7157207
Epoch: 2360 Sparsity: 0.9836371527777779 Training loss: 3.627445
Epoch: 2361 Sparsity: 0.9859809027777778 Training loss: 2.7784958
Epoch: 2362 Sparsity: 0.974826388888889 Training loss: 2.7796564
Epoch: 2363 Sparsity: 0.9861979166666666 Training loss: 3.0920749
Epoch: 2364 Sparsity: 0.9775173611111111 Training loss: 2.502732
Epoch: 2365 Sparsity: 0.985546875 Training loss: 2.513565
Epoch: 2366 Sparsity: 0.9741319444444446 Training loss: 2.502153
Epoch: 2367 Sparsity: 0.9833333333333332 Training loss: 2.4278777
Epoch: 2368 Sparsity: 0.9800347222222223 Training loss: 2.9319806
Epoch: 2369 Sparsity: 0.9828125 Training loss: 2.7331302
Epoch: 2370 Sparsity: 0.9681423611111111 Training loss: 2.4859185
Epoch: 2371 Sparsity: 0.9848958333333334 Training loss: 2.6926258
Epoch: 2372 Sparsity: 0.980685763888889 Training loss: 2.3270464
Epoch: 2373 Sparsity: 0.984201388888889 Training loss: 3.2915688
Epoch: 2374 Sparsity: 0.9835069444444444 Training loss: 2.3041475
Epoch: 2375 Sparsity: 0.9777777777777776 Training loss: 2.4712338
Epoch: 2376 Sparsity: 0.9896701388888889 Training loss: 2.4986038
Epoch: 2377 Sparsity: 0.9847222222222222 Training loss: 2.398949
Epoch: 2378 Sparsity: 0.9856336805555556 Training loss: 2.3536808
Epoch: 2379 Sparsity: 0.990407986111111 Training loss: 2.3617501
Epoch: 2380 Sparsity: 0.980295138888889 Training loss: 2.543879
Epoch: 2381 Sparsity: 0.9894965277777779 Training loss: 2.6730423
Epoch: 2382 Sparsity: 0.9821180555555555 Training loss: 2.4077983
Epoch: 2383 Sparsity: 0.9801215277777778 Training loss: 2.5007887
Epoch: 2384 Sparsity: 0.9890190972222224 Training loss: 2.3726623
Epoch: 2385 Sparsity: 0.9767361111111112 Training loss: 3.8658748
Epoch: 2386 Sparsity: 0.9746527777777778 Training loss: 2.4712272
Epoch: 2387 Sparsity: 0.9844184027777778 Training loss: 2.6691134
Epoch: 2388 Sparsity: 0.9733506944444444 Training loss: 2.8885431
Epoch: 2389 Sparsity: 0.9851128472222224 Training loss: 2.4539008
Epoch: 2390 Sparsity: 0.9704861111111113 Training loss: 2.868743
Epoch: 2391 Sparsity: 0.9855034722222221 Training loss: 2.7224889
Epoch: 2392 Sparsity: 0.979296875 Training loss: 2.4144585
Epoch: 2393 Sparsity: 0.9844184027777778 Training loss: 2.7769883
Epoch: 2394 Sparsity: 0.9729600694444442 Training loss: 2.5354095
Epoch: 2395 Sparsity: 0.979296875 Training loss: 2.682946
Epoch: 2396 Sparsity: 0.9799045138888889 Training loss: 2.3753085
Epoch: 2397 Sparsity: 0.9846788194444445 Training loss: 2.96335
Epoch: 2398 Sparsity: 0.9671875 Training loss: 3.8953085
Epoch: 2399 Sparsity: 0.9817708333333334 Training loss: 2.7544982
Epoch: 2400 Sparsity: 0.9739149305555556 Training loss: 2.3265672
Epoch: 2401 Sparsity: 0.9827690972222223 Training loss: 2.7898147
Epoch: 2402 Sparsity: 0.9681423611111111 Training loss: 2.3021657
Epoch: 2403 Sparsity: 0.9866319444444445 Training loss: 2.5819724
Epoch: 2404 Sparsity: 0.9703993055555555 Training loss: 2.35451
Epoch: 2405 Sparsity: 0.9783854166666666 Training loss: 2.3293834
Epoch: 2406 Sparsity: 0.9898003472222221 Training loss: 2.6334875
Epoch: 2407 Sparsity: 0.9798611111111113 Training loss: 2.6952798
Epoch: 2408 Sparsity: 0.9805555555555555 Training loss: 3.0821054
Epoch: 2409 Sparsity: 0.968576388888889 Training loss: 2.5001113
Epoch: 2410 Sparsity: 0.9861979166666666 Training loss: 3.126637
Epoch: 2411 Sparsity: 0.9786892361111112 Training loss: 2.31903
Epoch: 2412 Sparsity: 0.9848524305555555 Training loss: 2.3521733
Epoch: 2413 Sparsity: 0.9859809027777778 Training loss: 2.9399269
Epoch: 2414 Sparsity: 0.960763888888889 Training loss: 4.4847455
Epoch: 2415 Sparsity: 0.980078125 Training loss: 2.6203263
Epoch: 2416 Sparsity: 0.9644097222222221 Training loss: 3.7433565
Epoch: 2417 Sparsity: 0.9844618055555555 Training loss: 2.717828
Epoch: 2418 Sparsity: 0.978342013888889 Training loss: 2.365423
Epoch: 2419 Sparsity: 0.9817708333333333 Training loss: 2.4134605
Epoch: 2420 Sparsity: 0.9816840277777779 Training loss: 2.3938143
Epoch: 2421 Sparsity: 0.982204861111111 Training loss: 2.7594178
Epoch: 2422 Sparsity: 0.947482638888889 Training loss: 4.1060395
Epoch: 2423 Sparsity: 0.9772569444444444 Training loss: 2.4322627
Epoch: 2424 Sparsity: 0.9540798611111111 Training loss: 2.6671968
Epoch: 2425 Sparsity: 0.9799479166666666 Training loss: 3.0813208
Epoch: 2426 Sparsity: 0.9765625 Training loss: 2.4695692
Epoch: 2427 Sparsity: 0.9714409722222221 Training loss: 2.3163564
Epoch: 2428 Sparsity: 0.9752604166666666 Training loss: 2.6529393
Epoch: 2429 Sparsity: 0.9860677083333333 Training loss: 2.5351791
Epoch: 2430 Sparsity: 0.9669704861111112 Training loss: 3.3431525
Epoch: 2431 Sparsity: 0.9836805555555556 Training loss: 2.6003296
Epoch: 2432 Sparsity: 0.9764756944444445 Training loss: 2.597415
Epoch: 2433 Sparsity: 0.9692274305555555 Training loss: 2.4068136
Epoch: 2434 Sparsity: 0.9833333333333334 Training loss: 2.4785864
Epoch: 2435 Sparsity: 0.9733072916666666 Training loss: 2.9338796
Epoch: 2436 Sparsity: 0.9838107638888889 Training loss: 2.5081131
Epoch: 2437 Sparsity: 0.97109375 Training loss: 2.5517104
Epoch: 2438 Sparsity: 0.9814670138888889 Training loss: 2.6100886
Epoch: 2439 Sparsity: 0.9583333333333334 Training loss: 2.612948
Epoch: 2440 Sparsity: 0.9697048611111112 Training loss: 3.083333
Epoch: 2441 Sparsity: 0.9788194444444445 Training loss: 2.498391
Epoch: 2442 Sparsity: 0.9784722222222222 Training loss: 2.3474767
Epoch: 2443 Sparsity: 0.9784722222222223 Training loss: 2.8205664
Epoch: 2444 Sparsity: 0.975 Training loss: 2.3886676
Epoch: 2445 Sparsity: 0.9824652777777777 Training loss: 2.3728297
Epoch: 2446 Sparsity: 0.98125 Training loss: 2.3985066
Epoch: 2447 Sparsity: 0.9887586805555557 Training loss: 2.458149
Epoch: 2448 Sparsity: 0.9831163194444444 Training loss: 2.5954711
Epoch: 2449 Sparsity: 0.9782552083333333 Training loss: 2.3908384
Epoch: 2450 Sparsity: 0.9859809027777777 Training loss: 2.8309166
Epoch: 2451 Sparsity: 0.9733506944444447 Training loss: 2.367668
Epoch: 2452 Sparsity: 0.9828559027777779 Training loss: 2.3368607
Epoch: 2453 Sparsity: 0.9732638888888889 Training loss: 2.33241
Epoch: 2454 Sparsity: 0.9796440972222221 Training loss: 2.7592194
Epoch: 2455 Sparsity: 0.9816840277777776 Training loss: 2.3271
Epoch: 2456 Sparsity: 0.9887586805555557 Training loss: 2.5898921
Epoch: 2457 Sparsity: 0.9765190972222223 Training loss: 3.652925
Epoch: 2458 Sparsity: 0.9736979166666668 Training loss: 2.2866292
Epoch: 2459 Sparsity: 0.978732638888889 Training loss: 3.0348825
Epoch: 2460 Sparsity: 0.9853732638888889 Training loss: 2.5532563
Epoch: 2461 Sparsity: 0.9680555555555557 Training loss: 2.5780592
Epoch: 2462 Sparsity: 0.976953125 Training loss: 3.2303276
Epoch: 2463 Sparsity: 0.978689236111111 Training loss: 2.4575841
Epoch: 2464 Sparsity: 0.9856336805555556 Training loss: 2.4252589
Epoch: 2465 Sparsity: 0.9797743055555556 Training loss: 2.5077758
Epoch: 2466 Sparsity: 0.9799913194444443 Training loss: 2.3480706
Epoch: 2467 Sparsity: 0.9882378472222223 Training loss: 2.3495185
Epoch: 2468 Sparsity: 0.9790798611111111 Training loss: 2.310582
Epoch: 2469 Sparsity: 0.9743923611111113 Training loss: 2.3618507
Epoch: 2470 Sparsity: 0.9825954861111112 Training loss: 2.589908
Epoch: 2471 Sparsity: 0.9813802083333332 Training loss: 2.3396287
Epoch: 2472 Sparsity: 0.9799045138888888 Training loss: 2.3769138
Epoch: 2473 Sparsity: 0.988454861111111 Training loss: 2.4940958
Epoch: 2474 Sparsity: 0.9718315972222221 Training loss: 2.8654974
Epoch: 2475 Sparsity: 0.9809895833333334 Training loss: 2.339397
Epoch: 2476 Sparsity: 0.9924479166666668 Training loss: 2.452012
Epoch: 2477 Sparsity: 0.9791232638888887 Training loss: 2.5134492
Epoch: 2478 Sparsity: 0.9814236111111111 Training loss: 2.3418705
Epoch: 2479 Sparsity: 0.9889322916666664 Training loss: 2.617705
Epoch: 2480 Sparsity: 0.9802083333333333 Training loss: 3.1566083
Epoch: 2481 Sparsity: 0.9857204861111111 Training loss: 2.9477127
Epoch: 2482 Sparsity: 0.9908854166666666 Training loss: 2.4349241
Epoch: 2483 Sparsity: 0.9823350694444445 Training loss: 2.2960687
Epoch: 2484 Sparsity: 0.9897569444444446 Training loss: 2.3717768
Epoch: 2485 Sparsity: 0.9810329861111111 Training loss: 2.292662
Epoch: 2486 Sparsity: 0.9858072916666666 Training loss: 2.765609
Epoch: 2487 Sparsity: 0.9797309027777779 Training loss: 2.5671737
Epoch: 2488 Sparsity: 0.987109375 Training loss: 2.9186442
Epoch: 2489 Sparsity: 0.9673177083333332 Training loss: 2.8825984
Epoch: 2490 Sparsity: 0.9874565972222223 Training loss: 2.9085128
Epoch: 2491 Sparsity: 0.9719184027777776 Training loss: 2.8598206
Epoch: 2492 Sparsity: 0.9857638888888889 Training loss: 2.83236
Epoch: 2493 Sparsity: 0.9740885416666666 Training loss: 2.3051538
Epoch: 2494 Sparsity: 0.9871527777777779 Training loss: 2.8378475
Epoch: 2495 Sparsity: 0.9785590277777777 Training loss: 3.0697885
Epoch: 2496 Sparsity: 0.9819878472222223 Training loss: 2.666248
Epoch: 2497 Sparsity: 0.9620659722222223 Training loss: 2.6995199
Epoch: 2498 Sparsity: 0.9827256944444442 Training loss: 3.36859
Epoch: 2499 Sparsity: 0.9809895833333334 Training loss: 2.3160067
Epoch: 2500 Sparsity: 0.9821614583333332 Training loss: 2.536556
Epoch: 2501 Sparsity: 0.9895833333333334 Training loss: 2.5659537
Epoch: 2502 Sparsity: 0.984548611111111 Training loss: 2.3660479
Epoch: 2503 Sparsity: 0.9876302083333334 Training loss: 2.7718046
Epoch: 2504 Sparsity: 0.969921875 Training loss: 3.5968692
Epoch: 2505 Sparsity: 0.9737413194444444 Training loss: 2.2956622
Epoch: 2506 Sparsity: 0.9859809027777778 Training loss: 3.4164925
Epoch: 2507 Sparsity: 0.9836805555555556 Training loss: 2.467836
Epoch: 2508 Sparsity: 0.9784722222222222 Training loss: 2.3343203
Epoch: 2509 Sparsity: 0.9862413194444445 Training loss: 2.9496043
Epoch: 2510 Sparsity: 0.9736545138888889 Training loss: 2.4990628
Epoch: 2511 Sparsity: 0.9877604166666666 Training loss: 3.1482978
Epoch: 2512 Sparsity: 0.9807725694444445 Training loss: 2.5354962
Epoch: 2513 Sparsity: 0.9713107638888889 Training loss: 3.2251153
Epoch: 2514 Sparsity: 0.9861111111111113 Training loss: 2.8353157
Epoch: 2515 Sparsity: 0.9755642361111112 Training loss: 2.3331301
Epoch: 2516 Sparsity: 0.980859375 Training loss: 2.3317907
Epoch: 2517 Sparsity: 0.990017361111111 Training loss: 2.5202994
Epoch: 2518 Sparsity: 0.9828125 Training loss: 2.301837
Epoch: 2519 Sparsity: 0.9616319444444444 Training loss: 2.7344697
Epoch: 2520 Sparsity: 0.9834635416666666 Training loss: 3.0385156
Epoch: 2521 Sparsity: 0.982595486111111 Training loss: 2.387492
Epoch: 2522 Sparsity: 0.9817274305555556 Training loss: 2.3927612
Epoch: 2523 Sparsity: 0.9875434027777776 Training loss: 2.43248
Epoch: 2524 Sparsity: 0.9868489583333334 Training loss: 2.4012716
Epoch: 2525 Sparsity: 0.9885850694444442 Training loss: 2.3983598
Epoch: 2526 Sparsity: 0.9773871527777777 Training loss: 4.4650583
Epoch: 2527 Sparsity: 0.9901909722222223 Training loss: 2.6783533
Epoch: 2528 Sparsity: 0.9823350694444443 Training loss: 4.099495
Epoch: 2529 Sparsity: 0.98671875 Training loss: 3.0569627
Epoch: 2530 Sparsity: 0.9726128472222222 Training loss: 2.6393335
Epoch: 2531 Sparsity: 0.9866319444444445 Training loss: 2.7404253
Epoch: 2532 Sparsity: 0.9903645833333332 Training loss: 2.5249019
Epoch: 2533 Sparsity: 0.972482638888889 Training loss: 3.4121544
Epoch: 2534 Sparsity: 0.9864149305555554 Training loss: 2.8571687
Epoch: 2535 Sparsity: 0.9704427083333333 Training loss: 2.4453168
Epoch: 2536 Sparsity: 0.9830729166666667 Training loss: 2.7554967
Epoch: 2537 Sparsity: 0.984157986111111 Training loss: 2.4195487
Epoch: 2538 Sparsity: 0.959375 Training loss: 3.9235113
Epoch: 2539 Sparsity: 0.981640625 Training loss: 2.6452863
Epoch: 2540 Sparsity: 0.9603732638888889 Training loss: 2.7499657
Epoch: 2541 Sparsity: 0.9817274305555556 Training loss: 2.8171308
Epoch: 2542 Sparsity: 0.9659288194444444 Training loss: 2.5694013
Epoch: 2543 Sparsity: 0.9842447916666666 Training loss: 2.556973
Epoch: 2544 Sparsity: 0.9717447916666668 Training loss: 2.3115613
Epoch: 2545 Sparsity: 0.9577690972222221 Training loss: 2.312546
Epoch: 2546 Sparsity: 0.9817708333333333 Training loss: 2.8907537
Epoch: 2547 Sparsity: 0.970486111111111 Training loss: 2.6183844
Epoch: 2548 Sparsity: 0.9865017361111112 Training loss: 2.655475
Epoch: 2549 Sparsity: 0.9781684027777778 Training loss: 2.3326635
Epoch: 2550 Sparsity: 0.9601996527777776 Training loss: 2.6596491
Epoch: 2551 Sparsity: 0.9840711805555555 Training loss: 2.7986097
Epoch: 2552 Sparsity: 0.9660590277777776 Training loss: 2.5414212
Epoch: 2553 Sparsity: 0.9776909722222221 Training loss: 2.7436478
Epoch: 2554 Sparsity: 0.9840277777777778 Training loss: 2.3509977
Epoch: 2555 Sparsity: 0.986501736111111 Training loss: 2.5780537
Epoch: 2556 Sparsity: 0.9707031250000002 Training loss: 2.6444955
Epoch: 2557 Sparsity: 0.9844184027777777 Training loss: 2.6447155
Epoch: 2558 Sparsity: 0.9713975694444443 Training loss: 2.4391792
Epoch: 2559 Sparsity: 0.9812065972222221 Training loss: 2.7014353
Epoch: 2560 Sparsity: 0.977734375 Training loss: 2.5261352
Epoch: 2561 Sparsity: 0.9816840277777776 Training loss: 2.3426344
Epoch: 2562 Sparsity: 0.9843315972222223 Training loss: 2.3629808
Epoch: 2563 Sparsity: 0.9740017361111111 Training loss: 3.109807
Epoch: 2564 Sparsity: 0.9845920138888887 Training loss: 2.769188
Epoch: 2565 Sparsity: 0.968967013888889 Training loss: 2.5248325
Epoch: 2566 Sparsity: 0.9840711805555555 Training loss: 2.6355178
Epoch: 2567 Sparsity: 0.9787326388888887 Training loss: 2.5633965
Epoch: 2568 Sparsity: 0.9869357638888889 Training loss: 2.4629996
Epoch: 2569 Sparsity: 0.9789496527777777 Training loss: 3.8054612
Epoch: 2570 Sparsity: 0.9887152777777779 Training loss: 2.7027583
Epoch: 2571 Sparsity: 0.9814670138888889 Training loss: 2.552068
Epoch: 2572 Sparsity: 0.9894097222222221 Training loss: 2.6091383
Epoch: 2573 Sparsity: 0.9782986111111113 Training loss: 3.6574807
Epoch: 2574 Sparsity: 0.9758680555555556 Training loss: 2.5067222
Epoch: 2575 Sparsity: 0.9693576388888889 Training loss: 2.3199613
Epoch: 2576 Sparsity: 0.9859374999999998 Training loss: 3.4217575
Epoch: 2577 Sparsity: 0.97578125 Training loss: 2.6536403
Epoch: 2578 Sparsity: 0.9836805555555556 Training loss: 3.2844038
Epoch: 2579 Sparsity: 0.9739149305555556 Training loss: 2.6283617
Epoch: 2580 Sparsity: 0.9819444444444445 Training loss: 2.3624473
Epoch: 2581 Sparsity: 0.978732638888889 Training loss: 2.426636
Epoch: 2582 Sparsity: 0.9838975694444445 Training loss: 2.4284678
Epoch: 2583 Sparsity: 0.9837673611111111 Training loss: 2.8923228
Epoch: 2584 Sparsity: 0.970529513888889 Training loss: 2.7075362
Epoch: 2585 Sparsity: 0.9821180555555555 Training loss: 2.5420904
Epoch: 2586 Sparsity: 0.9779513888888889 Training loss: 2.4510987
Epoch: 2587 Sparsity: 0.9839409722222223 Training loss: 2.29729
Epoch: 2588 Sparsity: 0.990451388888889 Training loss: 2.6330032
Epoch: 2589 Sparsity: 0.9788194444444445 Training loss: 3.1343625
Epoch: 2590 Sparsity: 0.9877604166666668 Training loss: 2.4725723
Epoch: 2591 Sparsity: 0.9899305555555555 Training loss: 2.6164105
Epoch: 2592 Sparsity: 0.9805989583333334 Training loss: 4.515921
Epoch: 2593 Sparsity: 0.9789930555555555 Training loss: 2.4557407
Epoch: 2594 Sparsity: 0.9887586805555555 Training loss: 2.6438055
Epoch: 2595 Sparsity: 0.9773437500000002 Training loss: 2.9634852
Epoch: 2596 Sparsity: 0.9883246527777778 Training loss: 2.63382
Epoch: 2597 Sparsity: 0.9744791666666666 Training loss: 2.574161
Epoch: 2598 Sparsity: 0.9856336805555556 Training loss: 2.439321
Epoch: 2599 Sparsity: 0.9778645833333334 Training loss: 2.477242
Epoch: 2600 Sparsity: 0.9866753472222223 Training loss: 3.1665366
Epoch: 2601 Sparsity: 0.9760850694444445 Training loss: 2.6447835
Epoch: 2602 Sparsity: 0.986328125 Training loss: 2.7736428
Epoch: 2603 Sparsity: 0.9809461805555555 Training loss: 2.3909402
Epoch: 2604 Sparsity: 0.9552517361111112 Training loss: 2.40802
Epoch: 2605 Sparsity: 0.9809461805555557 Training loss: 4.225018
Epoch: 2606 Sparsity: 0.9873263888888889 Training loss: 2.597434
Epoch: 2607 Sparsity: 0.9697916666666666 Training loss: 2.3314347
Epoch: 2608 Sparsity: 0.9871527777777779 Training loss: 2.8661704
Epoch: 2609 Sparsity: 0.9791666666666666 Training loss: 2.6593869
Epoch: 2610 Sparsity: 0.980295138888889 Training loss: 2.772618
Epoch: 2611 Sparsity: 0.9622829861111111 Training loss: 2.7269454
Epoch: 2612 Sparsity: 0.9858072916666666 Training loss: 3.0858502
Epoch: 2613 Sparsity: 0.9864583333333334 Training loss: 2.5789232
Epoch: 2614 Sparsity: 0.9721788194444445 Training loss: 2.655678
Epoch: 2615 Sparsity: 0.9848090277777777 Training loss: 2.438856
Epoch: 2616 Sparsity: 0.9747829861111109 Training loss: 2.4713557
Epoch: 2617 Sparsity: 0.9835503472222223 Training loss: 2.6635149
Epoch: 2618 Sparsity: 0.9713107638888889 Training loss: 2.4776952
Epoch: 2619 Sparsity: 0.9770833333333334 Training loss: 2.6471913
Epoch: 2620 Sparsity: 0.9770833333333334 Training loss: 2.444794
Epoch: 2621 Sparsity: 0.9803385416666668 Training loss: 2.6424778
Epoch: 2622 Sparsity: 0.9551215277777777 Training loss: 2.570785
Epoch: 2623 Sparsity: 0.9842447916666668 Training loss: 2.6093783
Epoch: 2624 Sparsity: 0.9668402777777777 Training loss: 2.6390824
Epoch: 2625 Sparsity: 0.9825520833333332 Training loss: 2.7387745
Epoch: 2626 Sparsity: 0.9825520833333334 Training loss: 3.1459498
Epoch: 2627 Sparsity: 0.973046875 Training loss: 2.9216883
Epoch: 2628 Sparsity: 0.9883246527777778 Training loss: 2.7591689
Epoch: 2629 Sparsity: 0.97890625 Training loss: 3.9590082
Epoch: 2630 Sparsity: 0.9850694444444443 Training loss: 2.524094
Epoch: 2631 Sparsity: 0.9741319444444445 Training loss: 2.3251634
Epoch: 2632 Sparsity: 0.981640625 Training loss: 2.4354532
Epoch: 2633 Sparsity: 0.9875868055555556 Training loss: 2.5970309
Epoch: 2634 Sparsity: 0.9867621527777779 Training loss: 2.4847383
Epoch: 2635 Sparsity: 0.9681423611111111 Training loss: 3.0968792
Epoch: 2636 Sparsity: 0.9831597222222224 Training loss: 2.3761175
Epoch: 2637 Sparsity: 0.9739583333333333 Training loss: 2.5200074
Epoch: 2638 Sparsity: 0.9821180555555558 Training loss: 2.3453228
Epoch: 2639 Sparsity: 0.9775173611111111 Training loss: 2.4175622
Epoch: 2640 Sparsity: 0.986154513888889 Training loss: 2.5800452
Epoch: 2641 Sparsity: 0.97578125 Training loss: 2.3464625
Epoch: 2642 Sparsity: 0.9901041666666666 Training loss: 2.5184
Epoch: 2643 Sparsity: 0.9805989583333334 Training loss: 2.967888
Epoch: 2644 Sparsity: 0.9843749999999998 Training loss: 2.937356
Epoch: 2645 Sparsity: 0.9793836805555556 Training loss: 2.4134734
Epoch: 2646 Sparsity: 0.9796006944444443 Training loss: 2.306299
Epoch: 2647 Sparsity: 0.985546875 Training loss: 3.5749059
Epoch: 2648 Sparsity: 0.9854166666666666 Training loss: 2.4852204
Epoch: 2649 Sparsity: 0.9916232638888889 Training loss: 2.7121637
Epoch: 2650 Sparsity: 0.9838975694444445 Training loss: 2.693133
Epoch: 2651 Sparsity: 0.9857204861111113 Training loss: 2.8461955
Epoch: 2652 Sparsity: 0.992404513888889 Training loss: 2.3717465
Epoch: 2653 Sparsity: 0.9800781250000001 Training loss: 3.8563278
Epoch: 2654 Sparsity: 0.9907118055555554 Training loss: 2.6888251
Epoch: 2655 Sparsity: 0.9832465277777777 Training loss: 2.8113089
Epoch: 2656 Sparsity: 0.98359375 Training loss: 2.5955825
Epoch: 2657 Sparsity: 0.988454861111111 Training loss: 2.7404776
Epoch: 2658 Sparsity: 0.9796875 Training loss: 2.9388628
Epoch: 2659 Sparsity: 0.9854600694444444 Training loss: 2.342138
Epoch: 2660 Sparsity: 0.992404513888889 Training loss: 2.4705575
Epoch: 2661 Sparsity: 0.9809027777777779 Training loss: 2.9336298
Epoch: 2662 Sparsity: 0.9882378472222223 Training loss: 2.529404
Epoch: 2663 Sparsity: 0.9799913194444445 Training loss: 2.3559964
Epoch: 2664 Sparsity: 0.9909722222222221 Training loss: 2.8115695
Epoch: 2665 Sparsity: 0.9785590277777777 Training loss: 2.580964
Epoch: 2666 Sparsity: 0.9893229166666668 Training loss: 2.9714627
Epoch: 2667 Sparsity: 0.9896701388888888 Training loss: 2.3639991
Epoch: 2668 Sparsity: 0.9810329861111111 Training loss: 3.3133705
Epoch: 2669 Sparsity: 0.9884548611111112 Training loss: 2.659246
Epoch: 2670 Sparsity: 0.9792534722222221 Training loss: 2.3429253
Epoch: 2671 Sparsity: 0.9890625000000002 Training loss: 2.3648639
Epoch: 2672 Sparsity: 0.9855902777777776 Training loss: 2.3165169
Epoch: 2673 Sparsity: 0.9819444444444445 Training loss: 2.6341364
Epoch: 2674 Sparsity: 0.9830295138888889 Training loss: 2.3154404
Epoch: 2675 Sparsity: 0.9868055555555555 Training loss: 2.3334188
Epoch: 2676 Sparsity: 0.9911892361111111 Training loss: 2.4485528
Epoch: 2677 Sparsity: 0.9764322916666666 Training loss: 2.5484798
Epoch: 2678 Sparsity: 0.9861979166666666 Training loss: 2.8815286
Epoch: 2679 Sparsity: 0.9892361111111112 Training loss: 2.86919
Epoch: 2680 Sparsity: 0.9791232638888889 Training loss: 2.2888465
Epoch: 2681 Sparsity: 0.9883246527777778 Training loss: 2.731851
Epoch: 2682 Sparsity: 0.9737847222222221 Training loss: 2.4919963
Epoch: 2683 Sparsity: 0.9883680555555555 Training loss: 3.1202905
Epoch: 2684 Sparsity: 0.9786458333333334 Training loss: 3.5528624
Epoch: 2685 Sparsity: 0.9885416666666667 Training loss: 2.438032
Epoch: 2686 Sparsity: 0.9797743055555557 Training loss: 2.537196
Epoch: 2687 Sparsity: 0.9873263888888889 Training loss: 2.4668987
Epoch: 2688 Sparsity: 0.978515625 Training loss: 2.702945
Epoch: 2689 Sparsity: 0.9879340277777778 Training loss: 2.5691528
Epoch: 2690 Sparsity: 0.9753472222222224 Training loss: 4.2338095
Epoch: 2691 Sparsity: 0.9876302083333334 Training loss: 2.6697328
Epoch: 2692 Sparsity: 0.9761284722222221 Training loss: 2.714625
Epoch: 2693 Sparsity: 0.9862847222222222 Training loss: 2.4561849
Epoch: 2694 Sparsity: 0.9744791666666665 Training loss: 2.4949749
Epoch: 2695 Sparsity: 0.9720920138888889 Training loss: 2.300409
Epoch: 2696 Sparsity: 0.9766059027777777 Training loss: 2.351533
Epoch: 2697 Sparsity: 0.9847222222222222 Training loss: 2.417705
Epoch: 2698 Sparsity: 0.9643229166666668 Training loss: 3.1782045
Epoch: 2699 Sparsity: 0.9848524305555557 Training loss: 2.4155614
Epoch: 2700 Sparsity: 0.9788628472222222 Training loss: 2.4861069
Epoch: 2701 Sparsity: 0.987890625 Training loss: 2.7287588
Epoch: 2702 Sparsity: 0.9792534722222221 Training loss: 2.414331
Epoch: 2703 Sparsity: 0.9842447916666666 Training loss: 2.4047034
Epoch: 2704 Sparsity: 0.9775607638888889 Training loss: 2.3770134
Epoch: 2705 Sparsity: 0.9894965277777776 Training loss: 2.5218494
Epoch: 2706 Sparsity: 0.9772569444444444 Training loss: 2.734698
Epoch: 2707 Sparsity: 0.9849392361111112 Training loss: 2.4308057
Epoch: 2708 Sparsity: 0.9720052083333334 Training loss: 2.4480379
Epoch: 2709 Sparsity: 0.982248263888889 Training loss: 2.4565558
Epoch: 2710 Sparsity: 0.9858940972222221 Training loss: 2.69542
Epoch: 2711 Sparsity: 0.9714409722222224 Training loss: 2.56425
Epoch: 2712 Sparsity: 0.9866753472222222 Training loss: 3.025643
Epoch: 2713 Sparsity: 0.9764322916666668 Training loss: 2.585987
Epoch: 2714 Sparsity: 0.9791232638888889 Training loss: 2.3394673
Epoch: 2715 Sparsity: 0.9823784722222223 Training loss: 2.6505394
Epoch: 2716 Sparsity: 0.987109375 Training loss: 2.6161222
Epoch: 2717 Sparsity: 0.9717881944444444 Training loss: 2.3764167
Epoch: 2718 Sparsity: 0.985546875 Training loss: 2.6344192
Epoch: 2719 Sparsity: 0.9819878472222221 Training loss: 2.3531315
Epoch: 2720 Sparsity: 0.9842447916666668 Training loss: 2.341172
Epoch: 2721 Sparsity: 0.9659288194444443 Training loss: 2.6276777
Epoch: 2722 Sparsity: 0.9837239583333333 Training loss: 3.2574944
Epoch: 2723 Sparsity: 0.97578125 Training loss: 2.5991333
Epoch: 2724 Sparsity: 0.9750434027777777 Training loss: 2.9361129
Epoch: 2725 Sparsity: 0.9796875 Training loss: 2.4502003
Epoch: 2726 Sparsity: 0.9691840277777777 Training loss: 2.5758615
Epoch: 2727 Sparsity: 0.986328125 Training loss: 2.9030452
Epoch: 2728 Sparsity: 0.9758680555555556 Training loss: 3.9238966
Epoch: 2729 Sparsity: 0.9830295138888889 Training loss: 2.4454396
Epoch: 2730 Sparsity: 0.9606770833333332 Training loss: 2.2852771
Epoch: 2731 Sparsity: 0.9832465277777779 Training loss: 2.2975008
Epoch: 2732 Sparsity: 0.9608940972222222 Training loss: 2.4974136
Epoch: 2733 Sparsity: 0.9818142361111111 Training loss: 2.9416215
Epoch: 2734 Sparsity: 0.9747395833333334 Training loss: 2.3811617
Epoch: 2735 Sparsity: 0.9789930555555555 Training loss: 2.820977
Epoch: 2736 Sparsity: 0.9599826388888889 Training loss: 2.4220796
Epoch: 2737 Sparsity: 0.9819010416666668 Training loss: 3.0970128
Epoch: 2738 Sparsity: 0.9770833333333334 Training loss: 2.5250394
Epoch: 2739 Sparsity: 0.9766059027777777 Training loss: 2.334145
Epoch: 2740 Sparsity: 0.986111111111111 Training loss: 2.4343886
Epoch: 2741 Sparsity: 0.9726996527777778 Training loss: 2.4597723
Epoch: 2742 Sparsity: 0.9853732638888889 Training loss: 2.5027044
Epoch: 2743 Sparsity: 0.9762152777777777 Training loss: 2.4331725
Epoch: 2744 Sparsity: 0.9829427083333334 Training loss: 2.6621127
Epoch: 2745 Sparsity: 0.9715277777777777 Training loss: 2.3726034
Epoch: 2746 Sparsity: 0.9829427083333334 Training loss: 2.9288976
Epoch: 2747 Sparsity: 0.9707465277777777 Training loss: 2.4374855
Epoch: 2748 Sparsity: 0.9857204861111111 Training loss: 2.472294
Epoch: 2749 Sparsity: 0.9780381944444443 Training loss: 2.655103
Epoch: 2750 Sparsity: 0.9858506944444445 Training loss: 2.6622458
Epoch: 2751 Sparsity: 0.9755208333333334 Training loss: 2.2989705
Epoch: 2752 Sparsity: 0.9838541666666666 Training loss: 2.6879532
Epoch: 2753 Sparsity: 0.9776909722222221 Training loss: 2.5589168
Epoch: 2754 Sparsity: 0.9879340277777777 Training loss: 2.4959543
Epoch: 2755 Sparsity: 0.9858072916666668 Training loss: 2.3636248
Epoch: 2756 Sparsity: 0.961154513888889 Training loss: 3.2221527
Epoch: 2757 Sparsity: 0.9850260416666666 Training loss: 2.937767
Epoch: 2758 Sparsity: 0.9579427083333334 Training loss: 2.5875335
Epoch: 2759 Sparsity: 0.9806857638888887 Training loss: 2.8089683
Epoch: 2760 Sparsity: 0.9647569444444445 Training loss: 2.4086032
Epoch: 2761 Sparsity: 0.9828559027777779 Training loss: 2.851853
Epoch: 2762 Sparsity: 0.9706163194444445 Training loss: 2.677517
Epoch: 2763 Sparsity: 0.9760850694444445 Training loss: 3.0810015
Epoch: 2764 Sparsity: 0.9674479166666666 Training loss: 2.3160055
Epoch: 2765 Sparsity: 0.9851562500000002 Training loss: 2.7932727
Epoch: 2766 Sparsity: 0.9692274305555555 Training loss: 2.5495665
Epoch: 2767 Sparsity: 0.986328125 Training loss: 3.0310411
Epoch: 2768 Sparsity: 0.975 Training loss: 2.926944
Epoch: 2769 Sparsity: 0.9833333333333332 Training loss: 2.7096243
Epoch: 2770 Sparsity: 0.9657986111111111 Training loss: 2.7412293
Epoch: 2771 Sparsity: 0.9858506944444445 Training loss: 3.0746083
Epoch: 2772 Sparsity: 0.9771701388888889 Training loss: 2.4578824
Epoch: 2773 Sparsity: 0.9835069444444444 Training loss: 3.4137847
Epoch: 2774 Sparsity: 0.9668836805555557 Training loss: 2.9491374
Epoch: 2775 Sparsity: 0.9815104166666666 Training loss: 2.4543345
Epoch: 2776 Sparsity: 0.9766927083333332 Training loss: 2.338371
Epoch: 2777 Sparsity: 0.9832465277777779 Training loss: 2.785995
Epoch: 2778 Sparsity: 0.9771267361111111 Training loss: 2.441081
Epoch: 2779 Sparsity: 0.986545138888889 Training loss: 2.6998515
Epoch: 2780 Sparsity: 0.9770399305555555 Training loss: 2.3412282
Epoch: 2781 Sparsity: 0.9907118055555555 Training loss: 2.8166375
Epoch: 2782 Sparsity: 0.9865451388888891 Training loss: 2.30917
Epoch: 2783 Sparsity: 0.9848090277777779 Training loss: 2.3971405
Epoch: 2784 Sparsity: 0.988498263888889 Training loss: 2.694455
Epoch: 2785 Sparsity: 0.9817274305555556 Training loss: 3.1735828
Epoch: 2786 Sparsity: 0.9818576388888889 Training loss: 2.700211
Epoch: 2787 Sparsity: 0.9913628472222221 Training loss: 2.5839546
Epoch: 2788 Sparsity: 0.984375 Training loss: 2.703542
Epoch: 2789 Sparsity: 0.9868923611111112 Training loss: 2.450314
Epoch: 2790 Sparsity: 0.9782552083333333 Training loss: 2.3695188
Epoch: 2791 Sparsity: 0.9870225694444443 Training loss: 2.753501
Epoch: 2792 Sparsity: 0.9803819444444445 Training loss: 2.4110298
Epoch: 2793 Sparsity: 0.956640625 Training loss: 2.3183084
Epoch: 2794 Sparsity: 0.9874565972222223 Training loss: 2.4250488
Epoch: 2795 Sparsity: 0.9831163194444443 Training loss: 2.3803039
Epoch: 2796 Sparsity: 0.980251736111111 Training loss: 2.3782523
Epoch: 2797 Sparsity: 0.9903211805555555 Training loss: 2.4334078
Epoch: 2798 Sparsity: 0.98046875 Training loss: 4.375778
Epoch: 2799 Sparsity: 0.98671875 Training loss: 2.57547
Epoch: 2800 Sparsity: 0.9781684027777778 Training loss: 2.690127
Epoch: 2801 Sparsity: 0.9812499999999998 Training loss: 2.367394
Epoch: 2802 Sparsity: 0.9842447916666666 Training loss: 2.3984716
Epoch: 2803 Sparsity: 0.9639322916666666 Training loss: 2.3728793
Epoch: 2804 Sparsity: 0.9844184027777778 Training loss: 2.4463603
Epoch: 2805 Sparsity: 0.9770833333333334 Training loss: 2.6550565
Epoch: 2806 Sparsity: 0.9840277777777778 Training loss: 2.6706471
Epoch: 2807 Sparsity: 0.9791666666666666 Training loss: 2.3875623
Epoch: 2808 Sparsity: 0.9780381944444445 Training loss: 2.4532433
Epoch: 2809 Sparsity: 0.9857204861111111 Training loss: 2.4819422
Epoch: 2810 Sparsity: 0.976388888888889 Training loss: 2.5991066
Epoch: 2811 Sparsity: 0.9864583333333334 Training loss: 2.896594
Epoch: 2812 Sparsity: 0.9780815972222223 Training loss: 2.6731029
Epoch: 2813 Sparsity: 0.9857204861111108 Training loss: 3.0064137
Epoch: 2814 Sparsity: 0.9836371527777779 Training loss: 2.3087907
Epoch: 2815 Sparsity: 0.984548611111111 Training loss: 2.8121169
Epoch: 2816 Sparsity: 0.9792100694444444 Training loss: 2.3602662
Epoch: 2817 Sparsity: 0.9793402777777779 Training loss: 3.7655683
Epoch: 2818 Sparsity: 0.9899739583333333 Training loss: 2.9172697
Epoch: 2819 Sparsity: 0.9864149305555555 Training loss: 2.3667042
Epoch: 2820 Sparsity: 0.9862413194444445 Training loss: 3.4858897
Epoch: 2821 Sparsity: 0.9713975694444444 Training loss: 2.593739
Epoch: 2822 Sparsity: 0.9876736111111111 Training loss: 2.8483825
Epoch: 2823 Sparsity: 0.981640625 Training loss: 2.528588
Epoch: 2824 Sparsity: 0.990234375 Training loss: 2.615941
Epoch: 2825 Sparsity: 0.9809027777777777 Training loss: 2.7049088
Epoch: 2826 Sparsity: 0.9866753472222222 Training loss: 2.6973345
Epoch: 2827 Sparsity: 0.9768663194444445 Training loss: 3.877595
Epoch: 2828 Sparsity: 0.9878906250000001 Training loss: 2.526807
Epoch: 2829 Sparsity: 0.9798177083333334 Training loss: 2.4536421
Epoch: 2830 Sparsity: 0.9874131944444443 Training loss: 2.5051193
Epoch: 2831 Sparsity: 0.9794704861111111 Training loss: 2.4775896
Epoch: 2832 Sparsity: 0.9838107638888889 Training loss: 2.6431825
Epoch: 2833 Sparsity: 0.9720052083333333 Training loss: 2.4853184
Epoch: 2834 Sparsity: 0.986154513888889 Training loss: 2.7432687
Epoch: 2835 Sparsity: 0.9736979166666666 Training loss: 2.9465363
Epoch: 2836 Sparsity: 0.9874131944444444 Training loss: 2.7187629
Epoch: 2837 Sparsity: 0.9810329861111111 Training loss: 2.5485218
Epoch: 2838 Sparsity: 0.9835069444444444 Training loss: 2.5823605
Epoch: 2839 Sparsity: 0.9838107638888889 Training loss: 2.476177
Epoch: 2840 Sparsity: 0.9804253472222222 Training loss: 2.7300644
Epoch: 2841 Sparsity: 0.9801649305555555 Training loss: 2.4583054
Epoch: 2842 Sparsity: 0.9865885416666668 Training loss: 2.7923765
Epoch: 2843 Sparsity: 0.9751736111111112 Training loss: 2.5309522
Epoch: 2844 Sparsity: 0.9802951388888888 Training loss: 2.3933785
Epoch: 2845 Sparsity: 0.9897135416666666 Training loss: 2.7811494
Epoch: 2846 Sparsity: 0.9794704861111111 Training loss: 2.5672517
Epoch: 2847 Sparsity: 0.9853298611111111 Training loss: 2.333815
Epoch: 2848 Sparsity: 0.9751302083333334 Training loss: 2.7132049
Epoch: 2849 Sparsity: 0.9851128472222224 Training loss: 2.3754985
Epoch: 2850 Sparsity: 0.9742621527777778 Training loss: 2.5898297
Epoch: 2851 Sparsity: 0.9845052083333334 Training loss: 3.1235034
Epoch: 2852 Sparsity: 0.9804253472222222 Training loss: 2.4074035
Epoch: 2853 Sparsity: 0.9798177083333333 Training loss: 2.333229
Epoch: 2854 Sparsity: 0.9845052083333334 Training loss: 2.4336982
Epoch: 2855 Sparsity: 0.9702690972222221 Training loss: 2.5090501
Epoch: 2856 Sparsity: 0.9859375 Training loss: 2.8393524
Epoch: 2857 Sparsity: 0.9797309027777779 Training loss: 2.5182571
Epoch: 2858 Sparsity: 0.9860677083333333 Training loss: 2.5806317
Epoch: 2859 Sparsity: 0.9678385416666668 Training loss: 2.44312
Epoch: 2860 Sparsity: 0.9860677083333333 Training loss: 2.949678
Epoch: 2861 Sparsity: 0.9732638888888889 Training loss: 3.6359138
Epoch: 2862 Sparsity: 0.9840711805555558 Training loss: 2.5006242
Epoch: 2863 Sparsity: 0.9625868055555555 Training loss: 2.9579616
Epoch: 2864 Sparsity: 0.9812065972222221 Training loss: 2.6737325
Epoch: 2865 Sparsity: 0.9721354166666666 Training loss: 2.401889
Epoch: 2866 Sparsity: 0.9832465277777779 Training loss: 2.7860224
Epoch: 2867 Sparsity: 0.9773871527777777 Training loss: 2.486224
Epoch: 2868 Sparsity: 0.9585069444444445 Training loss: 2.3938613
Epoch: 2869 Sparsity: 0.9851128472222224 Training loss: 2.9809008
Epoch: 2870 Sparsity: 0.9725260416666666 Training loss: 2.7017775
Epoch: 2871 Sparsity: 0.988107638888889 Training loss: 2.4882545
Epoch: 2872 Sparsity: 0.9687065972222222 Training loss: 2.678001
Epoch: 2873 Sparsity: 0.984548611111111 Training loss: 2.5565264
Epoch: 2874 Sparsity: 0.974609375 Training loss: 3.1533265
Epoch: 2875 Sparsity: 0.974826388888889 Training loss: 2.6729715
Epoch: 2876 Sparsity: 0.9811631944444444 Training loss: 2.3153198
Epoch: 2877 Sparsity: 0.9892361111111112 Training loss: 2.8984358
Epoch: 2878 Sparsity: 0.9734809027777777 Training loss: 4.4916663
Epoch: 2879 Sparsity: 0.9889322916666666 Training loss: 2.7782507
Epoch: 2880 Sparsity: 0.9799913194444445 Training loss: 2.3450992
Epoch: 2881 Sparsity: 0.9882378472222223 Training loss: 2.7018392
Epoch: 2882 Sparsity: 0.9801649305555555 Training loss: 2.6214185
Epoch: 2883 Sparsity: 0.9890190972222221 Training loss: 2.9548204
Epoch: 2884 Sparsity: 0.9690104166666667 Training loss: 2.497097
Epoch: 2885 Sparsity: 0.9879340277777778 Training loss: 3.2666945
Epoch: 2886 Sparsity: 0.9799479166666668 Training loss: 2.6720955
Epoch: 2887 Sparsity: 0.9829861111111111 Training loss: 3.1641638
Epoch: 2888 Sparsity: 0.9732204861111111 Training loss: 2.4602056
Epoch: 2889 Sparsity: 0.9847656250000002 Training loss: 2.6342888
Epoch: 2890 Sparsity: 0.9758246527777776 Training loss: 2.4795694
Epoch: 2891 Sparsity: 0.9876736111111113 Training loss: 2.5486443
Epoch: 2892 Sparsity: 0.9834201388888891 Training loss: 2.6618638
Epoch: 2893 Sparsity: 0.9891493055555557 Training loss: 2.8571863
Epoch: 2894 Sparsity: 0.980078125 Training loss: 2.6642518
Epoch: 2895 Sparsity: 0.9882378472222222 Training loss: 2.7043822
Epoch: 2896 Sparsity: 0.9676649305555557 Training loss: 3.8457937
Epoch: 2897 Sparsity: 0.9877170138888888 Training loss: 2.4647827
Epoch: 2898 Sparsity: 0.9776041666666668 Training loss: 3.153565
Epoch: 2899 Sparsity: 0.9864583333333332 Training loss: 2.6085734
Epoch: 2900 Sparsity: 0.9779513888888888 Training loss: 3.0441206
Epoch: 2901 Sparsity: 0.9832465277777779 Training loss: 2.9932446
Epoch: 2902 Sparsity: 0.9794704861111111 Training loss: 2.4134734
Epoch: 2903 Sparsity: 0.9797743055555556 Training loss: 2.330288
Epoch: 2904 Sparsity: 0.9894097222222221 Training loss: 2.597897
Epoch: 2905 Sparsity: 0.9811197916666667 Training loss: 2.4658015
Epoch: 2906 Sparsity: 0.9864583333333334 Training loss: 2.3819764
Epoch: 2907 Sparsity: 0.9890625 Training loss: 2.458398
Epoch: 2908 Sparsity: 0.9875434027777776 Training loss: 2.954024
Epoch: 2909 Sparsity: 0.9906250000000002 Training loss: 3.627786
Epoch: 2910 Sparsity: 0.984201388888889 Training loss: 3.1237757
Epoch: 2911 Sparsity: 0.9854600694444444 Training loss: 2.4044962
Epoch: 2912 Sparsity: 0.976779513888889 Training loss: 2.3490098
Epoch: 2913 Sparsity: 0.9899305555555553 Training loss: 3.2822354
Epoch: 2914 Sparsity: 0.9815972222222223 Training loss: 2.382706
Epoch: 2915 Sparsity: 0.9911024305555557 Training loss: 2.5715983
Epoch: 2916 Sparsity: 0.98359375 Training loss: 2.6676033
Epoch: 2917 Sparsity: 0.9854166666666666 Training loss: 2.7695935
Epoch: 2918 Sparsity: 0.9735677083333334 Training loss: 2.6560447
Epoch: 2919 Sparsity: 0.984157986111111 Training loss: 2.4938502
Epoch: 2920 Sparsity: 0.9828993055555555 Training loss: 2.3104637
Epoch: 2921 Sparsity: 0.9893663194444443 Training loss: 2.6292124
Epoch: 2922 Sparsity: 0.9819444444444445 Training loss: 3.943268
Epoch: 2923 Sparsity: 0.9851128472222224 Training loss: 3.1327713
Epoch: 2924 Sparsity: 0.9922309027777778 Training loss: 2.4900563
Epoch: 2925 Sparsity: 0.9876302083333334 Training loss: 3.482415
Epoch: 2926 Sparsity: 0.9928819444444444 Training loss: 4.741318
Epoch: 2927 Sparsity: 0.9911458333333334 Training loss: 2.4025
Epoch: 2928 Sparsity: 0.9770399305555555 Training loss: 2.686254
Epoch: 2929 Sparsity: 0.9901909722222222 Training loss: 2.5187557
Epoch: 2930 Sparsity: 0.9762152777777778 Training loss: 2.7636297
Epoch: 2931 Sparsity: 0.9884114583333334 Training loss: 2.428875
Epoch: 2932 Sparsity: 0.9834201388888889 Training loss: 2.3689668
Epoch: 2933 Sparsity: 0.9894097222222223 Training loss: 2.428661
Epoch: 2934 Sparsity: 0.9870225694444444 Training loss: 2.5680687
Epoch: 2935 Sparsity: 0.990234375 Training loss: 2.386597
Epoch: 2936 Sparsity: 0.9789930555555555 Training loss: 2.37196
Epoch: 2937 Sparsity: 0.9906684027777777 Training loss: 2.573724
Epoch: 2938 Sparsity: 0.9880208333333333 Training loss: 2.5146616
Epoch: 2939 Sparsity: 0.9855034722222223 Training loss: 2.3386352
Epoch: 2940 Sparsity: 0.9921006944444445 Training loss: 2.645907
Epoch: 2941 Sparsity: 0.9917100694444445 Training loss: 2.5883448
Epoch: 2942 Sparsity: 0.9791666666666666 Training loss: 2.7240305
Epoch: 2943 Sparsity: 0.9899739583333333 Training loss: 2.3732302
Epoch: 2944 Sparsity: 0.9887152777777777 Training loss: 2.3561292
Epoch: 2945 Sparsity: 0.9858940972222223 Training loss: 2.584405
Epoch: 2946 Sparsity: 0.990625 Training loss: 3.0865355
Epoch: 2947 Sparsity: 0.9819878472222223 Training loss: 2.600925
Epoch: 2948 Sparsity: 0.9908854166666667 Training loss: 2.5955427
Epoch: 2949 Sparsity: 0.9866753472222222 Training loss: 2.5472472
Epoch: 2950 Sparsity: 0.982638888888889 Training loss: 2.336472
Epoch: 2951 Sparsity: 0.9884114583333334 Training loss: 3.1291656
Epoch: 2952 Sparsity: 0.9852430555555556 Training loss: 2.674292
Epoch: 2953 Sparsity: 0.9918836805555555 Training loss: 3.052459
Epoch: 2954 Sparsity: 0.9811197916666666 Training loss: 2.4230576
Epoch: 2955 Sparsity: 0.9890190972222221 Training loss: 2.4723933
Epoch: 2956 Sparsity: 0.9817708333333334 Training loss: 2.4385455
Epoch: 2957 Sparsity: 0.9872395833333334 Training loss: 2.484109
Epoch: 2958 Sparsity: 0.9760416666666666 Training loss: 2.5326493
Epoch: 2959 Sparsity: 0.9851128472222221 Training loss: 2.3408523
Epoch: 2960 Sparsity: 0.9853298611111111 Training loss: 2.386192
Epoch: 2961 Sparsity: 0.9869357638888887 Training loss: 2.6601377
Epoch: 2962 Sparsity: 0.9910590277777777 Training loss: 2.431827
Epoch: 2963 Sparsity: 0.9913194444444443 Training loss: 2.7806773
Epoch: 2964 Sparsity: 0.9749131944444445 Training loss: 2.5825331
Epoch: 2965 Sparsity: 0.9844184027777778 Training loss: 4.12144
Epoch: 2966 Sparsity: 0.9890190972222224 Training loss: 2.4212198
Epoch: 2967 Sparsity: 0.9836371527777779 Training loss: 2.317383
Epoch: 2968 Sparsity: 0.9799479166666668 Training loss: 2.6650417
Epoch: 2969 Sparsity: 0.9851562500000002 Training loss: 2.8178911
Epoch: 2970 Sparsity: 0.9819010416666666 Training loss: 2.4238813
Epoch: 2971 Sparsity: 0.9854600694444444 Training loss: 3.1307588
Epoch: 2972 Sparsity: 0.9831163194444444 Training loss: 2.380174
Epoch: 2973 Sparsity: 0.9886284722222222 Training loss: 2.5145679
Epoch: 2974 Sparsity: 0.9776041666666666 Training loss: 2.5840318
Epoch: 2975 Sparsity: 0.9876736111111111 Training loss: 2.655956
Epoch: 2976 Sparsity: 0.9829861111111111 Training loss: 2.5349054
Epoch: 2977 Sparsity: 0.988671875 Training loss: 2.5925527
Epoch: 2978 Sparsity: 0.96875 Training loss: 2.3895552
Epoch: 2979 Sparsity: 0.9843315972222222 Training loss: 2.404824
Epoch: 2980 Sparsity: 0.9889322916666666 Training loss: 2.4752097
Epoch: 2981 Sparsity: 0.9831163194444444 Training loss: 2.393651
Epoch: 2982 Sparsity: 0.98984375 Training loss: 3.6099584
Epoch: 2983 Sparsity: 0.98046875 Training loss: 2.7187035
Epoch: 2984 Sparsity: 0.9901475694444445 Training loss: 2.7279153
Epoch: 2985 Sparsity: 0.98125 Training loss: 2.6554384
Epoch: 2986 Sparsity: 0.9903211805555555 Training loss: 3.4538956
Epoch: 2987 Sparsity: 0.9786458333333332 Training loss: 3.1036978
Epoch: 2988 Sparsity: 0.9855034722222221 Training loss: 2.4029822
Epoch: 2989 Sparsity: 0.988454861111111 Training loss: 2.6859715
Epoch: 2990 Sparsity: 0.9814670138888888 Training loss: 3.18929
Epoch: 2991 Sparsity: 0.9832899305555556 Training loss: 2.4957309
Epoch: 2992 Sparsity: 0.9883246527777777 Training loss: 2.843089
Epoch: 2993 Sparsity: 0.9783854166666666 Training loss: 2.3777997
Epoch: 2994 Sparsity: 0.9824652777777777 Training loss: 2.3250005
Epoch: 2995 Sparsity: 0.9869357638888887 Training loss: 2.531005
Epoch: 2996 Sparsity: 0.9691840277777779 Training loss: 2.6003554
Epoch: 2997 Sparsity: 0.9731336805555555 Training loss: 2.3247392
Epoch: 2998 Sparsity: 0.9850694444444444 Training loss: 2.8762286
Epoch: 2999 Sparsity: 0.9907552083333332 Training loss: 2.8425775
Epoch: 3000 Sparsity: 0.9798611111111111 Training loss: 2.322125
Epoch: 3001 Sparsity: 0.98984375 Training loss: 2.8797998
Epoch: 3002 Sparsity: 0.9851128472222224 Training loss: 2.7152228
Epoch: 3003 Sparsity: 0.988107638888889 Training loss: 3.0050776
Epoch: 3004 Sparsity: 0.9733506944444443 Training loss: 3.0835903
Epoch: 3005 Sparsity: 0.9861111111111113 Training loss: 2.728536
Epoch: 3006 Sparsity: 0.9710069444444445 Training loss: 2.591454
Epoch: 3007 Sparsity: 0.9843750000000002 Training loss: 2.3745737
Epoch: 3008 Sparsity: 0.9727430555555557 Training loss: 2.6774867
Epoch: 3009 Sparsity: 0.9869791666666666 Training loss: 2.506686
Epoch: 3010 Sparsity: 0.9786024305555555 Training loss: 2.5175896
Epoch: 3011 Sparsity: 0.9850694444444444 Training loss: 2.3265731
Epoch: 3012 Sparsity: 0.9894965277777776 Training loss: 2.4388573
Epoch: 3013 Sparsity: 0.9825086805555555 Training loss: 2.5310519
Epoch: 3014 Sparsity: 0.9907552083333332 Training loss: 2.7916346
Epoch: 3015 Sparsity: 0.9813802083333332 Training loss: 2.4678028
Epoch: 3016 Sparsity: 0.9736111111111111 Training loss: 4.2909985
Epoch: 3017 Sparsity: 0.9792534722222221 Training loss: 2.390091
Epoch: 3018 Sparsity: 0.9866319444444445 Training loss: 2.711535
Epoch: 3019 Sparsity: 0.9789930555555557 Training loss: 2.5952806
Epoch: 3020 Sparsity: 0.989453125 Training loss: 2.4294546
Epoch: 3021 Sparsity: 0.9869791666666667 Training loss: 2.482928
Epoch: 3022 Sparsity: 0.9873697916666666 Training loss: 2.6221201
Epoch: 3023 Sparsity: 0.9905815972222223 Training loss: 2.6595352
Epoch: 3024 Sparsity: 0.976736111111111 Training loss: 2.5123096
Epoch: 3025 Sparsity: 0.988454861111111 Training loss: 2.799503
Epoch: 3026 Sparsity: 0.9796875 Training loss: 2.9676058
Epoch: 3027 Sparsity: 0.9852430555555556 Training loss: 2.470523
Epoch: 3028 Sparsity: 0.9814670138888888 Training loss: 2.525139
Epoch: 3029 Sparsity: 0.98671875 Training loss: 2.586236
Epoch: 3030 Sparsity: 0.9766927083333332 Training loss: 2.379144
Epoch: 3031 Sparsity: 0.9862847222222222 Training loss: 3.314175
Epoch: 3032 Sparsity: 0.980251736111111 Training loss: 2.5302696
Epoch: 3033 Sparsity: 0.9875868055555556 Training loss: 2.3661566
Epoch: 3034 Sparsity: 0.9810763888888889 Training loss: 2.4267097
Epoch: 3035 Sparsity: 0.9924913194444442 Training loss: 2.4578977
Epoch: 3036 Sparsity: 0.9840277777777777 Training loss: 2.3961082
Epoch: 3037 Sparsity: 0.9926649305555555 Training loss: 2.4135253
Epoch: 3038 Sparsity: 0.9810763888888889 Training loss: 3.5954242
Epoch: 3039 Sparsity: 0.9873697916666666 Training loss: 2.5283005
Epoch: 3040 Sparsity: 0.9791232638888887 Training loss: 2.3815618
Epoch: 3041 Sparsity: 0.98515625 Training loss: 2.8119943
Epoch: 3042 Sparsity: 0.9740885416666666 Training loss: 2.8069901
Epoch: 3043 Sparsity: 0.9860677083333333 Training loss: 3.0878477
Epoch: 3044 Sparsity: 0.9899305555555555 Training loss: 2.5821066
Epoch: 3045 Sparsity: 0.9818576388888889 Training loss: 2.7137654
Epoch: 3046 Sparsity: 0.9888454861111109 Training loss: 2.9351268
Epoch: 3047 Sparsity: 0.9898003472222223 Training loss: 2.5107367
Epoch: 3048 Sparsity: 0.9818142361111111 Training loss: 2.3706176
Epoch: 3049 Sparsity: 0.9894531249999998 Training loss: 2.491047
Epoch: 3050 Sparsity: 0.9794270833333334 Training loss: 2.4354858
Epoch: 3051 Sparsity: 0.9885416666666667 Training loss: 2.943357
Epoch: 3052 Sparsity: 0.9765625 Training loss: 2.780177
Epoch: 3053 Sparsity: 0.9875434027777776 Training loss: 2.4992597
Epoch: 3054 Sparsity: 0.9871527777777779 Training loss: 2.498601
Epoch: 3055 Sparsity: 0.9729166666666668 Training loss: 2.6795325
Epoch: 3056 Sparsity: 0.9854166666666668 Training loss: 3.012142
Epoch: 3057 Sparsity: 0.9750434027777779 Training loss: 2.6210577
Epoch: 3058 Sparsity: 0.9844618055555555 Training loss: 2.9292529
Epoch: 3059 Sparsity: 0.9668402777777778 Training loss: 2.3471825
Epoch: 3060 Sparsity: 0.9832899305555556 Training loss: 2.331782
Epoch: 3061 Sparsity: 0.9769965277777777 Training loss: 2.3324041
Epoch: 3062 Sparsity: 0.98984375 Training loss: 2.7216227
Epoch: 3063 Sparsity: 0.976953125 Training loss: 2.315624
Epoch: 3064 Sparsity: 0.9891059027777779 Training loss: 3.1923354
Epoch: 3065 Sparsity: 0.9758246527777779 Training loss: 2.6881683
Epoch: 3066 Sparsity: 0.9912760416666666 Training loss: 2.6283348
Epoch: 3067 Sparsity: 0.9707465277777777 Training loss: 2.4788792
Epoch: 3068 Sparsity: 0.9869791666666666 Training loss: 2.6983728
Epoch: 3069 Sparsity: 0.9805989583333334 Training loss: 3.9436712
Epoch: 3070 Sparsity: 0.9829861111111111 Training loss: 2.3753269
Epoch: 3071 Sparsity: 0.9825954861111112 Training loss: 2.3940015
Epoch: 3072 Sparsity: 0.9889322916666667 Training loss: 2.5099616
Epoch: 3073 Sparsity: 0.9740017361111111 Training loss: 2.4734492
Epoch: 3074 Sparsity: 0.9869357638888889 Training loss: 2.727025
Epoch: 3075 Sparsity: 0.9721354166666668 Training loss: 2.7329032
Epoch: 3076 Sparsity: 0.9880208333333332 Training loss: 2.550363
Epoch: 3077 Sparsity: 0.9811197916666666 Training loss: 2.3743563
Epoch: 3078 Sparsity: 0.9839409722222221 Training loss: 2.3784928
Epoch: 3079 Sparsity: 0.969921875 Training loss: 2.5197341
Epoch: 3080 Sparsity: 0.9840277777777777 Training loss: 2.6489007
Epoch: 3081 Sparsity: 0.9794704861111111 Training loss: 2.4209254
Epoch: 3082 Sparsity: 0.962673611111111 Training loss: 2.5190969
Epoch: 3083 Sparsity: 0.9817708333333334 Training loss: 2.775155
Epoch: 3084 Sparsity: 0.9747395833333334 Training loss: 2.2947717
Epoch: 3085 Sparsity: 0.9832465277777777 Training loss: 2.3809633
Epoch: 3086 Sparsity: 0.9853732638888889 Training loss: 2.373316
Epoch: 3087 Sparsity: 0.9833333333333334 Training loss: 2.3163886
Epoch: 3088 Sparsity: 0.9569444444444443 Training loss: 2.3418126
Epoch: 3089 Sparsity: 0.9853732638888889 Training loss: 3.7493982
Epoch: 3090 Sparsity: 0.956640625 Training loss: 2.6800478
Epoch: 3091 Sparsity: 0.9833333333333334 Training loss: 3.4871287
Epoch: 3092 Sparsity: 0.9809461805555555 Training loss: 2.507795
Epoch: 3093 Sparsity: 0.9856336805555556 Training loss: 2.7244418
Epoch: 3094 Sparsity: 0.9896267361111111 Training loss: 2.4913821
Epoch: 3095 Sparsity: 0.980078125 Training loss: 3.9265583
Epoch: 3096 Sparsity: 0.9849392361111111 Training loss: 2.8416066
Epoch: 3097 Sparsity: 0.9728732638888887 Training loss: 2.4271863
Epoch: 3098 Sparsity: 0.9859809027777777 Training loss: 2.4183047
Epoch: 3099 Sparsity: 0.9833767361111111 Training loss: 2.3542504
Epoch: 3100 Sparsity: 0.9639756944444444 Training loss: 3.4936051
Epoch: 3101 Sparsity: 0.9809895833333334 Training loss: 3.1097798
Epoch: 3102 Sparsity: 0.9689670138888887 Training loss: 2.3929799
Epoch: 3103 Sparsity: 0.9822916666666666 Training loss: 2.7580798
Epoch: 3104 Sparsity: 0.9789496527777779 Training loss: 2.341378
Epoch: 3105 Sparsity: 0.9644097222222223 Training loss: 2.8785148
Epoch: 3106 Sparsity: 0.9863715277777778 Training loss: 2.8159149
Epoch: 3107 Sparsity: 0.9801215277777777 Training loss: 4.198773
Epoch: 3108 Sparsity: 0.9825520833333332 Training loss: 2.4908023
Epoch: 3109 Sparsity: 0.9515625 Training loss: 2.8805203
Epoch: 3110 Sparsity: 0.9797743055555556 Training loss: 2.4004264
Epoch: 3111 Sparsity: 0.9888020833333334 Training loss: 3.132676
Epoch: 3112 Sparsity: 0.9756944444444444 Training loss: 3.575943
Epoch: 3113 Sparsity: 0.988107638888889 Training loss: 3.3620887
Epoch: 3114 Sparsity: 0.969140625 Training loss: 5.040373
Epoch: 3115 Sparsity: 0.9578124999999998 Training loss: 4.3541083
Epoch: 3116 Sparsity: 0.9766927083333332 Training loss: 3.0466607
Epoch: 3117 Sparsity: 0.984548611111111 Training loss: 2.5272622
Epoch: 3118 Sparsity: 0.9796440972222223 Training loss: 2.303093
Epoch: 3119 Sparsity: 0.9878038194444445 Training loss: 2.7625482
Epoch: 3120 Sparsity: 0.9828993055555555 Training loss: 2.580815
Epoch: 3121 Sparsity: 0.9865885416666667 Training loss: 2.5041623
Epoch: 3122 Sparsity: 0.9778645833333334 Training loss: 2.3966646
Epoch: 3123 Sparsity: 0.9878038194444445 Training loss: 2.5043027
Epoch: 3124 Sparsity: 0.985546875 Training loss: 2.3007941
Epoch: 3125 Sparsity: 0.9865885416666667 Training loss: 2.4137604
Epoch: 3126 Sparsity: 0.9865017361111112 Training loss: 2.7892992
Epoch: 3127 Sparsity: 0.978732638888889 Training loss: 2.7200022
Epoch: 3128 Sparsity: 0.9845486111111112 Training loss: 3.3844192
Epoch: 3129 Sparsity: 0.9701388888888889 Training loss: 2.749804
Epoch: 3130 Sparsity: 0.988107638888889 Training loss: 3.0427468
Epoch: 3131 Sparsity: 0.9827256944444445 Training loss: 2.4408462
Epoch: 3132 Sparsity: 0.9789496527777779 Training loss: 2.3246942
Epoch: 3133 Sparsity: 0.9862413194444445 Training loss: 2.8878062
Epoch: 3134 Sparsity: 0.9719618055555556 Training loss: 2.7206311
Epoch: 3135 Sparsity: 0.9774305555555556 Training loss: 2.3461354
Epoch: 3136 Sparsity: 0.9878472222222221 Training loss: 2.4399912
Epoch: 3137 Sparsity: 0.9797309027777776 Training loss: 3.3087046
Epoch: 3138 Sparsity: 0.9862847222222222 Training loss: 3.4242632
Epoch: 3139 Sparsity: 0.9886284722222222 Training loss: 2.4447067
Epoch: 3140 Sparsity: 0.9788194444444445 Training loss: 2.9544091
Epoch: 3141 Sparsity: 0.9805121527777778 Training loss: 2.340268
Epoch: 3142 Sparsity: 0.9758246527777776 Training loss: 2.317089
Epoch: 3143 Sparsity: 0.9831597222222224 Training loss: 2.4335823
Epoch: 3144 Sparsity: 0.976388888888889 Training loss: 2.3351462
Epoch: 3145 Sparsity: 0.9870225694444444 Training loss: 2.5881524
Epoch: 3146 Sparsity: 0.9772569444444444 Training loss: 2.2864108
Epoch: 3147 Sparsity: 0.9908854166666667 Training loss: 2.7170851
Epoch: 3148 Sparsity: 0.9853732638888889 Training loss: 2.3685746
Epoch: 3149 Sparsity: 0.9921875 Training loss: 2.5594554
Epoch: 3150 Sparsity: 0.9863715277777777 Training loss: 2.4065382
Epoch: 3151 Sparsity: 0.988454861111111 Training loss: 2.9929855
Epoch: 3152 Sparsity: 0.9912326388888889 Training loss: 2.4049025
Epoch: 3153 Sparsity: 0.9832465277777779 Training loss: 2.435397
Epoch: 3154 Sparsity: 0.979296875 Training loss: 2.330813
Epoch: 3155 Sparsity: 0.9907986111111111 Training loss: 2.7066305
Epoch: 3156 Sparsity: 0.9797309027777779 Training loss: 2.3434217
Epoch: 3157 Sparsity: 0.9907552083333332 Training loss: 2.6584244
Epoch: 3158 Sparsity: 0.982204861111111 Training loss: 2.447784
Epoch: 3159 Sparsity: 0.9889756944444444 Training loss: 2.3948147
Epoch: 3160 Sparsity: 0.9809027777777777 Training loss: 2.4469087
Epoch: 3161 Sparsity: 0.9796006944444444 Training loss: 2.3423166
Epoch: 3162 Sparsity: 0.9846354166666667 Training loss: 2.3418095
Epoch: 3163 Sparsity: 0.9858940972222223 Training loss: 2.5131335
Epoch: 3164 Sparsity: 0.9891059027777777 Training loss: 2.7557707
Epoch: 3165 Sparsity: 0.9854166666666666 Training loss: 2.3116071
Epoch: 3166 Sparsity: 0.9889322916666667 Training loss: 2.7568839
Epoch: 3167 Sparsity: 0.9869357638888889 Training loss: 3.200035
Epoch: 3168 Sparsity: 0.9879340277777778 Training loss: 2.4271932
Epoch: 3169 Sparsity: 0.9832899305555556 Training loss: 2.3644497
Epoch: 3170 Sparsity: 0.994140625 Training loss: 2.694034
Epoch: 3171 Sparsity: 0.9892361111111111 Training loss: 5.756957
Epoch: 3172 Sparsity: 0.9892361111111111 Training loss: 2.7873359
Epoch: 3173 Sparsity: 0.9867621527777779 Training loss: 2.4674451
Epoch: 3174 Sparsity: 0.9883680555555555 Training loss: 2.6075275
Epoch: 3175 Sparsity: 0.9683159722222221 Training loss: 4.1623774
Epoch: 3176 Sparsity: 0.9825520833333332 Training loss: 2.2858996
Epoch: 3177 Sparsity: 0.9908854166666666 Training loss: 2.6697311
Epoch: 3178 Sparsity: 0.9857204861111113 Training loss: 2.3090665
Epoch: 3179 Sparsity: 0.9874131944444444 Training loss: 2.527752
Epoch: 3180 Sparsity: 0.9912326388888889 Training loss: 2.3765368
Epoch: 3181 Sparsity: 0.9872395833333332 Training loss: 2.3314717
Epoch: 3182 Sparsity: 0.9937065972222223 Training loss: 3.4557366
Epoch: 3183 Sparsity: 0.9860677083333333 Training loss: 2.4879596
Epoch: 3184 Sparsity: 0.9915798611111113 Training loss: 2.7323759
Epoch: 3185 Sparsity: 0.9948784722222224 Training loss: 2.3776443
Epoch: 3186 Sparsity: 0.989453125 Training loss: 4.1627407
Epoch: 3187 Sparsity: 0.9942274305555555 Training loss: 3.2712238
Epoch: 3188 Sparsity: 0.9865885416666667 Training loss: 2.5547664
Epoch: 3189 Sparsity: 0.9928385416666667 Training loss: 2.5894897
Epoch: 3190 Sparsity: 0.9880208333333333 Training loss: 2.4231603
Epoch: 3191 Sparsity: 0.9876302083333333 Training loss: 2.3166432
Epoch: 3192 Sparsity: 0.9922743055555555 Training loss: 2.4201648
Epoch: 3193 Sparsity: 0.9820746527777777 Training loss: 2.3892848
Epoch: 3194 Sparsity: 0.9904079861111112 Training loss: 2.7013597
Epoch: 3195 Sparsity: 0.9778211805555556 Training loss: 3.1041458
Epoch: 3196 Sparsity: 0.991015625 Training loss: 2.593423
Epoch: 3197 Sparsity: 0.9765625 Training loss: 2.8174894
Epoch: 3198 Sparsity: 0.9878038194444445 Training loss: 2.3823776
Epoch: 3199 Sparsity: 0.98359375 Training loss: 2.4363048
Epoch: 3200 Sparsity: 0.9901041666666666 Training loss: 2.4711268
Epoch: 3201 Sparsity: 0.9848524305555555 Training loss: 2.3016102
Epoch: 3202 Sparsity: 0.9858940972222223 Training loss: 2.4345899
Epoch: 3203 Sparsity: 0.9827690972222223 Training loss: 2.6580472
Epoch: 3204 Sparsity: 0.9903645833333332 Training loss: 2.5665147
Epoch: 3205 Sparsity: 0.9739149305555557 Training loss: 2.4401233
Epoch: 3206 Sparsity: 0.9908854166666667 Training loss: 2.7274363
Epoch: 3207 Sparsity: 0.9796440972222221 Training loss: 2.4107997
Epoch: 3208 Sparsity: 0.9918836805555558 Training loss: 2.7587385
Epoch: 3209 Sparsity: 0.9862413194444445 Training loss: 2.3051996
Epoch: 3210 Sparsity: 0.9891493055555556 Training loss: 3.4031994
Epoch: 3211 Sparsity: 0.9871527777777777 Training loss: 2.6188796
Epoch: 3212 Sparsity: 0.9797309027777779 Training loss: 2.3572311
Epoch: 3213 Sparsity: 0.9903211805555555 Training loss: 2.4924586
Epoch: 3214 Sparsity: 0.9859375 Training loss: 2.3270736
Epoch: 3215 Sparsity: 0.989453125 Training loss: 3.043081
Epoch: 3216 Sparsity: 0.9870225694444444 Training loss: 2.4080374
Epoch: 3217 Sparsity: 0.9871961805555556 Training loss: 3.027638
Epoch: 3218 Sparsity: 0.9808159722222222 Training loss: 2.6664567
Epoch: 3219 Sparsity: 0.988498263888889 Training loss: 3.1597798
Epoch: 3220 Sparsity: 0.9916232638888889 Training loss: 2.5054588
Epoch: 3221 Sparsity: 0.9816840277777779 Training loss: 2.564957
Epoch: 3222 Sparsity: 0.9888454861111112 Training loss: 2.4643738
Epoch: 3223 Sparsity: 0.9749565972222222 Training loss: 2.7361338
Epoch: 3224 Sparsity: 0.9821180555555558 Training loss: 2.396826
Epoch: 3225 Sparsity: 0.9815538194444443 Training loss: 2.60225
Epoch: 3226 Sparsity: 0.9717881944444444 Training loss: 2.4060018
Epoch: 3227 Sparsity: 0.9876302083333333 Training loss: 2.6973073
Epoch: 3228 Sparsity: 0.9692274305555555 Training loss: 2.6347337
Epoch: 3229 Sparsity: 0.9843315972222222 Training loss: 2.3629053
Epoch: 3230 Sparsity: 0.9710503472222222 Training loss: 2.3807492
Epoch: 3231 Sparsity: 0.9869357638888889 Training loss: 2.6632485
Epoch: 3232 Sparsity: 0.9793836805555556 Training loss: 2.3955343
Epoch: 3233 Sparsity: 0.978515625 Training loss: 3.4151855
Epoch: 3234 Sparsity: 0.9638020833333334 Training loss: 3.272196
Epoch: 3235 Sparsity: 0.9809027777777777 Training loss: 2.8555198
Epoch: 3236 Sparsity: 0.9887152777777777 Training loss: 2.6034656
Epoch: 3237 Sparsity: 0.9857204861111113 Training loss: 4.895414
Epoch: 3238 Sparsity: 0.989453125 Training loss: 2.870788
Epoch: 3239 Sparsity: 0.9808159722222223 Training loss: 2.3291383
Epoch: 3240 Sparsity: 0.983203125 Training loss: 2.3049734
Epoch: 3241 Sparsity: 0.9895833333333333 Training loss: 2.4320526
Epoch: 3242 Sparsity: 0.9733940972222224 Training loss: 4.366577
Epoch: 3243 Sparsity: 0.9878906249999998 Training loss: 2.580533
Epoch: 3244 Sparsity: 0.9797743055555556 Training loss: 2.3120568
Epoch: 3245 Sparsity: 0.9846354166666668 Training loss: 2.5396142
Epoch: 3246 Sparsity: 0.9777777777777779 Training loss: 2.4554026
Epoch: 3247 Sparsity: 0.9878472222222223 Training loss: 2.5395796
Epoch: 3248 Sparsity: 0.9832031250000002 Training loss: 2.396493
Epoch: 3249 Sparsity: 0.9874131944444443 Training loss: 2.7668808
Epoch: 3250 Sparsity: 0.9831163194444444 Training loss: 2.3600235
Epoch: 3251 Sparsity: 0.9838541666666666 Training loss: 2.804307
Epoch: 3252 Sparsity: 0.9899739583333333 Training loss: 2.5172274
Epoch: 3253 Sparsity: 0.9847222222222223 Training loss: 3.572662
Epoch: 3254 Sparsity: 0.9831597222222221 Training loss: 2.5169885
Epoch: 3255 Sparsity: 0.9907118055555557 Training loss: 2.6253963
Epoch: 3256 Sparsity: 0.983203125 Training loss: 5.185737
Epoch: 3257 Sparsity: 0.9805121527777777 Training loss: 2.4100235
Epoch: 3258 Sparsity: 0.978689236111111 Training loss: 2.3171394
Epoch: 3259 Sparsity: 0.9885416666666668 Training loss: 2.5377383
Epoch: 3260 Sparsity: 0.9793836805555556 Training loss: 2.8864202
Epoch: 3261 Sparsity: 0.9889756944444444 Training loss: 2.7811384
Epoch: 3262 Sparsity: 0.9828125 Training loss: 2.62741
Epoch: 3263 Sparsity: 0.9910590277777777 Training loss: 2.3571427
Epoch: 3264 Sparsity: 0.9813368055555557 Training loss: 5.0054703
Epoch: 3265 Sparsity: 0.9899305555555555 Training loss: 2.8772485
Epoch: 3266 Sparsity: 0.9776475694444444 Training loss: 2.6299732
Epoch: 3267 Sparsity: 0.9819444444444445 Training loss: 3.6638274
Epoch: 3268 Sparsity: 0.9854600694444444 Training loss: 2.7450569
Epoch: 3269 Sparsity: 0.9746961805555555 Training loss: 2.4428062
Epoch: 3270 Sparsity: 0.9852430555555556 Training loss: 2.5918298
Epoch: 3271 Sparsity: 0.9816840277777776 Training loss: 2.4216185
Epoch: 3272 Sparsity: 0.9930121527777779 Training loss: 2.850898
Epoch: 3273 Sparsity: 0.9789930555555555 Training loss: 4.6411495
Epoch: 3274 Sparsity: 0.9888454861111112 Training loss: 2.4664416
Epoch: 3275 Sparsity: 0.9729166666666667 Training loss: 3.5492692
Epoch: 3276 Sparsity: 0.9879774305555555 Training loss: 2.9151876
Epoch: 3277 Sparsity: 0.9802083333333333 Training loss: 3.7611778
Epoch: 3278 Sparsity: 0.9870659722222221 Training loss: 2.6955893
Epoch: 3279 Sparsity: 0.9881510416666666 Training loss: 2.4098647
Epoch: 3280 Sparsity: 0.9862413194444445 Training loss: 2.3532884
Epoch: 3281 Sparsity: 0.969921875 Training loss: 2.4257514
Epoch: 3282 Sparsity: 0.9876302083333334 Training loss: 2.5012727
Epoch: 3283 Sparsity: 0.9686631944444445 Training loss: 2.8283854
Epoch: 3284 Sparsity: 0.985546875 Training loss: 2.735877
Epoch: 3285 Sparsity: 0.9856770833333334 Training loss: 2.53603
Epoch: 3286 Sparsity: 0.9887152777777777 Training loss: 3.0452142
Epoch: 3287 Sparsity: 0.9781684027777777 Training loss: 2.797032
Epoch: 3288 Sparsity: 0.9889322916666666 Training loss: 2.6748135
Epoch: 3289 Sparsity: 0.9875 Training loss: 2.3133361
Epoch: 3290 Sparsity: 0.9861545138888888 Training loss: 2.965813
Epoch: 3291 Sparsity: 0.9821614583333333 Training loss: 2.6377485
Epoch: 3292 Sparsity: 0.98828125 Training loss: 2.4319773
Epoch: 3293 Sparsity: 0.9862847222222222 Training loss: 2.4676268
Epoch: 3294 Sparsity: 0.9754340277777777 Training loss: 2.4285238
Epoch: 3295 Sparsity: 0.9875868055555556 Training loss: 3.4216003
Epoch: 3296 Sparsity: 0.9687934027777777 Training loss: 2.7694554
Epoch: 3297 Sparsity: 0.9864149305555555 Training loss: 2.9475126
Epoch: 3298 Sparsity: 0.9829427083333332 Training loss: 2.5000827
Epoch: 3299 Sparsity: 0.9853298611111111 Training loss: 2.4413085
Epoch: 3300 Sparsity: 0.9796875 Training loss: 3.2282453
Epoch: 3301 Sparsity: 0.9866319444444442 Training loss: 2.6738188
Epoch: 3302 Sparsity: 0.972482638888889 Training loss: 2.2956572
Epoch: 3303 Sparsity: 0.9864149305555555 Training loss: 3.099231
Epoch: 3304 Sparsity: 0.9827256944444445 Training loss: 2.8847086
Epoch: 3305 Sparsity: 0.9641927083333333 Training loss: 2.463342
Epoch: 3306 Sparsity: 0.9869791666666666 Training loss: 3.1874075
Epoch: 3307 Sparsity: 0.9897569444444445 Training loss: 2.403073
Epoch: 3308 Sparsity: 0.9830729166666666 Training loss: 2.3214762
Epoch: 3309 Sparsity: 0.9859375 Training loss: 2.5678477
Epoch: 3310 Sparsity: 0.9792534722222221 Training loss: 2.5125964
Epoch: 3311 Sparsity: 0.9874131944444446 Training loss: 3.0340831
Epoch: 3312 Sparsity: 0.9868923611111111 Training loss: 2.4221091
Epoch: 3313 Sparsity: 0.9842881944444443 Training loss: 2.7530382
Epoch: 3314 Sparsity: 0.9892795138888889 Training loss: 2.572023
Epoch: 3315 Sparsity: 0.9894097222222221 Training loss: 2.5913527
Epoch: 3316 Sparsity: 0.9869791666666667 Training loss: 3.8369539
Epoch: 3317 Sparsity: 0.9892361111111111 Training loss: 2.645057
Epoch: 3318 Sparsity: 0.9773003472222224 Training loss: 2.3250995
Epoch: 3319 Sparsity: 0.9889322916666666 Training loss: 3.0017405
Epoch: 3320 Sparsity: 0.9772569444444444 Training loss: 2.671624
Epoch: 3321 Sparsity: 0.9834201388888888 Training loss: 2.3137407
Epoch: 3322 Sparsity: 0.9875434027777776 Training loss: 2.5991654
Epoch: 3323 Sparsity: 0.9848958333333332 Training loss: 2.5138483
Epoch: 3324 Sparsity: 0.9921006944444445 Training loss: 2.711162
Epoch: 3325 Sparsity: 0.9865017361111112 Training loss: 3.491939
Epoch: 3326 Sparsity: 0.9807291666666667 Training loss: 2.7457538
Epoch: 3327 Sparsity: 0.9717881944444443 Training loss: 2.314065
Epoch: 3328 Sparsity: 0.9859375 Training loss: 3.1477704
Epoch: 3329 Sparsity: 0.978689236111111 Training loss: 2.653317
Epoch: 3330 Sparsity: 0.9877170138888889 Training loss: 2.3610435
Epoch: 3331 Sparsity: 0.9855034722222223 Training loss: 2.3590631
Epoch: 3332 Sparsity: 0.9858940972222223 Training loss: 2.491856
Epoch: 3333 Sparsity: 0.9866319444444445 Training loss: 2.4654834
Epoch: 3334 Sparsity: 0.982248263888889 Training loss: 2.390272
Epoch: 3335 Sparsity: 0.9878472222222221 Training loss: 2.5254292
Epoch: 3336 Sparsity: 0.9729600694444445 Training loss: 2.8946526
Epoch: 3337 Sparsity: 0.9866753472222223 Training loss: 2.691634
Epoch: 3338 Sparsity: 0.9830295138888889 Training loss: 2.9552
Epoch: 3339 Sparsity: 0.9802083333333333 Training loss: 2.3430436
Epoch: 3340 Sparsity: 0.9828559027777777 Training loss: 2.306276
Epoch: 3341 Sparsity: 0.9868055555555555 Training loss: 2.6743765
Epoch: 3342 Sparsity: 0.984592013888889 Training loss: 2.316105
Epoch: 3343 Sparsity: 0.9835069444444444 Training loss: 2.399824
Epoch: 3344 Sparsity: 0.9828993055555557 Training loss: 2.313491
Epoch: 3345 Sparsity: 0.980295138888889 Training loss: 2.3313415
Epoch: 3346 Sparsity: 0.9863715277777777 Training loss: 2.3316357
Epoch: 3347 Sparsity: 0.9917100694444445 Training loss: 2.6660619
Epoch: 3348 Sparsity: 0.9817708333333334 Training loss: 5.5058107
Epoch: 3349 Sparsity: 0.9850694444444443 Training loss: 2.4038696
Epoch: 3350 Sparsity: 0.9648003472222222 Training loss: 4.9912405
Epoch: 3351 Sparsity: 0.9846354166666667 Training loss: 2.4538958
Epoch: 3352 Sparsity: 0.9785590277777777 Training loss: 2.6877587
Epoch: 3353 Sparsity: 0.9867621527777777 Training loss: 2.5745697
Epoch: 3354 Sparsity: 0.9832899305555556 Training loss: 2.6255412
Epoch: 3355 Sparsity: 0.9887586805555555 Training loss: 2.3524582
Epoch: 3356 Sparsity: 0.9797743055555553 Training loss: 5.602776
Epoch: 3357 Sparsity: 0.98046875 Training loss: 2.4935548
Epoch: 3358 Sparsity: 0.9819444444444443 Training loss: 2.4034708
Epoch: 3359 Sparsity: 0.9721354166666666 Training loss: 2.466233
Epoch: 3360 Sparsity: 0.9848524305555555 Training loss: 2.8315701
Epoch: 3361 Sparsity: 0.984375 Training loss: 2.3380282
Epoch: 3362 Sparsity: 0.9902777777777777 Training loss: 2.3655381
Epoch: 3363 Sparsity: 0.9773871527777779 Training loss: 2.7811115
Epoch: 3364 Sparsity: 0.9820746527777778 Training loss: 4.2144027
Epoch: 3365 Sparsity: 0.9879774305555555 Training loss: 2.367561
Epoch: 3366 Sparsity: 0.9609809027777777 Training loss: 3.5933106
Epoch: 3367 Sparsity: 0.9870225694444444 Training loss: 3.0829651
Epoch: 3368 Sparsity: 0.9723958333333333 Training loss: 2.5235372
Epoch: 3369 Sparsity: 0.9854166666666668 Training loss: 2.5840197
Epoch: 3370 Sparsity: 0.9848958333333334 Training loss: 2.3706555
Epoch: 3371 Sparsity: 0.9848090277777779 Training loss: 2.5448253
Epoch: 3372 Sparsity: 0.9883246527777778 Training loss: 3.03505
Epoch: 3373 Sparsity: 0.9769531250000002 Training loss: 2.4462206
Epoch: 3374 Sparsity: 0.9888020833333332 Training loss: 2.9982834
Epoch: 3375 Sparsity: 0.9812934027777777 Training loss: 2.5657656
Epoch: 3376 Sparsity: 0.982638888888889 Training loss: 3.073369
Epoch: 3377 Sparsity: 0.9923177083333332 Training loss: 2.7156944
Epoch: 3378 Sparsity: 0.9867621527777779 Training loss: 2.3659477
Epoch: 3379 Sparsity: 0.9903645833333334 Training loss: 2.7451148
Epoch: 3380 Sparsity: 0.9846354166666668 Training loss: 2.432489
Epoch: 3381 Sparsity: 0.9888020833333334 Training loss: 2.5007918
Epoch: 3382 Sparsity: 0.9799479166666668 Training loss: 2.4764333
Epoch: 3383 Sparsity: 0.978689236111111 Training loss: 2.3505652
Epoch: 3384 Sparsity: 0.9871961805555556 Training loss: 2.659826
Epoch: 3385 Sparsity: 0.9840277777777778 Training loss: 2.3537383
Epoch: 3386 Sparsity: 0.9907986111111111 Training loss: 2.365234
Epoch: 3387 Sparsity: 0.9742621527777778 Training loss: 3.068012
Epoch: 3388 Sparsity: 0.9879774305555555 Training loss: 2.8718076
Epoch: 3389 Sparsity: 0.9823784722222223 Training loss: 2.428393
Epoch: 3390 Sparsity: 0.9911892361111111 Training loss: 2.5705597
Epoch: 3391 Sparsity: 0.9757378472222223 Training loss: 2.6099417
Epoch: 3392 Sparsity: 0.9855902777777776 Training loss: 2.3818002
Epoch: 3393 Sparsity: 0.9811197916666667 Training loss: 2.546742
Epoch: 3394 Sparsity: 0.9878472222222223 Training loss: 2.7047794
Epoch: 3395 Sparsity: 0.9832899305555556 Training loss: 2.672298
Epoch: 3396 Sparsity: 0.9868055555555555 Training loss: 3.065802
Epoch: 3397 Sparsity: 0.9668836805555555 Training loss: 2.3660033
Epoch: 3398 Sparsity: 0.9878038194444445 Training loss: 2.4742773
Epoch: 3399 Sparsity: 0.9736545138888888 Training loss: 2.678394
Epoch: 3400 Sparsity: 0.9781250000000001 Training loss: 2.3820229
Epoch: 3401 Sparsity: 0.9906684027777777 Training loss: 2.5926087
Epoch: 3402 Sparsity: 0.9845052083333334 Training loss: 2.8674154
Epoch: 3403 Sparsity: 0.9851562499999998 Training loss: 2.4855833
Epoch: 3404 Sparsity: 0.990625 Training loss: 2.3565629
Epoch: 3405 Sparsity: 0.9800347222222221 Training loss: 3.7931721
Epoch: 3406 Sparsity: 0.9859375 Training loss: 2.5838132
Epoch: 3407 Sparsity: 0.9907986111111112 Training loss: 2.3518784
Epoch: 3408 Sparsity: 0.99140625 Training loss: 2.3751009
Epoch: 3409 Sparsity: 0.980078125 Training loss: 5.026992
Epoch: 3410 Sparsity: 0.9908420138888887 Training loss: 2.575368
Epoch: 3411 Sparsity: 0.9819010416666666 Training loss: 2.4469075
Epoch: 3412 Sparsity: 0.9879340277777778 Training loss: 2.333941
Epoch: 3413 Sparsity: 0.9838975694444445 Training loss: 2.316693
Epoch: 3414 Sparsity: 0.991970486111111 Training loss: 2.7221422
Epoch: 3415 Sparsity: 0.982638888888889 Training loss: 3.780031
Epoch: 3416 Sparsity: 0.990407986111111 Training loss: 2.7043657
Epoch: 3417 Sparsity: 0.9836805555555556 Training loss: 2.422078
Epoch: 3418 Sparsity: 0.9884548611111112 Training loss: 2.5287664
Epoch: 3419 Sparsity: 0.9803385416666666 Training loss: 3.1878636
Epoch: 3420 Sparsity: 0.9885416666666668 Training loss: 2.6524568
Epoch: 3421 Sparsity: 0.9884548611111112 Training loss: 2.366305
Epoch: 3422 Sparsity: 0.992361111111111 Training loss: 2.7354615
Epoch: 3423 Sparsity: 0.9828125 Training loss: 3.099257
Epoch: 3424 Sparsity: 0.9891059027777777 Training loss: 2.8660352
Epoch: 3425 Sparsity: 0.9901909722222222 Training loss: 2.608354
Epoch: 3426 Sparsity: 0.9788628472222222 Training loss: 2.4935164
Epoch: 3427 Sparsity: 0.9858940972222221 Training loss: 2.5241592
Epoch: 3428 Sparsity: 0.9906684027777779 Training loss: 2.4302347
Epoch: 3429 Sparsity: 0.9843315972222223 Training loss: 3.9823084
Epoch: 3430 Sparsity: 0.990060763888889 Training loss: 2.5423458
Epoch: 3431 Sparsity: 0.9876302083333334 Training loss: 2.4915113
Epoch: 3432 Sparsity: 0.9848524305555555 Training loss: 2.3026521
Epoch: 3433 Sparsity: 0.9844618055555555 Training loss: 2.507234
Epoch: 3434 Sparsity: 0.9905815972222223 Training loss: 2.9423714
Epoch: 3435 Sparsity: 0.9818576388888889 Training loss: 4.576441
Epoch: 3436 Sparsity: 0.9879340277777778 Training loss: 2.318349
Epoch: 3437 Sparsity: 0.9875000000000002 Training loss: 2.5259888
Epoch: 3438 Sparsity: 0.9877604166666668 Training loss: 2.5983107
Epoch: 3439 Sparsity: 0.9836371527777779 Training loss: 2.415268
Epoch: 3440 Sparsity: 0.9766059027777778 Training loss: 2.293087
Epoch: 3441 Sparsity: 0.9877170138888889 Training loss: 3.425677
Epoch: 3442 Sparsity: 0.9878038194444445 Training loss: 2.535475
Epoch: 3443 Sparsity: 0.9711805555555555 Training loss: 3.1712866
Epoch: 3444 Sparsity: 0.9881944444444445 Training loss: 2.814382
Epoch: 3445 Sparsity: 0.9821614583333333 Training loss: 2.5608184
Epoch: 3446 Sparsity: 0.9912760416666666 Training loss: 2.6539483
Epoch: 3447 Sparsity: 0.982595486111111 Training loss: 2.6107657
Epoch: 3448 Sparsity: 0.9881510416666666 Training loss: 2.8216536
Epoch: 3449 Sparsity: 0.9827256944444442 Training loss: 2.3218427
Epoch: 3450 Sparsity: 0.9780815972222221 Training loss: 2.3354208
Epoch: 3451 Sparsity: 0.9735677083333332 Training loss: 2.2996535
Epoch: 3452 Sparsity: 0.9869791666666666 Training loss: 3.321804
Epoch: 3453 Sparsity: 0.9741753472222221 Training loss: 2.353121
Epoch: 3454 Sparsity: 0.9849826388888889 Training loss: 2.490459
Epoch: 3455 Sparsity: 0.9822482638888888 Training loss: 2.3972125
Epoch: 3456 Sparsity: 0.9891493055555556 Training loss: 2.370352
Epoch: 3457 Sparsity: 0.9814670138888888 Training loss: 2.636018
Epoch: 3458 Sparsity: 0.9897135416666668 Training loss: 2.6977625
Epoch: 3459 Sparsity: 0.980295138888889 Training loss: 2.4156392
Epoch: 3460 Sparsity: 0.9779947916666666 Training loss: 2.2846606
Epoch: 3461 Sparsity: 0.98671875 Training loss: 2.3529294
Epoch: 3462 Sparsity: 0.9887152777777777 Training loss: 2.5356972
Epoch: 3463 Sparsity: 0.9807725694444445 Training loss: 2.5985408
Epoch: 3464 Sparsity: 0.9862413194444445 Training loss: 2.4053106
Epoch: 3465 Sparsity: 0.9809461805555555 Training loss: 2.6603441
Epoch: 3466 Sparsity: 0.9891927083333334 Training loss: 2.8913674
Epoch: 3467 Sparsity: 0.986111111111111 Training loss: 2.4894845
Epoch: 3468 Sparsity: 0.9904947916666667 Training loss: 3.1166801
Epoch: 3469 Sparsity: 0.9862413194444445 Training loss: 2.365793
Epoch: 3470 Sparsity: 0.9815972222222223 Training loss: 2.3078847
Epoch: 3471 Sparsity: 0.9701822916666666 Training loss: 2.3249831
Epoch: 3472 Sparsity: 0.9865885416666667 Training loss: 4.737574
Epoch: 3473 Sparsity: 0.9848524305555555 Training loss: 2.5638626
Epoch: 3474 Sparsity: 0.9853298611111112 Training loss: 2.316778
Epoch: 3475 Sparsity: 0.9834201388888889 Training loss: 2.2745008
Epoch: 3476 Sparsity: 0.9829427083333334 Training loss: 2.3728633
Epoch: 3477 Sparsity: 0.9889756944444443 Training loss: 2.3834646
Epoch: 3478 Sparsity: 0.986111111111111 Training loss: 2.4696188
Epoch: 3479 Sparsity: 0.9934461805555556 Training loss: 2.9542396
Epoch: 3480 Sparsity: 0.9854166666666668 Training loss: 5.019752
Epoch: 3481 Sparsity: 0.9842881944444445 Training loss: 2.4080117
Epoch: 3482 Sparsity: 0.9914930555555556 Training loss: 2.597384
Epoch: 3483 Sparsity: 0.9879340277777778 Training loss: 3.0797944
Epoch: 3484 Sparsity: 0.9776041666666668 Training loss: 2.5333648
Epoch: 3485 Sparsity: 0.9864149305555555 Training loss: 3.2860723
Epoch: 3486 Sparsity: 0.9822916666666665 Training loss: 2.83407
Epoch: 3487 Sparsity: 0.9864583333333334 Training loss: 3.2097142
Epoch: 3488 Sparsity: 0.9776909722222223 Training loss: 2.5701745
Epoch: 3489 Sparsity: 0.984201388888889 Training loss: 2.6531956
Epoch: 3490 Sparsity: 0.9703559027777777 Training loss: 2.6486342
Epoch: 3491 Sparsity: 0.9843315972222222 Training loss: 2.957398
Epoch: 3492 Sparsity: 0.9855034722222221 Training loss: 2.3421257
Epoch: 3493 Sparsity: 0.9715277777777779 Training loss: 2.679686
Epoch: 3494 Sparsity: 0.9860243055555553 Training loss: 5.071668
Epoch: 3495 Sparsity: 0.988498263888889 Training loss: 2.3848298
Epoch: 3496 Sparsity: 0.9927083333333334 Training loss: 2.539849
Epoch: 3497 Sparsity: 0.9848958333333334 Training loss: 2.7311347
Epoch: 3498 Sparsity: 0.9895399305555557 Training loss: 2.7940488
Epoch: 3499 Sparsity: 0.9851128472222224 Training loss: 2.31966
Epoch: 3500 Sparsity: 0.9868923611111111 Training loss: 3.0092926
Epoch: 3501 Sparsity: 0.9886284722222222 Training loss: 2.6627944
Epoch: 3502 Sparsity: 0.9867187500000002 Training loss: 2.538928
Epoch: 3503 Sparsity: 0.9834201388888889 Training loss: 2.4643238
Epoch: 3504 Sparsity: 0.9863715277777778 Training loss: 2.5598722
Epoch: 3505 Sparsity: 0.9884982638888887 Training loss: 2.8996522
Epoch: 3506 Sparsity: 0.9872395833333332 Training loss: 2.5166068
Epoch: 3507 Sparsity: 0.98984375 Training loss: 2.909653
Epoch: 3508 Sparsity: 0.9908420138888889 Training loss: 2.3553007
Epoch: 3509 Sparsity: 0.9888020833333332 Training loss: 2.7103572
Epoch: 3510 Sparsity: 0.971875 Training loss: 2.8729506
Epoch: 3511 Sparsity: 0.9852864583333332 Training loss: 2.543052
Epoch: 3512 Sparsity: 0.98828125 Training loss: 2.3772416
Epoch: 3513 Sparsity: 0.9771267361111111 Training loss: 2.3843296
Epoch: 3514 Sparsity: 0.9892795138888889 Training loss: 2.6841156
Epoch: 3515 Sparsity: 0.98203125 Training loss: 3.029394
Epoch: 3516 Sparsity: 0.989453125 Training loss: 3.1957817
Epoch: 3517 Sparsity: 0.9914930555555556 Training loss: 2.5833824
Epoch: 3518 Sparsity: 0.9805121527777778 Training loss: 3.1565263
Epoch: 3519 Sparsity: 0.9886718749999999 Training loss: 2.6619635
Epoch: 3520 Sparsity: 0.9895399305555556 Training loss: 2.5721743
Epoch: 3521 Sparsity: 0.9875434027777779 Training loss: 2.3230007
Epoch: 3522 Sparsity: 0.98984375 Training loss: 2.958348
Epoch: 3523 Sparsity: 0.9760416666666668 Training loss: 2.7443311
Epoch: 3524 Sparsity: 0.9888020833333334 Training loss: 3.2577097
Epoch: 3525 Sparsity: 0.9799479166666666 Training loss: 2.9954193
Epoch: 3526 Sparsity: 0.986154513888889 Training loss: 3.6278238
Epoch: 3527 Sparsity: 0.9909288194444444 Training loss: 2.342354
Epoch: 3528 Sparsity: 0.9910156249999998 Training loss: 2.725503
Epoch: 3529 Sparsity: 0.9923611111111112 Training loss: 2.940482
Epoch: 3530 Sparsity: 0.9808593749999999 Training loss: 4.305815
Epoch: 3531 Sparsity: 0.9830729166666667 Training loss: 4.3025656
Epoch: 3532 Sparsity: 0.9892361111111111 Training loss: 3.6908329
Epoch: 3533 Sparsity: 0.9814236111111112 Training loss: 4.0087385
Epoch: 3534 Sparsity: 0.983203125 Training loss: 2.4064558
Epoch: 3535 Sparsity: 0.992361111111111 Training loss: 2.6856372
Epoch: 3536 Sparsity: 0.9807291666666667 Training loss: 2.9354396
Epoch: 3537 Sparsity: 0.9877170138888889 Training loss: 3.516108
Epoch: 3538 Sparsity: 0.9890190972222221 Training loss: 2.3676345
Epoch: 3539 Sparsity: 0.970876736111111 Training loss: 2.3195033
Epoch: 3540 Sparsity: 0.9738715277777779 Training loss: 4.3173757
Epoch: 3541 Sparsity: 0.9871527777777779 Training loss: 2.5763047
Epoch: 3542 Sparsity: 0.9787760416666667 Training loss: 2.404561
Epoch: 3543 Sparsity: 0.9878472222222223 Training loss: 2.946099
Epoch: 3544 Sparsity: 0.9752170138888889 Training loss: 2.7182817
Epoch: 3545 Sparsity: 0.9844184027777777 Training loss: 2.5774126
Epoch: 3546 Sparsity: 0.9819010416666666 Training loss: 2.321682
Epoch: 3547 Sparsity: 0.9909288194444443 Training loss: 2.617777
Epoch: 3548 Sparsity: 0.9852430555555556 Training loss: 2.514991
Epoch: 3549 Sparsity: 0.9899305555555555 Training loss: 3.2244375
Epoch: 3550 Sparsity: 0.9838541666666668 Training loss: 2.448386
Epoch: 3551 Sparsity: 0.9893663194444444 Training loss: 2.554404
Epoch: 3552 Sparsity: 0.9825954861111112 Training loss: 2.647949
Epoch: 3553 Sparsity: 0.9877170138888889 Training loss: 2.770644
Epoch: 3554 Sparsity: 0.9926215277777777 Training loss: 3.1854007
Epoch: 3555 Sparsity: 0.9848524305555555 Training loss: 2.5959573
Epoch: 3556 Sparsity: 0.9902777777777778 Training loss: 2.7412245
Epoch: 3557 Sparsity: 0.9893229166666666 Training loss: 2.7753034
Epoch: 3558 Sparsity: 0.9887152777777777 Training loss: 2.3426538
Epoch: 3559 Sparsity: 0.9891927083333334 Training loss: 2.3886933
Epoch: 3560 Sparsity: 0.9907986111111111 Training loss: 2.572362
Epoch: 3561 Sparsity: 0.9934895833333334 Training loss: 2.7804618
Epoch: 3562 Sparsity: 0.9820746527777778 Training loss: 3.4183052
Epoch: 3563 Sparsity: 0.9895833333333334 Training loss: 3.317414
Epoch: 3564 Sparsity: 0.9809027777777779 Training loss: 2.8800993
Epoch: 3565 Sparsity: 0.9926215277777777 Training loss: 3.31462
Epoch: 3566 Sparsity: 0.9830729166666667 Training loss: 2.7425783
Epoch: 3567 Sparsity: 0.9908420138888889 Training loss: 3.2950864
Epoch: 3568 Sparsity: 0.984592013888889 Training loss: 2.3271365
Epoch: 3569 Sparsity: 0.986501736111111 Training loss: 2.309284
Epoch: 3570 Sparsity: 0.991970486111111 Training loss: 2.6234255
Epoch: 3571 Sparsity: 0.9915364583333334 Training loss: 2.3861327
Epoch: 3572 Sparsity: 0.9833767361111112 Training loss: 2.901594
Epoch: 3573 Sparsity: 0.9900607638888888 Training loss: 2.808201
Epoch: 3574 Sparsity: 0.9875868055555556 Training loss: 2.3360527
Epoch: 3575 Sparsity: 0.9914930555555556 Training loss: 2.376777
Epoch: 3576 Sparsity: 0.9942274305555554 Training loss: 2.7648659
Epoch: 3577 Sparsity: 0.984592013888889 Training loss: 2.7630453
Epoch: 3578 Sparsity: 0.9938368055555555 Training loss: 3.366468
Epoch: 3579 Sparsity: 0.9861545138888888 Training loss: 2.8340085
Epoch: 3580 Sparsity: 0.9921440972222223 Training loss: 2.7214587
Epoch: 3581 Sparsity: 0.9846354166666667 Training loss: 2.9748125
Epoch: 3582 Sparsity: 0.98984375 Training loss: 2.4223487
Epoch: 3583 Sparsity: 0.9927083333333332 Training loss: 2.6718614
Epoch: 3584 Sparsity: 0.986501736111111 Training loss: 2.4558067
Epoch: 3585 Sparsity: 0.9909288194444444 Training loss: 2.7378895
Epoch: 3586 Sparsity: 0.99140625 Training loss: 2.530165
Epoch: 3587 Sparsity: 0.9795138888888891 Training loss: 2.3709636
Epoch: 3588 Sparsity: 0.9908420138888887 Training loss: 4.385172
Epoch: 3589 Sparsity: 0.97890625 Training loss: 2.811434
Epoch: 3590 Sparsity: 0.9907118055555555 Training loss: 2.4909208
Epoch: 3591 Sparsity: 0.9822916666666668 Training loss: 2.4346359
Epoch: 3592 Sparsity: 0.9938368055555555 Training loss: 2.732872
Epoch: 3593 Sparsity: 0.9882812500000002 Training loss: 2.7136981
Epoch: 3594 Sparsity: 0.9921440972222223 Training loss: 2.4028716
Epoch: 3595 Sparsity: 0.9907118055555555 Training loss: 2.5292563
Epoch: 3596 Sparsity: 0.9796440972222223 Training loss: 2.3222206
Epoch: 3597 Sparsity: 0.9892795138888889 Training loss: 3.112371
Epoch: 3598 Sparsity: 0.9841145833333332 Training loss: 2.3892703
Epoch: 3599 Sparsity: 0.9931423611111111 Training loss: 2.5566754
Epoch: 3600 Sparsity: 0.9871961805555556 Training loss: 2.889081
Epoch: 3601 Sparsity: 0.99453125 Training loss: 2.6921208
Epoch: 3602 Sparsity: 0.9857638888888888 Training loss: 2.6732495
Epoch: 3603 Sparsity: 0.9930555555555556 Training loss: 2.402701
Epoch: 3604 Sparsity: 0.9864149305555557 Training loss: 2.6358407
Epoch: 3605 Sparsity: 0.9935763888888889 Training loss: 2.9996977
Epoch: 3606 Sparsity: 0.988498263888889 Training loss: 2.528097
Epoch: 3607 Sparsity: 0.9914930555555556 Training loss: 3.0624728
Epoch: 3608 Sparsity: 0.992578125 Training loss: 2.4803245
Epoch: 3609 Sparsity: 0.9911024305555556 Training loss: 2.3761766
Epoch: 3610 Sparsity: 0.9901475694444445 Training loss: 2.4779367
Epoch: 3611 Sparsity: 0.9914496527777779 Training loss: 2.3234699
Epoch: 3612 Sparsity: 0.9927517361111112 Training loss: 3.6318076
Epoch: 3613 Sparsity: 0.9895833333333334 Training loss: 2.538012
Epoch: 3614 Sparsity: 0.9961371527777777 Training loss: 2.3408964
Epoch: 3615 Sparsity: 0.990234375 Training loss: 2.8377228
Epoch: 3616 Sparsity: 0.9908420138888887 Training loss: 3.2309449
Epoch: 3617 Sparsity: 0.9948784722222224 Training loss: 2.6230977
Epoch: 3618 Sparsity: 0.9916666666666668 Training loss: 2.723768
Epoch: 3619 Sparsity: 0.9797309027777776 Training loss: 2.4437187
Epoch: 3620 Sparsity: 0.9924913194444445 Training loss: 2.8571014
Epoch: 3621 Sparsity: 0.9917534722222223 Training loss: 2.3889947
Epoch: 3622 Sparsity: 0.9908420138888889 Training loss: 2.3161178
Epoch: 3623 Sparsity: 0.9941406249999998 Training loss: 2.6199453
Epoch: 3624 Sparsity: 0.9806857638888887 Training loss: 2.394319
Epoch: 3625 Sparsity: 0.9923177083333332 Training loss: 2.9691274
Epoch: 3626 Sparsity: 0.990234375 Training loss: 3.089321
Epoch: 3627 Sparsity: 0.9934027777777779 Training loss: 3.156688
Epoch: 3628 Sparsity: 0.9899739583333333 Training loss: 3.371082
Epoch: 3629 Sparsity: 0.9916232638888888 Training loss: 3.7722142
Epoch: 3630 Sparsity: 0.9913194444444444 Training loss: 2.4456975
Epoch: 3631 Sparsity: 0.9760416666666666 Training loss: 2.5417154
Epoch: 3632 Sparsity: 0.9930555555555556 Training loss: 2.793455
Epoch: 3633 Sparsity: 0.980685763888889 Training loss: 2.6146424
Epoch: 3634 Sparsity: 0.9903645833333334 Training loss: 3.5450387
Epoch: 3635 Sparsity: 0.9808593749999999 Training loss: 3.9014323
Epoch: 3636 Sparsity: 0.9924479166666667 Training loss: 3.0320706
Epoch: 3637 Sparsity: 0.9878038194444445 Training loss: 2.5375228
Epoch: 3638 Sparsity: 0.9935329861111111 Training loss: 2.478652
Epoch: 3639 Sparsity: 0.9864149305555554 Training loss: 2.5454602
Epoch: 3640 Sparsity: 0.9922309027777777 Training loss: 3.570171
Epoch: 3641 Sparsity: 0.9902777777777777 Training loss: 2.331555
Epoch: 3642 Sparsity: 0.9790798611111111 Training loss: 2.7867079
Epoch: 3643 Sparsity: 0.9911024305555556 Training loss: 2.6920955
Epoch: 3644 Sparsity: 0.9795138888888889 Training loss: 2.408974
Epoch: 3645 Sparsity: 0.9899739583333333 Training loss: 2.6826146
Epoch: 3646 Sparsity: 0.9894097222222223 Training loss: 2.5046654
Epoch: 3647 Sparsity: 0.9937065972222223 Training loss: 2.4348047
Epoch: 3648 Sparsity: 0.9858072916666668 Training loss: 2.5525584
Epoch: 3649 Sparsity: 0.9905381944444445 Training loss: 2.3915677
Epoch: 3650 Sparsity: 0.9921006944444445 Training loss: 3.2470698
Epoch: 3651 Sparsity: 0.9827256944444445 Training loss: 2.3418136
Epoch: 3652 Sparsity: 0.9922743055555555 Training loss: 2.9054403
Epoch: 3653 Sparsity: 0.9884114583333334 Training loss: 2.3646545
Epoch: 3654 Sparsity: 0.980685763888889 Training loss: 2.62304
Epoch: 3655 Sparsity: 0.991015625 Training loss: 3.621364
Epoch: 3656 Sparsity: 0.988671875 Training loss: 2.7733757
Epoch: 3657 Sparsity: 0.992404513888889 Training loss: 2.644021
Epoch: 3658 Sparsity: 0.992013888888889 Training loss: 2.332425
Epoch: 3659 Sparsity: 0.9896267361111111 Training loss: 2.4195263
Epoch: 3660 Sparsity: 0.9913628472222223 Training loss: 3.3841372
Epoch: 3661 Sparsity: 0.990060763888889 Training loss: 2.4171765
Epoch: 3662 Sparsity: 0.9912326388888888 Training loss: 2.9222827
Epoch: 3663 Sparsity: 0.9878472222222221 Training loss: 2.9369316
Epoch: 3664 Sparsity: 0.9912326388888889 Training loss: 3.2730098
Epoch: 3665 Sparsity: 0.9893229166666668 Training loss: 2.4887276
Epoch: 3666 Sparsity: 0.990451388888889 Training loss: 3.2375407
Epoch: 3667 Sparsity: 0.9811631944444444 Training loss: 2.555669
Epoch: 3668 Sparsity: 0.9916232638888889 Training loss: 2.6934502
Epoch: 3669 Sparsity: 0.9811197916666667 Training loss: 3.053157
Epoch: 3670 Sparsity: 0.9908420138888889 Training loss: 3.3882282
Epoch: 3671 Sparsity: 0.9799045138888891 Training loss: 2.6667361
Epoch: 3672 Sparsity: 0.9910156249999998 Training loss: 2.5864983
Epoch: 3673 Sparsity: 0.9887586805555557 Training loss: 2.350403
Epoch: 3674 Sparsity: 0.9919270833333332 Training loss: 2.6571617
Epoch: 3675 Sparsity: 0.9873263888888889 Training loss: 2.388505
Epoch: 3676 Sparsity: 0.99375 Training loss: 2.6014457
Epoch: 3677 Sparsity: 0.9871961805555556 Training loss: 2.9158695
Epoch: 3678 Sparsity: 0.98828125 Training loss: 2.3886385
Epoch: 3679 Sparsity: 0.9917100694444443 Training loss: 2.472293
Epoch: 3680 Sparsity: 0.9891493055555554 Training loss: 2.5008702
Epoch: 3681 Sparsity: 0.9923611111111112 Training loss: 2.8702502
Epoch: 3682 Sparsity: 0.9819444444444443 Training loss: 2.6212587
Epoch: 3683 Sparsity: 0.9936197916666666 Training loss: 2.7180862
Epoch: 3684 Sparsity: 0.9883680555555555 Training loss: 2.3695674
Epoch: 3685 Sparsity: 0.9932725694444444 Training loss: 2.3911946
Epoch: 3686 Sparsity: 0.9876736111111111 Training loss: 2.5868158
Epoch: 3687 Sparsity: 0.9954427083333334 Training loss: 2.5480647
Epoch: 3688 Sparsity: 0.9896701388888889 Training loss: 2.718595
Epoch: 3689 Sparsity: 0.9903211805555555 Training loss: 2.3411791
Epoch: 3690 Sparsity: 0.9944010416666667 Training loss: 2.5065284
Epoch: 3691 Sparsity: 0.9878038194444443 Training loss: 2.5412455
Epoch: 3692 Sparsity: 0.9940104166666666 Training loss: 3.2907872
Epoch: 3693 Sparsity: 0.9888454861111112 Training loss: 2.4242043
Epoch: 3694 Sparsity: 0.9930121527777779 Training loss: 2.7053182
Epoch: 3695 Sparsity: 0.9944010416666668 Training loss: 2.430269
Epoch: 3696 Sparsity: 0.9934461805555556 Training loss: 2.3170345
Epoch: 3697 Sparsity: 0.9958333333333332 Training loss: 2.7590995
Epoch: 3698 Sparsity: 0.990060763888889 Training loss: 2.8735757
Epoch: 3699 Sparsity: 0.9950086805555556 Training loss: 2.5633059
Epoch: 3700 Sparsity: 0.9927517361111111 Training loss: 2.3229187
Epoch: 3701 Sparsity: 0.9935763888888889 Training loss: 2.9259572
Epoch: 3702 Sparsity: 0.9935329861111113 Training loss: 2.9202738
Epoch: 3703 Sparsity: 0.9833333333333334 Training loss: 2.7526624
Epoch: 3704 Sparsity: 0.992013888888889 Training loss: 2.6749926
Epoch: 3705 Sparsity: 0.9909722222222221 Training loss: 2.411933
Epoch: 3706 Sparsity: 0.9921875 Training loss: 2.980474
Epoch: 3707 Sparsity: 0.984548611111111 Training loss: 2.556147
Epoch: 3708 Sparsity: 0.9950520833333334 Training loss: 2.903398
Epoch: 3709 Sparsity: 0.9848524305555555 Training loss: 5.0167522
Epoch: 3710 Sparsity: 0.974435763888889 Training loss: 2.307678
Epoch: 3711 Sparsity: 0.992013888888889 Training loss: 5.6995325
Epoch: 3712 Sparsity: 0.9766059027777777 Training loss: 2.3024788
Epoch: 3713 Sparsity: 0.9917534722222223 Training loss: 4.2232537
Epoch: 3714 Sparsity: 0.9785590277777777 Training loss: 2.7366185
Epoch: 3715 Sparsity: 0.9929253472222224 Training loss: 3.1886353
Epoch: 3716 Sparsity: 0.9871961805555556 Training loss: 2.4968355
Epoch: 3717 Sparsity: 0.9929253472222221 Training loss: 2.640601
Epoch: 3718 Sparsity: 0.9891927083333334 Training loss: 5.4746227
Epoch: 3719 Sparsity: 0.9894097222222221 Training loss: 3.2589493
Epoch: 3720 Sparsity: 0.992013888888889 Training loss: 2.4769304
Epoch: 3721 Sparsity: 0.9907118055555555 Training loss: 2.7555614
Epoch: 3722 Sparsity: 0.9722656249999998 Training loss: 2.843353
Epoch: 3723 Sparsity: 0.9905381944444447 Training loss: 3.6584735
Epoch: 3724 Sparsity: 0.9789930555555555 Training loss: 2.4323354
Epoch: 3725 Sparsity: 0.9924045138888887 Training loss: 3.2118642
Epoch: 3726 Sparsity: 0.9907986111111111 Training loss: 2.3552575
Epoch: 3727 Sparsity: 0.9894965277777776 Training loss: 2.6348062
Epoch: 3728 Sparsity: 0.9901041666666666 Training loss: 2.3992589
Epoch: 3729 Sparsity: 0.9774739583333332 Training loss: 2.3421996
Epoch: 3730 Sparsity: 0.9891059027777779 Training loss: 2.7486327
Epoch: 3731 Sparsity: 0.9889322916666667 Training loss: 2.6071615
Epoch: 3732 Sparsity: 0.9927517361111112 Training loss: 2.5465422
Epoch: 3733 Sparsity: 0.9883246527777778 Training loss: 2.4553654
Epoch: 3734 Sparsity: 0.9791666666666666 Training loss: 2.7444713
Epoch: 3735 Sparsity: 0.990017361111111 Training loss: 2.621392
Epoch: 3736 Sparsity: 0.9912760416666666 Training loss: 2.6125185
Epoch: 3737 Sparsity: 0.9773003472222224 Training loss: 2.4858394
Epoch: 3738 Sparsity: 0.9910590277777777 Training loss: 2.6022365
Epoch: 3739 Sparsity: 0.9794704861111112 Training loss: 2.616687
Epoch: 3740 Sparsity: 0.9904947916666667 Training loss: 3.9544818
Epoch: 3741 Sparsity: 0.98125 Training loss: 2.4890714
Epoch: 3742 Sparsity: 0.9911024305555556 Training loss: 3.0738833
Epoch: 3743 Sparsity: 0.9862847222222223 Training loss: 2.3313777
Epoch: 3744 Sparsity: 0.9891927083333334 Training loss: 2.3664062
Epoch: 3745 Sparsity: 0.9876302083333333 Training loss: 2.775046
Epoch: 3746 Sparsity: 0.9865885416666667 Training loss: 2.6530817
Epoch: 3747 Sparsity: 0.9651475694444445 Training loss: 2.4833758
Epoch: 3748 Sparsity: 0.9873263888888888 Training loss: 3.4148881
Epoch: 3749 Sparsity: 0.9646267361111113 Training loss: 3.4558935
Epoch: 3750 Sparsity: 0.9861979166666666 Training loss: 3.2578104
Epoch: 3751 Sparsity: 0.9905815972222223 Training loss: 2.7718163
Epoch: 3752 Sparsity: 0.9797743055555556 Training loss: 2.9419887
Epoch: 3753 Sparsity: 0.9907552083333334 Training loss: 2.9411879
Epoch: 3754 Sparsity: 0.9848090277777777 Training loss: 2.4121249
Epoch: 3755 Sparsity: 0.9905381944444442 Training loss: 3.086442
Epoch: 3756 Sparsity: 0.9765190972222223 Training loss: 3.3338761
Epoch: 3757 Sparsity: 0.9898871527777778 Training loss: 3.4265072
Epoch: 3758 Sparsity: 0.982638888888889 Training loss: 2.4671214
Epoch: 3759 Sparsity: 0.9904947916666667 Training loss: 2.4233541
Epoch: 3760 Sparsity: 0.9918402777777778 Training loss: 2.4600613
Epoch: 3761 Sparsity: 0.9713107638888887 Training loss: 2.4487803
Epoch: 3762 Sparsity: 0.9890190972222221 Training loss: 3.5122502
Epoch: 3763 Sparsity: 0.9745225694444445 Training loss: 3.082852
Epoch: 3764 Sparsity: 0.9890625 Training loss: 3.339873
Epoch: 3765 Sparsity: 0.9810763888888889 Training loss: 2.9909015
Epoch: 3766 Sparsity: 0.9887586805555555 Training loss: 2.579185
Epoch: 3767 Sparsity: 0.9839409722222221 Training loss: 2.4314413
Epoch: 3768 Sparsity: 0.9877604166666666 Training loss: 2.4502747
Epoch: 3769 Sparsity: 0.9873263888888889 Training loss: 3.0121837
Epoch: 3770 Sparsity: 0.9749565972222222 Training loss: 2.6804078
Epoch: 3771 Sparsity: 0.9886718750000002 Training loss: 2.7211313
Epoch: 3772 Sparsity: 0.9790798611111111 Training loss: 2.3457677
Epoch: 3773 Sparsity: 0.9889322916666666 Training loss: 2.5553238
Epoch: 3774 Sparsity: 0.9845052083333334 Training loss: 2.5985935
Epoch: 3775 Sparsity: 0.9835069444444444 Training loss: 3.8311615
Epoch: 3776 Sparsity: 0.964453125 Training loss: 2.3668637
Epoch: 3777 Sparsity: 0.983984375 Training loss: 4.1221533
Epoch: 3778 Sparsity: 0.9716145833333334 Training loss: 2.6141243
Epoch: 3779 Sparsity: 0.9860677083333332 Training loss: 3.0280335
Epoch: 3780 Sparsity: 0.9792100694444444 Training loss: 2.4900668
Epoch: 3781 Sparsity: 0.9657118055555557 Training loss: 4.503336
Epoch: 3782 Sparsity: 0.982638888888889 Training loss: 3.4407823
Epoch: 3783 Sparsity: 0.9893229166666666 Training loss: 2.7042565
Epoch: 3784 Sparsity: 0.9883680555555555 Training loss: 2.4724302
Epoch: 3785 Sparsity: 0.9764322916666668 Training loss: 3.8657222
Epoch: 3786 Sparsity: 0.9892361111111111 Training loss: 2.505477
Epoch: 3787 Sparsity: 0.9771701388888889 Training loss: 2.3869808
Epoch: 3788 Sparsity: 0.9852864583333332 Training loss: 2.363171
Epoch: 3789 Sparsity: 0.9863281250000002 Training loss: 2.3961043
Epoch: 3790 Sparsity: 0.9730902777777779 Training loss: 2.7157826
Epoch: 3791 Sparsity: 0.9888020833333334 Training loss: 2.5880253
Epoch: 3792 Sparsity: 0.9794704861111111 Training loss: 2.5879512
Epoch: 3793 Sparsity: 0.9910590277777777 Training loss: 2.6884604
Epoch: 3794 Sparsity: 0.9870659722222224 Training loss: 2.5601661
Epoch: 3795 Sparsity: 0.987890625 Training loss: 2.715541
Epoch: 3796 Sparsity: 0.974609375 Training loss: 2.5451198
Epoch: 3797 Sparsity: 0.9893663194444444 Training loss: 2.6735315
Epoch: 3798 Sparsity: 0.9810763888888889 Training loss: 2.8256602
Epoch: 3799 Sparsity: 0.9833333333333334 Training loss: 2.309563
Epoch: 3800 Sparsity: 0.9891493055555556 Training loss: 3.06844
Epoch: 3801 Sparsity: 0.984592013888889 Training loss: 2.6635308
Epoch: 3802 Sparsity: 0.9861979166666668 Training loss: 3.4833324
Epoch: 3803 Sparsity: 0.9878038194444445 Training loss: 2.5448716
Epoch: 3804 Sparsity: 0.9837239583333333 Training loss: 2.814844
Epoch: 3805 Sparsity: 0.9837239583333333 Training loss: 2.9678495
Epoch: 3806 Sparsity: 0.9885416666666667 Training loss: 2.551116
Epoch: 3807 Sparsity: 0.9855034722222223 Training loss: 2.8211157
Epoch: 3808 Sparsity: 0.9862413194444445 Training loss: 2.4318087
Epoch: 3809 Sparsity: 0.9884548611111112 Training loss: 2.4046106
Epoch: 3810 Sparsity: 0.9817708333333333 Training loss: 2.4535284
Epoch: 3811 Sparsity: 0.9874565972222223 Training loss: 3.0438266
Epoch: 3812 Sparsity: 0.9838541666666666 Training loss: 2.8144841
Epoch: 3813 Sparsity: 0.9863715277777778 Training loss: 2.7136266
Epoch: 3814 Sparsity: 0.986328125 Training loss: 2.3743308
Epoch: 3815 Sparsity: 0.9806857638888887 Training loss: 2.578185
Epoch: 3816 Sparsity: 0.990407986111111 Training loss: 2.5160396
Epoch: 3817 Sparsity: 0.9762152777777778 Training loss: 2.6584282
Epoch: 3818 Sparsity: 0.9884114583333334 Training loss: 2.9349303
Epoch: 3819 Sparsity: 0.9788628472222223 Training loss: 2.4590876
Epoch: 3820 Sparsity: 0.9892795138888889 Training loss: 2.558297
Epoch: 3821 Sparsity: 0.9867187500000002 Training loss: 2.7301812
Epoch: 3822 Sparsity: 0.9930121527777777 Training loss: 2.7190125
Epoch: 3823 Sparsity: 0.984548611111111 Training loss: 3.0148933
Epoch: 3824 Sparsity: 0.99453125 Training loss: 2.4644115
Epoch: 3825 Sparsity: 0.990625 Training loss: 3.8096666
Epoch: 3826 Sparsity: 0.9878472222222221 Training loss: 4.2079344
Epoch: 3827 Sparsity: 0.9926215277777779 Training loss: 2.3608363
Epoch: 3828 Sparsity: 0.988107638888889 Training loss: 2.4322083
Epoch: 3829 Sparsity: 0.98046875 Training loss: 2.3840053
Epoch: 3830 Sparsity: 0.9900173611111113 Training loss: 2.4017537
Epoch: 3831 Sparsity: 0.96953125 Training loss: 2.7788608
Epoch: 3832 Sparsity: 0.988454861111111 Training loss: 3.6121948
Epoch: 3833 Sparsity: 0.9756076388888889 Training loss: 2.698924
Epoch: 3834 Sparsity: 0.9886718750000002 Training loss: 2.7278106
Epoch: 3835 Sparsity: 0.9753472222222221 Training loss: 2.5884733
Epoch: 3836 Sparsity: 0.9877170138888889 Training loss: 3.1126535
Epoch: 3837 Sparsity: 0.9803385416666666 Training loss: 2.9409244
Epoch: 3838 Sparsity: 0.9921875 Training loss: 2.6025589
Epoch: 3839 Sparsity: 0.982421875 Training loss: 4.1192408
Epoch: 3840 Sparsity: 0.9872829861111111 Training loss: 2.9197006
Epoch: 3841 Sparsity: 0.9731770833333332 Training loss: 2.4594388
Epoch: 3842 Sparsity: 0.988064236111111 Training loss: 2.6915243
Epoch: 3843 Sparsity: 0.9723090277777778 Training loss: 2.4567153
Epoch: 3844 Sparsity: 0.9883246527777777 Training loss: 2.7834113
Epoch: 3845 Sparsity: 0.990451388888889 Training loss: 2.514336
Epoch: 3846 Sparsity: 0.973828125 Training loss: 2.9502394
Epoch: 3847 Sparsity: 0.988671875 Training loss: 2.870674
Epoch: 3848 Sparsity: 0.9794704861111112 Training loss: 2.4896822
Epoch: 3849 Sparsity: 0.988498263888889 Training loss: 3.042453
Epoch: 3850 Sparsity: 0.9895833333333334 Training loss: 2.3341758
Epoch: 3851 Sparsity: 0.9895833333333334 Training loss: 2.975476
Epoch: 3852 Sparsity: 0.9853298611111111 Training loss: 2.456471
Epoch: 3853 Sparsity: 0.9871527777777777 Training loss: 2.6543849
Epoch: 3854 Sparsity: 0.9792534722222224 Training loss: 2.3229358
Epoch: 3855 Sparsity: 0.9913194444444444 Training loss: 2.7836175
Epoch: 3856 Sparsity: 0.9850694444444444 Training loss: 2.6916559
Epoch: 3857 Sparsity: 0.9837239583333333 Training loss: 2.6253393
Epoch: 3858 Sparsity: 0.984765625 Training loss: 2.6462426
Epoch: 3859 Sparsity: 0.9896267361111111 Training loss: 2.7021043
Epoch: 3860 Sparsity: 0.9782118055555555 Training loss: 3.0416937
Epoch: 3861 Sparsity: 0.9872829861111112 Training loss: 2.3732343
Epoch: 3862 Sparsity: 0.9793836805555556 Training loss: 2.3204002
Epoch: 3863 Sparsity: 0.988498263888889 Training loss: 2.5879061
Epoch: 3864 Sparsity: 0.978732638888889 Training loss: 2.823469
Epoch: 3865 Sparsity: 0.9812934027777779 Training loss: 2.779033
Epoch: 3866 Sparsity: 0.980295138888889 Training loss: 4.4757733
Epoch: 3867 Sparsity: 0.9870225694444444 Training loss: 2.5250475
Epoch: 3868 Sparsity: 0.9869791666666666 Training loss: 2.5265813
Epoch: 3869 Sparsity: 0.9847222222222223 Training loss: 2.317741
Epoch: 3870 Sparsity: 0.9865885416666667 Training loss: 2.5430453
Epoch: 3871 Sparsity: 0.9868055555555555 Training loss: 2.7315338
Epoch: 3872 Sparsity: 0.972439236111111 Training loss: 2.5930123
Epoch: 3873 Sparsity: 0.9875 Training loss: 2.7832673
Epoch: 3874 Sparsity: 0.9783854166666666 Training loss: 2.8335605
Epoch: 3875 Sparsity: 0.9891059027777779 Training loss: 2.9686048
Epoch: 3876 Sparsity: 0.9850260416666666 Training loss: 2.5296435
Epoch: 3877 Sparsity: 0.984548611111111 Training loss: 2.6902544
Epoch: 3878 Sparsity: 0.992013888888889 Training loss: 2.531642
Epoch: 3879 Sparsity: 0.9849392361111111 Training loss: 2.6010795
Epoch: 3880 Sparsity: 0.9868055555555555 Training loss: 2.4579554
Epoch: 3881 Sparsity: 0.9700954861111111 Training loss: 2.314357
Epoch: 3882 Sparsity: 0.9892795138888889 Training loss: 2.9029927
Epoch: 3883 Sparsity: 0.9813802083333332 Training loss: 2.5169137
Epoch: 3884 Sparsity: 0.9819878472222223 Training loss: 2.5050333
Epoch: 3885 Sparsity: 0.9825520833333334 Training loss: 2.4844816
Epoch: 3886 Sparsity: 0.9698784722222223 Training loss: 2.4371526
Epoch: 3887 Sparsity: 0.9850260416666667 Training loss: 3.0390599
Epoch: 3888 Sparsity: 0.9897135416666666 Training loss: 2.4006326
Epoch: 3889 Sparsity: 0.9804253472222223 Training loss: 5.036098
Epoch: 3890 Sparsity: 0.9897135416666668 Training loss: 2.483477
Epoch: 3891 Sparsity: 0.9877170138888889 Training loss: 2.506411
Epoch: 3892 Sparsity: 0.987890625 Training loss: 2.5258088
Epoch: 3893 Sparsity: 0.978515625 Training loss: 2.545708
Epoch: 3894 Sparsity: 0.9900607638888888 Training loss: 2.4085634
Epoch: 3895 Sparsity: 0.9828125 Training loss: 2.3319552
Epoch: 3896 Sparsity: 0.992361111111111 Training loss: 2.772374
Epoch: 3897 Sparsity: 0.9850260416666667 Training loss: 4.961012
Epoch: 3898 Sparsity: 0.9872395833333334 Training loss: 2.3969927
Epoch: 3899 Sparsity: 0.990625 Training loss: 2.5654385
Epoch: 3900 Sparsity: 0.9863715277777778 Training loss: 2.4178293
Epoch: 3901 Sparsity: 0.9694010416666666 Training loss: 2.507932
Epoch: 3902 Sparsity: 0.9844618055555555 Training loss: 3.712398
Epoch: 3903 Sparsity: 0.9714843749999998 Training loss: 2.531974
Epoch: 3904 Sparsity: 0.9872829861111111 Training loss: 2.6795094
Epoch: 3905 Sparsity: 0.9731770833333334 Training loss: 4.0360007
Epoch: 3906 Sparsity: 0.9877170138888888 Training loss: 2.3859413
Epoch: 3907 Sparsity: 0.98203125 Training loss: 2.269244
Epoch: 3908 Sparsity: 0.9882812500000002 Training loss: 2.9819672
Epoch: 3909 Sparsity: 0.9847222222222223 Training loss: 2.3101382
Epoch: 3910 Sparsity: 0.9869357638888889 Training loss: 2.6734035
Epoch: 3911 Sparsity: 0.9853732638888889 Training loss: 2.7260997
Epoch: 3912 Sparsity: 0.9759548611111111 Training loss: 2.3267605
Epoch: 3913 Sparsity: 0.9907118055555555 Training loss: 3.3781052
Epoch: 3914 Sparsity: 0.9850694444444443 Training loss: 2.3719113
Epoch: 3915 Sparsity: 0.9886284722222222 Training loss: 2.9833765
Epoch: 3916 Sparsity: 0.9902777777777777 Training loss: 2.3166006
Epoch: 3917 Sparsity: 0.9831597222222224 Training loss: 2.4529183
Epoch: 3918 Sparsity: 0.9823350694444445 Training loss: 2.288802
Epoch: 3919 Sparsity: 0.9911024305555556 Training loss: 2.5428154
Epoch: 3920 Sparsity: 0.9878472222222221 Training loss: 2.519751
Epoch: 3921 Sparsity: 0.9926649305555555 Training loss: 2.7715416
Epoch: 3922 Sparsity: 0.9861545138888888 Training loss: 2.3788087
Epoch: 3923 Sparsity: 0.9877604166666666 Training loss: 2.3152044
Epoch: 3924 Sparsity: 0.9926215277777777 Training loss: 3.508167
Epoch: 3925 Sparsity: 0.9898437500000001 Training loss: 2.335425
Epoch: 3926 Sparsity: 0.9944010416666668 Training loss: 2.584325
Epoch: 3927 Sparsity: 0.9907118055555554 Training loss: 2.8437135
Epoch: 3928 Sparsity: 0.9901475694444445 Training loss: 2.3108833
Epoch: 3929 Sparsity: 0.9859375 Training loss: 2.3025732
Epoch: 3930 Sparsity: 0.9896267361111111 Training loss: 3.0205083
Epoch: 3931 Sparsity: 0.9944010416666668 Training loss: 2.869632
Epoch: 3932 Sparsity: 0.9941406249999998 Training loss: 2.5427568
Epoch: 3933 Sparsity: 0.9829861111111111 Training loss: 3.2843788
Epoch: 3934 Sparsity: 0.9940104166666666 Training loss: 2.3944252
Epoch: 3935 Sparsity: 0.9899739583333333 Training loss: 2.91592
Epoch: 3936 Sparsity: 0.9936631944444445 Training loss: 2.8019078
Epoch: 3937 Sparsity: 0.9911458333333334 Training loss: 2.4551606
Epoch: 3938 Sparsity: 0.9953125 Training loss: 2.5428243
Epoch: 3939 Sparsity: 0.9839409722222221 Training loss: 3.2853463
Epoch: 3940 Sparsity: 0.9862847222222222 Training loss: 4.6563907
Epoch: 3941 Sparsity: 0.9909722222222221 Training loss: 2.4921534
Epoch: 3942 Sparsity: 0.980642361111111 Training loss: 2.2999837
Epoch: 3943 Sparsity: 0.9916666666666666 Training loss: 2.4755547
Epoch: 3944 Sparsity: 0.9786458333333334 Training loss: 3.0481153
Epoch: 3945 Sparsity: 0.9875868055555553 Training loss: 2.3954966
Epoch: 3946 Sparsity: 0.9852430555555556 Training loss: 2.6911235
Epoch: 3947 Sparsity: 0.9834635416666666 Training loss: 3.0268178
Epoch: 3948 Sparsity: 0.98828125 Training loss: 2.8314605
Epoch: 3949 Sparsity: 0.9837239583333333 Training loss: 2.5939114
Epoch: 3950 Sparsity: 0.9865885416666667 Training loss: 2.54651
Epoch: 3951 Sparsity: 0.9826822916666667 Training loss: 2.3420863
Epoch: 3952 Sparsity: 0.9888454861111111 Training loss: 2.303857
Epoch: 3953 Sparsity: 0.9905815972222222 Training loss: 3.1141336
Epoch: 3954 Sparsity: 0.9769965277777777 Training loss: 2.331782
Epoch: 3955 Sparsity: 0.9885416666666667 Training loss: 3.4438708
Epoch: 3956 Sparsity: 0.9804687499999998 Training loss: 2.6007931
Epoch: 3957 Sparsity: 0.9848958333333334 Training loss: 4.085763
Epoch: 3958 Sparsity: 0.9878038194444445 Training loss: 2.7742019
Epoch: 3959 Sparsity: 0.9895833333333334 Training loss: 2.4762406
Epoch: 3960 Sparsity: 0.9931857638888888 Training loss: 2.4780605
Epoch: 3961 Sparsity: 0.9888454861111112 Training loss: 2.310305
Epoch: 3962 Sparsity: 0.9932725694444446 Training loss: 2.6028244
Epoch: 3963 Sparsity: 0.9930989583333334 Training loss: 3.2508552
Epoch: 3964 Sparsity: 0.9813368055555556 Training loss: 3.2296097
Epoch: 3965 Sparsity: 0.9903211805555555 Training loss: 2.3174603
Epoch: 3966 Sparsity: 0.9865885416666668 Training loss: 2.2982483
Epoch: 3967 Sparsity: 0.9901475694444445 Training loss: 2.6309114
Epoch: 3968 Sparsity: 0.9846788194444442 Training loss: 2.8403018
Epoch: 3969 Sparsity: 0.9870225694444444 Training loss: 2.4214528
Epoch: 3970 Sparsity: 0.9912326388888888 Training loss: 2.3998828
Epoch: 3971 Sparsity: 0.984201388888889 Training loss: 2.8894286
Epoch: 3972 Sparsity: 0.9885416666666667 Training loss: 2.7009454
Epoch: 3973 Sparsity: 0.987890625 Training loss: 2.2906842
Epoch: 3974 Sparsity: 0.9883246527777778 Training loss: 2.5441854
Epoch: 3975 Sparsity: 0.9782118055555558 Training loss: 2.2922044
Epoch: 3976 Sparsity: 0.9904947916666668 Training loss: 2.871694
Epoch: 3977 Sparsity: 0.9837673611111111 Training loss: 2.3502138
Epoch: 3978 Sparsity: 0.9905381944444445 Training loss: 2.7921004
Epoch: 3979 Sparsity: 0.9883246527777778 Training loss: 2.3212905
Epoch: 3980 Sparsity: 0.9913194444444444 Training loss: 3.6081107
Epoch: 3981 Sparsity: 0.9833767361111112 Training loss: 2.3399122
Epoch: 3982 Sparsity: 0.9919270833333333 Training loss: 2.6046863
Epoch: 3983 Sparsity: 0.9773871527777779 Training loss: 3.768626
Epoch: 3984 Sparsity: 0.9919270833333333 Training loss: 2.6534538
Epoch: 3985 Sparsity: 0.9848524305555555 Training loss: 2.3094332
Epoch: 3986 Sparsity: 0.9918836805555555 Training loss: 2.5966709
Epoch: 3987 Sparsity: 0.9824652777777777 Training loss: 3.0892665
Epoch: 3988 Sparsity: 0.9914930555555556 Training loss: 2.3984618
Epoch: 3989 Sparsity: 0.9858940972222223 Training loss: 2.6062424
Epoch: 3990 Sparsity: 0.9749565972222223 Training loss: 2.6212363
Epoch: 3991 Sparsity: 0.9926215277777777 Training loss: 2.5973248
Epoch: 3992 Sparsity: 0.9759548611111111 Training loss: 3.6363685
Epoch: 3993 Sparsity: 0.9892795138888889 Training loss: 2.3197896
Epoch: 3994 Sparsity: 0.9735243055555556 Training loss: 2.3131785
Epoch: 3995 Sparsity: 0.9914930555555556 Training loss: 3.3557358
Epoch: 3996 Sparsity: 0.9790364583333334 Training loss: 2.912192
Epoch: 3997 Sparsity: 0.9908420138888889 Training loss: 2.8017175
Epoch: 3998 Sparsity: 0.9853298611111111 Training loss: 2.7864735
Epoch: 3999 Sparsity: 0.9895833333333334 Training loss: 2.4148827
Epoch: 4000 Sparsity: 0.9868923611111111 Training loss: 2.4084175
Epoch: 4001 Sparsity: 0.992404513888889 Training loss: 2.8078425
Epoch: 4002 Sparsity: 0.9888454861111111 Training loss: 2.4307544
Epoch: 4003 Sparsity: 0.9924479166666668 Training loss: 2.6096523
Epoch: 4004 Sparsity: 0.9832899305555556 Training loss: 2.8426814
Epoch: 4005 Sparsity: 0.9863281249999998 Training loss: 2.7894242
Epoch: 4006 Sparsity: 0.990017361111111 Training loss: 2.661565
Epoch: 4007 Sparsity: 0.9855902777777776 Training loss: 2.4340522
Epoch: 4008 Sparsity: 0.9882378472222222 Training loss: 2.8496585
Epoch: 4009 Sparsity: 0.978732638888889 Training loss: 2.3747933
Epoch: 4010 Sparsity: 0.9898871527777777 Training loss: 4.0219088
Epoch: 4011 Sparsity: 0.9884114583333334 Training loss: 2.747262
Epoch: 4012 Sparsity: 0.9830295138888889 Training loss: 2.4263997
Epoch: 4013 Sparsity: 0.9873263888888889 Training loss: 2.486793
Epoch: 4014 Sparsity: 0.9812065972222224 Training loss: 2.3038063
Epoch: 4015 Sparsity: 0.9831597222222221 Training loss: 2.4549577
Epoch: 4016 Sparsity: 0.987109375 Training loss: 2.3002083
Epoch: 4017 Sparsity: 0.9923177083333334 Training loss: 2.7360494
Epoch: 4018 Sparsity: 0.9868923611111112 Training loss: 2.341107
Epoch: 4019 Sparsity: 0.981640625 Training loss: 2.3598053
Epoch: 4020 Sparsity: 0.9896267361111111 Training loss: 2.7715492
Epoch: 4021 Sparsity: 0.9837673611111111 Training loss: 2.3937526
Epoch: 4022 Sparsity: 0.9904947916666667 Training loss: 2.5134027
Epoch: 4023 Sparsity: 0.9865885416666667 Training loss: 2.3925796
Epoch: 4024 Sparsity: 0.989453125 Training loss: 3.121726
Epoch: 4025 Sparsity: 0.9882378472222222 Training loss: 2.3886342
Epoch: 4026 Sparsity: 0.9913194444444444 Training loss: 2.9567704
Epoch: 4027 Sparsity: 0.9938368055555555 Training loss: 3.0233855
Epoch: 4028 Sparsity: 0.97890625 Training loss: 2.8455439
Epoch: 4029 Sparsity: 0.990060763888889 Training loss: 2.4996185
Epoch: 4030 Sparsity: 0.9910590277777777 Training loss: 2.5863965
Epoch: 4031 Sparsity: 0.9885850694444442 Training loss: 2.6003115
Epoch: 4032 Sparsity: 0.9666666666666666 Training loss: 2.5150044
Epoch: 4033 Sparsity: 0.9872829861111111 Training loss: 3.461825
Epoch: 4034 Sparsity: 0.9726996527777777 Training loss: 2.875225
Epoch: 4035 Sparsity: 0.9869791666666666 Training loss: 2.9390526
Epoch: 4036 Sparsity: 0.9895399305555556 Training loss: 2.82156
Epoch: 4037 Sparsity: 0.9672309027777777 Training loss: 3.229934
Epoch: 4038 Sparsity: 0.9875 Training loss: 3.5467293
Epoch: 4039 Sparsity: 0.966970486111111 Training loss: 2.6195047
Epoch: 4040 Sparsity: 0.9875434027777776 Training loss: 3.3052547
Epoch: 4041 Sparsity: 0.9887586805555555 Training loss: 2.3422549
Epoch: 4042 Sparsity: 0.9714409722222221 Training loss: 2.7028666
Epoch: 4043 Sparsity: 0.9898003472222223 Training loss: 2.5144696
Epoch: 4044 Sparsity: 0.9764322916666666 Training loss: 2.4955704
Epoch: 4045 Sparsity: 0.9905381944444445 Training loss: 3.5634353
Epoch: 4046 Sparsity: 0.9794270833333332 Training loss: 2.9765341
Epoch: 4047 Sparsity: 0.9783854166666666 Training loss: 2.3081067
Epoch: 4048 Sparsity: 0.9835069444444444 Training loss: 2.6016858
Epoch: 4049 Sparsity: 0.9784722222222222 Training loss: 2.4987404
Epoch: 4050 Sparsity: 0.9907986111111111 Training loss: 2.3669984
Epoch: 4051 Sparsity: 0.9852864583333334 Training loss: 2.356727
Epoch: 4052 Sparsity: 0.9805989583333334 Training loss: 2.3081214
2019-10-27 23:59:35.720239: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at conv_grad_input_ops.cc:522 : Resource exhausted: OOM when allocating tensor with shape[30,39,39,256] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu
Traceback (most recent call last):
  File "omniglot_mrcl_pretraining.py", line 47, in <module>
    loss = pretrain_classification_mrcl(x_traj, y_traj, x_rand, y_rand, rln, tln, classification_parameters)
  File "/home/mihaela_stoycheva/mrcl/experiments/exp4_2/omniglot_model.py", line 112, in pretrain_classification_mrcl
    rln_gradients = theta_tape.gradient(outer_loss, rln.trainable_variables)
  File "/home/mihaela_stoycheva/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py", line 1014, in gradient
    unconnected_gradients=unconnected_gradients)
  File "/home/mihaela_stoycheva/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/imperative_grad.py", line 76, in imperative_grad
    compat.as_str(unconnected_gradients.value))
  File "/home/mihaela_stoycheva/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py", line 138, in _gradient_function
    return grad_fn(mock_op, *out_grads)
  File "/home/mihaela_stoycheva/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_grad.py", line 596, in _Conv2DGrad
    data_format=data_format),
  File "/home/mihaela_stoycheva/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_nn_ops.py", line 1372, in conv2d_backprop_input
    _six.raise_from(_core._status_to_exception(e.code, message), None)
  File "<string>", line 3, in raise_from
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[30,39,39,256] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:Conv2DBackpropInput]
